{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T01:24:24.836800Z",
     "start_time": "2021-03-13T01:24:24.821118Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import spacy\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialización de SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T22:50:06.431885Z",
     "start_time": "2021-03-12T22:50:01.486480Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "localizacion_spark = '/opt/spark-2.4.5'\n",
    "findspark.init(localizacion_spark)\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# numero de cores: 4, memoria ram que se le permite a spark usar: 7GB\n",
    "spark_configurations = SparkConf()\\\n",
    "    .setMaster('local[4]')\\\n",
    "    .setAppName('Tarea_1')\\\n",
    "    .set(\"spark.driver.memory\", \"7g\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf = spark_configurations)\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[4]')\\\n",
    "    .appName(\"Tarea_1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T20:22:13.751066Z",
     "start_time": "2021-03-13T20:22:13.674333Z"
    }
   },
   "outputs": [],
   "source": [
    "def documentReaderSpark(data_path, sparkContext):\n",
    "    \"\"\"\n",
    "    Reads the documents using the RDD format of Spark. Each partition of the database \n",
    "    is a single document.\n",
    "    :param data_path: path of the folder where all the documents are located\n",
    "    :param sparkContext: object SparkContext() initialized\n",
    "    :return: RDD of the documents\n",
    "    \"\"\"\n",
    "    documents = sc\\\n",
    "        .wholeTextFiles(data_path,\n",
    "                        minPartitions=None, \n",
    "                        use_unicode=True)\\\n",
    "        .map(lambda s: (re.search('<public publicId=\"(.*?)\" uri=\"(.*?)\" />',s[1]).group(1),\n",
    "                        s[1].replace(\"\\n\",\"\")\\\n",
    "                            .replace(\"\\xa0\",\" \"))\n",
    "            )\\\n",
    "        .map(lambda s: (int(s[0].replace('d','')),re.search('<raw><!\\[CDATA\\[(.*?)\\]\\]></raw>',s[1]).group(1)))\n",
    "    # print(documents.collect()[0])\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T20:22:14.207414Z",
     "start_time": "2021-03-13T20:22:14.182665Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizationSpark(documents_rdd, use_spacy=False):\n",
    "    \"\"\"\n",
    "    Tokenizes, removes stop words, normalizes and lemmatizes the documents\n",
    "    :param documents_rdd:RDD of the documents\n",
    "    :param use_spacy: Boolean used to specify the use of the package Spacy. By default uses\n",
    "    nltk\n",
    "    :return: RDD of term and corresponding document\n",
    "    \"\"\"\n",
    "    if use_spacy:\n",
    "        nlp_spacy_en = spacy.load('en_core_web_sm')\n",
    "        nltk_lemmaList = documents_rdd\\\n",
    "            .map(lambda s : (s[0], nlp_spacy_en(s[1])))\\\n",
    "            .flatMap(lambda s : [(lemma,s[0]) for lemma in [token.lemma_ for token in s[1]]\n",
    "                                 if nlp_spacy_en.vocab[lemma].is_stop == False\n",
    "                                 and nlp_spacy_en.vocab[lemma].is_punct == False])\\\n",
    "            .map(lambda t : ((t[0], t[1]),1))\n",
    "    else:\n",
    "        nltk_stop_words_en = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "        p_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "        wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        nltk_lemmaList = documents_rdd\\\n",
    "            .map(lambda s : (s[0], [word for word in nltk.word_tokenize(s[1]) \n",
    "                                    if word.isalnum()]))\\\n",
    "            .flatMap(lambda s : [(token,s[0]) for token in s[1] \n",
    "                                 if token not in nltk_stop_words_en])\\\n",
    "            .map(lambda s : ((wordnet_lemmatizer.lemmatize(s[0]), s[1]),1))\n",
    "    \n",
    "    # print(nltk_lemmaList.filter(lambda x : 223==x[0][1]).collect())\n",
    "    return nltk_lemmaList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T20:22:15.540306Z",
     "start_time": "2021-03-13T20:22:15.516475Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def makeInvertedIndexSpark(tokenized_documents_rdd):\n",
    "    \"\"\"\n",
    "    Creates and saves the inverted index in pickle format. Additionally this implementation calculates\n",
    "    term frecuency.\n",
    "    :param tokenized_documents_rdd:RDD of the terms\n",
    "    :return: Dictionary of the inverted index\n",
    "    \"\"\"\n",
    "    inverted_index = tokenized_documents_rdd\\\n",
    "        .reduceByKey(lambda a, b : a+b )\\\n",
    "        .map(lambda s : (s[0][0], [[s[0][1],s[1]]]))\\\n",
    "        .reduceByKey(lambda a, b : sorted(a+b) )\\\n",
    "        .sortBy(lambda s : s[0])\\\n",
    "        .map(lambda s :  { s[0]: {'freq' : len(s[1]), 'posting':s[1]}})\\\n",
    "        .collect()\n",
    "    respuesta = {}\n",
    "    for item in inverted_index:\n",
    "        respuesta.update(item)\n",
    "    with open(os.path.join('docs','inverted_index.pkl'), 'wb') as handle:\n",
    "        pickle.dump(respuesta, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea el indice\n",
    "documents_path = os.path.join('docs', 'docs-raw-texts')\n",
    "documents = documentReaderSpark(documents_path, sc)\n",
    "tokenized_docs = tokenizationSpark(documents)\n",
    "inverted_index = makeInvertedIndexSpark(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T01:29:51.453449Z",
     "start_time": "2021-03-13T01:29:50.713054Z"
    }
   },
   "outputs": [],
   "source": [
    "# se termina la sesion de spark\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "410px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
