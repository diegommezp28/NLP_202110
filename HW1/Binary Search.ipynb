{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Search Engine. Binary Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\allan\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\allan\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\allan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\allan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\allan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xml.dom import minidom\n",
    "from xml.etree import cElementTree as ElementTree\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d001', 'William Beaumont and the Human Digestion William Beaumont and the Human Digestion.  William Beaumont: Physiology of digestion Image Source.  On November 21, 1785, US-American surgeon William Beaumont was born. He became best known as “Father of Gastric Physiology” following his research on human digestion. William Beaumont was born in Lebanon, Connecticut and became a physician. He served as a surgeon’s mate in the Army during the War of 1812. He opened a private practice in Plattsburgh, New York, but rejoined the Army as a surgeon in 1819. Beaumont was stationed at Fort Mackinac on Mackinac Island in Michigan in the early 1820s when it existed to protect the interests of the American Fur Company. The fort became the refuge for a wounded 19-year-old French-Canadian fur trader named Alexis St. Martin when a shotgun went off by accident in the American Fur Company store at close range June 6th, 1822. St. Martin’s wound was quite serious because his stomach was perforated and several ribs were broken. Nobody really expected that the young man would survive but he really did. The skin around St. Martin’s wound fused to the hole in his stomach, leaving a permanent opening – a gastric fistula. [1] Beaumont quickly noticed that there was much research potential. Back then, not too much was known about the digestive system. In order to gain more information, Beaumont performed numerous experiments on St. Martin over a period of eight years. The experiments must have been really uncomfortable for the man, who was inserted bits of different foods tied to strings through the hole in his stomach, pulling them out periodically to observe digestion. Beaumont also removed gastric juice, examining it to better understand its nature. Beaumont became the “Father of Gastric Physiology” and his findings were published in the book “Experiments and Observations on the Gastric Juice and the Physiology of Digestion” in 1833. The work is now considered as the basis of much of the early knowledge on digestion. William Beaumont discovered that hydrochloric acid is the main chemical responsible for breaking down food and he suggested that another important digestive chemical, which is now known as pepsin. He suggested that digestion is a chemical process, not merely a mechanical one caused by stomach muscle movement. Also, Beaumont gave insights on how emotions, temperature, and physical activity can affect digestion. Beaumont’s famous patient, St. Martin, outlived the scientist even though his wound never completely healed. He had several children and died at the age of 83. [2] At yovisto, you may be interested in a video lecture on The Digestive System.')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def document_reader():\n",
    "    \"\"\"\n",
    "    This method reads the documents\n",
    "    :return: Dictionary of documents (di: content of document i)\n",
    "    \"\"\"\n",
    "    documents_path = os.path.join(os.getcwd(), 'docs/docs-raw-texts')\n",
    "    documentos = {}\n",
    "    for filename in os.listdir(documents_path):\n",
    "        file_path = os.path.join(documents_path, filename)\n",
    "        xmldoc = minidom.parse(file_path)\n",
    "        id = xmldoc.getElementsByTagName('public')[0].attributes['publicId'].value\n",
    "        title = xmldoc.getElementsByTagName('fileDesc')[0].attributes['title'].value\n",
    "        data = next(ElementTree.parse(file_path).iter('raw')).text\n",
    "        documentos[id] = (title + ' ' + data).replace(u'\\xa0', u' ').replace('\\n', ' ')\n",
    "\n",
    "    return documentos\n",
    "documents = document_reader()\n",
    "print(list(documents.items())[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q01': 'Fabrication of music instruments', 'q02': 'famous German poetry', 'q03': 'Romanticism', 'q04': 'University of Edinburgh research', 'q06': 'bridge construction', 'q07': 'Walk of Fame stars', 'q08': 'Scientists who worked on the atomic bomb', 'q09': 'Invention of the Internet', 'q10': 'early telecommunication methods', 'q12': 'Who explored the South Pole', 'q13': 'famous members of the Royal Navy', 'q14': 'Nobel Prize winning inventions', 'q16': 'South America', 'q17': 'Edward Teller and Marie Curie', 'q18': 'Computing Language for the programming of Artificial Intelligence', 'q19': 'William Hearst movie', 'q22': 'How did Captain James Cook become an explorer', 'q23': 'How did Grace Hopper get famous', 'q24': 'Computers in Astronomy', 'q25': 'WWII aircraft', 'q26': 'Literary critics on Thomas Moore', 'q27': 'Nazis confiscate or destroy art and literature', 'q28': 'Modern Age in English Literature', 'q29': 'modern Physiology', 'q32': 'Roman Empire', 'q34': 'Scientists who have contributed to photosynthesis', 'q36': \"Aviation pioneers' publications\", 'q37': 'Gutenberg Bible', 'q38': 'Religious beliefs of scientists and explorers', 'q40': 'Carl Friedrich Gauss influence on colleagues', 'q41': 'Personalities from Hannover', 'q42': \"Skinner's experiments with the operant conditioning chamber\", 'q44': \"Napoleon's Russian Campaign\", 'q45': 'Friends and enemies of Napoleon Bonaparte', 'q46': 'First woman who won a Nobel Prize'}\n"
     ]
    }
   ],
   "source": [
    "def queries_reader():\n",
    "    \"\"\"\n",
    "    Reads the query\n",
    "    :return: Dictionary of documents (di: content of document i)\n",
    "    \"\"\"\n",
    "    queries_path = os.path.join(os.getcwd(), 'docs/queries-raw-texts')\n",
    "    queries = {}\n",
    "    queries_paths = os.listdir(queries_path)\n",
    "    queries_paths.sort()\n",
    "    for filename in queries_paths:\n",
    "        file_path = os.path.join(queries_path, filename)\n",
    "        xmldoc = minidom.parse(file_path)\n",
    "        id = xmldoc.getElementsByTagName('public')[0].attributes['publicId'].value\n",
    "        query = next(ElementTree.parse(file_path).iter('raw')).text\n",
    "        queries[id] = query.replace(u'\\xa0', u' ').replace('\\n', ' ')\n",
    "    return queries\n",
    "\n",
    "queries = queries_reader()\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['William', 'Beaumont', 'Human', 'Digestion', 'William', 'Beaumont', 'Human', 'Digestion', 'William', 'Beaumont', 'Physiology', 'digestion', 'Image', 'Source', 'On', 'November', '21', '1785', 'surgeon', 'William', 'Beaumont', 'born', 'He', 'became', 'best', 'known', 'Father', 'Gastric', 'Physiology', 'following', 'research', 'human', 'digestion', 'William', 'Beaumont', 'born', 'Lebanon', 'Connecticut', 'became', 'physician', 'He', 'served', 'surgeon', 'mate', 'Army', 'War', '1812', 'He', 'opened', 'private', 'practice', 'Plattsburgh', 'New', 'York', 'rejoined', 'Army', 'surgeon', '1819', 'Beaumont', 'stationed', 'Fort', 'Mackinac', 'Mackinac', 'Island', 'Michigan', 'early', '1820s', 'existed', 'protect', 'interest', 'American', 'Fur', 'Company', 'The', 'fort', 'became', 'refuge', 'wounded', 'fur', 'trader', 'named', 'Alexis', 'Martin', 'shotgun', 'went', 'accident', 'American', 'Fur', 'Company', 'store', 'close', 'range', 'June', '6th', '1822', 'Martin', 'wound', 'quite', 'serious', 'stomach', 'perforated', 'several', 'rib', 'broken', 'Nobody', 'really', 'expected', 'young', 'man', 'would', 'survive', 'really', 'The', 'skin', 'around', 'Martin', 'wound', 'fused', 'hole', 'stomach', 'leaving', 'permanent', 'opening', 'gastric', 'fistula', '1', 'Beaumont', 'quickly', 'noticed', 'much', 'research', 'potential', 'Back', 'much', 'known', 'digestive', 'system', 'In', 'order', 'gain', 'information', 'Beaumont', 'performed', 'numerous', 'experiment', 'Martin', 'period', 'eight', 'year', 'The', 'experiment', 'must', 'really', 'uncomfortable', 'man', 'inserted', 'bit', 'different', 'food', 'tied', 'string', 'hole', 'stomach', 'pulling', 'periodically', 'observe', 'digestion', 'Beaumont', 'also', 'removed', 'gastric', 'juice', 'examining', 'better', 'understand', 'nature', 'Beaumont', 'became', 'Father', 'Gastric', 'Physiology', 'finding', 'published', 'book', 'Experiments', 'Observations', 'Gastric', 'Juice', 'Physiology', 'Digestion', '1833', 'The', 'work', 'considered', 'basis', 'much', 'early', 'knowledge', 'digestion', 'William', 'Beaumont', 'discovered', 'hydrochloric', 'acid', 'main', 'chemical', 'responsible', 'breaking', 'food', 'suggested', 'another', 'important', 'digestive', 'chemical', 'known', 'pepsin', 'He', 'suggested', 'digestion', 'chemical', 'process', 'merely', 'mechanical', 'one', 'caused', 'stomach', 'muscle', 'movement', 'Also', 'Beaumont', 'gave', 'insight', 'emotion', 'temperature', 'physical', 'activity', 'affect', 'digestion', 'Beaumont', 'famous', 'patient', 'Martin', 'outlived', 'scientist', 'even', 'though', 'wound', 'never', 'completely', 'healed', 'He', 'several', 'child', 'died', 'age', '83', '2', 'At', 'yovisto', 'may', 'interested', 'video', 'lecture', 'The', 'Digestive', 'System']\n"
     ]
    }
   ],
   "source": [
    "def tokenization(documentos):\n",
    "    \"\"\"\n",
    "    :param documentos: Receives a dictionary  \n",
    "    :return: dict with key id of documents/queries and value is an array of terms\n",
    "    \"\"\"\n",
    "    nltk_stop_words_en = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    nltk_lemmaList = {key: [wordnet_lemmatizer.lemmatize(token) \n",
    "                             for token in nltk.word_tokenize(doc) if (token.isalnum()) and (token not in nltk_stop_words_en)] for key, doc in documentos.items()}\n",
    "\n",
    "    return nltk_lemmaList\n",
    "\n",
    "tokenized_docs = tokenization(documents)\n",
    "tokenized_queries = tokenization(queries)\n",
    "\n",
    "print(list(tokenized_docs.items())[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 18936\n",
      "Size of the matrix:  18936 x 331\n"
     ]
    }
   ],
   "source": [
    "def matrix_construction(tokenized_docs):\n",
    "    \"\"\"\n",
    "    :param tokenized_docs: dict with key id of documents and value an array of terms\n",
    "    :return: Matrix term-document t1 = [d1, d2, ..., dn], where di is 1 o 0\n",
    "    \"\"\"\n",
    "    term_document = {}\n",
    "    for id,doc in tokenized_docs.items():\n",
    "        id = int(id[-3:]) #paasa dnjk al entero njk.\n",
    "        for token in doc:\n",
    "            if token not in term_document:\n",
    "                term_document[token] = [0] * len(list(tokenized_docs.items())) \n",
    "            \n",
    "            term_document[token][id-1] = 1\n",
    "    \n",
    "    return term_document\n",
    "\n",
    "term_document = matrix_construction(tokenized_docs)\n",
    "\n",
    "print('Size of vocabulary:', len(list(term_document.items())))\n",
    "print('Size of the matrix: ',len(list(term_document.items())), 'x', len(list(term_document.items())[0][1]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: [token,vector]\n",
      "Row in the csv: ['William', '1000000000000010000000000001000000100000000000000000001100000000000010000000000000000001001100100100010001001010000000000000000010000001010000000010000000000000000000000000001000110000000011100000100000000000000100000000000000000100000000001000000000000100100000000100000111000000000000001010010000110000000011000000000100100000010']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def serialize_and_save(term_document):\n",
    "    \"\"\"\n",
    "    :param term_document: Matrix term-document t1 = [d1, d2, ..., dn], where di is 1 o 0\n",
    "    :return: csv with columns (token,vector) donde vector es el string concatenado de 0s y 1s de la funcion binary_search\n",
    "    \"\"\"\n",
    "    serialize = [['token','vector']]\n",
    "    \n",
    "    for token,arr in term_document.items():\n",
    "        row =  [token, \"\".join([str(val) for val in arr])]\n",
    "        serialize.append(row)\n",
    "    \n",
    "    with open('document_term_file.csv', mode='w', encoding='utf-8', newline='') as document_term_file:\n",
    "        term_document_writer = csv.writer(document_term_file, delimiter=',')\n",
    "        term_document_writer.writerows(serialize)\n",
    "\n",
    "    return serialize\n",
    "value = serialize_and_save(term_document)\n",
    "    \n",
    "print('columns: [token,vector]')\n",
    "print('Row in the csv:',value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q01': ['Fabrication', 'music', 'instrument'], 'q02': ['famous', 'German', 'poetry'], 'q03': ['Romanticism'], 'q04': ['University', 'Edinburgh', 'research'], 'q06': ['bridge', 'construction'], 'q07': ['Walk', 'Fame', 'star'], 'q08': ['Scientists', 'worked', 'atomic', 'bomb'], 'q09': ['Invention', 'Internet'], 'q10': ['early', 'telecommunication', 'method'], 'q12': ['Who', 'explored', 'South', 'Pole'], 'q13': ['famous', 'member', 'Royal', 'Navy'], 'q14': ['Nobel', 'Prize', 'winning', 'invention'], 'q16': ['South', 'America'], 'q17': ['Edward', 'Teller', 'Marie', 'Curie'], 'q18': ['Computing', 'Language', 'programming', 'Artificial', 'Intelligence'], 'q19': ['William', 'Hearst', 'movie'], 'q22': ['How', 'Captain', 'James', 'Cook', 'become', 'explorer'], 'q23': ['How', 'Grace', 'Hopper', 'get', 'famous'], 'q24': ['Computers', 'Astronomy'], 'q25': ['WWII', 'aircraft'], 'q26': ['Literary', 'critic', 'Thomas', 'Moore'], 'q27': ['Nazis', 'confiscate', 'destroy', 'art', 'literature'], 'q28': ['Modern', 'Age', 'English', 'Literature'], 'q29': ['modern', 'Physiology'], 'q32': ['Roman', 'Empire'], 'q34': ['Scientists', 'contributed', 'photosynthesis'], 'q36': ['Aviation', 'pioneer', \"'\", 'publication'], 'q37': ['Gutenberg', 'Bible'], 'q38': ['Religious', 'belief', 'scientist', 'explorer'], 'q40': ['Carl', 'Friedrich', 'Gauss', 'influence', 'colleague'], 'q41': ['Personalities', 'Hannover'], 'q42': ['Skinner', \"'s\", 'experiment', 'operant', 'conditioning', 'chamber'], 'q44': ['Napoleon', \"'s\", 'Russian', 'Campaign'], 'q45': ['Friends', 'enemy', 'Napoleon', 'Bonaparte'], 'q46': ['First', 'woman', 'Nobel', 'Prize']}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def calculate_result(query, dict_with_vector):\n",
    "    result = [1] * 331\n",
    "    bad_result = [0] * 331\n",
    "    for token in query[1]:\n",
    "        if token in dict_with_vector:\n",
    "            result = np.bitwise_and(dict_with_vector[token], result) \n",
    "        else:\n",
    "            result = bad_result\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def binary_search(queries):\n",
    "    \"\"\"\n",
    "    This method prints tokenized the queries, and print the list of documents that a query appears completely\n",
    "    :param queries: dict with key the id of a query and value the query\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    with open('document_term_file.csv', 'r', encoding='utf-8', newline='') as document_term_file:\n",
    "        rows = csv.reader(document_term_file)\n",
    "        dict_with_vector = {}\n",
    "        idx = 0\n",
    "        for row in rows:\n",
    "            idx+=1\n",
    "            if idx == 1:\n",
    "                continue\n",
    "            token = row[0]\n",
    "            string = row[1]\n",
    "            vector = [int(letter) for letter in list(string)]\n",
    "            dict_with_vector[token] = vector\n",
    "        ans = {}\n",
    "        for query in queries:\n",
    "            result = calculate_result(query, dict_with_vector)\n",
    "\n",
    "            idDoc = 0\n",
    "            string = ''\n",
    "            for res in result:\n",
    "                idDoc += 1\n",
    "                if res == 1:\n",
    "                    string += 'd' + str(idDoc) + ','\n",
    "            ans[query[0]] = string[:-1]\n",
    "        with open('BS-queries_results.csv', mode='w', encoding='utf-8', newline='') as BS_queries:\n",
    "            term_document_writer = csv.writer(BS_queries, delimiter='\\t')\n",
    "            for k,v in ans.items():\n",
    "                term_document_writer.writerow([k,v])\n",
    "\n",
    "binary_search(list(tokenized_queries.items()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
