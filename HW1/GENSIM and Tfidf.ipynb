{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "from xml.dom import minidom\n",
    "from xml.etree import cElementTree as ElementTree\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gensim Corpus and Tf.idf Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Corpus and Tf.Idf Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d001', 'William Beaumont and the Human Digestion William Beaumont and the Human Digestion.  William Beaumont: Physiology of digestion Image Source.  On November 21, 1785, US-American surgeon William Beaumont was born. He became best known as “Father of Gastric Physiology” following his research on human digestion. William Beaumont was born in Lebanon, Connecticut and became a physician. He served as a surgeon’s mate in the Army during the War of 1812. He opened a private practice in Plattsburgh, New York, but rejoined the Army as a surgeon in 1819. Beaumont was stationed at Fort Mackinac on Mackinac Island in Michigan in the early 1820s when it existed to protect the interests of the American Fur Company. The fort became the refuge for a wounded 19-year-old French-Canadian fur trader named Alexis St. Martin when a shotgun went off by accident in the American Fur Company store at close range June 6th, 1822. St. Martin’s wound was quite serious because his stomach was perforated and several ribs were broken. Nobody really expected that the young man would survive but he really did. The skin around St. Martin’s wound fused to the hole in his stomach, leaving a permanent opening – a gastric fistula. [1] Beaumont quickly noticed that there was much research potential. Back then, not too much was known about the digestive system. In order to gain more information, Beaumont performed numerous experiments on St. Martin over a period of eight years. The experiments must have been really uncomfortable for the man, who was inserted bits of different foods tied to strings through the hole in his stomach, pulling them out periodically to observe digestion. Beaumont also removed gastric juice, examining it to better understand its nature. Beaumont became the “Father of Gastric Physiology” and his findings were published in the book “Experiments and Observations on the Gastric Juice and the Physiology of Digestion” in 1833. The work is now considered as the basis of much of the early knowledge on digestion. William Beaumont discovered that hydrochloric acid is the main chemical responsible for breaking down food and he suggested that another important digestive chemical, which is now known as pepsin. He suggested that digestion is a chemical process, not merely a mechanical one caused by stomach muscle movement. Also, Beaumont gave insights on how emotions, temperature, and physical activity can affect digestion. Beaumont’s famous patient, St. Martin, outlived the scientist even though his wound never completely healed. He had several children and died at the age of 83. [2] At yovisto, you may be interested in a video lecture on The Digestive System.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def documentReader(path, queries = False):\n",
    "    \"\"\"\n",
    "    DocString\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    documents_path = os.path.join(os.getcwd(), path)\n",
    "    documentos = {}\n",
    "    for filename in os.listdir(documents_path):\n",
    "        file_path = os.path.join(documents_path, filename)\n",
    "        xmldoc = minidom.parse(file_path)\n",
    "        id = xmldoc.getElementsByTagName('public')[0].attributes['publicId'].value\n",
    "        title = '' if queries else xmldoc.getElementsByTagName('fileDesc')[0].attributes['title'].value\n",
    "        data = next(ElementTree.parse(file_path).iter('raw')).text\n",
    "        documentos[id] = (title + ' ' + data).replace(u'\\xa0', u' ').replace('\\n', ' ')\n",
    "\n",
    "    return documentos\n",
    "documentos = documentReader('docs/docs-raw-texts')\n",
    "NRO_DOCS = len(documentos)\n",
    "DOCS_IDs = list(documentos.keys())\n",
    "print(list(documentos.items())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documentos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-693ebe3ab754>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mdocDict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdoc\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdocumentos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     \u001B[0mdocDict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'documentos' is not defined"
     ]
    }
   ],
   "source": [
    "p = PorterStemmer()\n",
    "def process(text):\n",
    "    doc_nor = text.lower()\n",
    "    doc_sw = remove_stopwords(doc_nor)\n",
    "    doc_stem = p.stem_sentence(doc_sw)\n",
    "    return nltk.word_tokenize(doc_stem)\n",
    "\n",
    "docDict = []\n",
    "for key, doc in documentos.items():\n",
    "    docDict.append(process(doc))\n",
    "\n",
    "docDict[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(docDict)\n",
    "dictionary.save('docs/midict.dict')\n",
    "print(dictionary.token2id['information'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##  Market Matrix format\n",
    "# Step 1: Build the corpus from big file\n",
    "class MyCorpus():\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "    def __iter__(self):\n",
    "        for key, doc in self.documents.items():\n",
    "            yield dictionary.doc2bow(process(doc))\n",
    "\n",
    "corpus_memory_friendly = MyCorpus(documentos)\n",
    "corpora.MmCorpus.serialize(\"docs/corpus.mm\",corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Read Maket Matrix format from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 20.0), (1, 21.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0), (8, 1.0), (9, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "corpus = corpora.MmCorpus(\"docs/corpus.mm\")\n",
    "# No hacer esto en una implementacion real\n",
    "for doc in corpus:\n",
    "    print(doc[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Build tf.idf model from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load('docs/midict.dict')\n",
    "corpus = corpora.MmCorpus('docs/corpus.mm')\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(241, 1), (5809, 1)]\n",
      "[(241, 0.2642196547502339), (5809, 0.9644625311766483)]\n"
     ]
    }
   ],
   "source": [
    "#Test to verify correct reading\n",
    "query = \"Machine learning\"\n",
    "query_doc_bow = dictionary.doc2bow(process(query)) # Important: Same corpus preprocess\n",
    "print(query_doc_bow)\n",
    "print(tfidf[query_doc_bow])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Make similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(tfidf[corpus])\n",
    "index.save('docs/similmatrix.index')\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Querying and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity.load('docs/similmatrix.index')\n",
    "sims = index[tfidf[query_doc_bow]]\n",
    "print(list(enumerate(sims))[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read and proccess queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def queries_reader():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    queries_path = os.path.join(os.getcwd(), 'docs/queries-raw-texts')\n",
    "    queries = {}\n",
    "    queries_paths = os.listdir(queries_path)\n",
    "    queries_paths.sort()\n",
    "    #print(documents_paths)\n",
    "    query_index = []\n",
    "    for filename in queries_paths:\n",
    "        file_path = os.path.join(queries_path, filename)\n",
    "        #print(filename)\n",
    "        xmldoc = minidom.parse(file_path)\n",
    "        id = xmldoc.getElementsByTagName('public')[0].attributes['publicId'].value\n",
    "        query = next(ElementTree.parse(file_path).iter('raw')).text\n",
    "        queries[id] = query.replace(u'\\xa0', u' ').replace('\\n', ' ')\n",
    "        query_index.append(id)\n",
    "    return queries, query_index\n",
    "\n",
    "queries, query_index = queries_reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d016', 'd259', 'd254', 'd186', 'd085', 'd209', 'd215', 'd170', 'd153', 'd008', 'd185', 'd154', 'd163', 'd315', 'd296', 'd060', 'd089', 'd243', 'd004', 'd006', 'd162', 'd100', 'd094', 'd179', 'd145', 'd059', 'd039', 'd329', 'd299', 'd273', 'd312', 'd028', 'd311', 'd082', 'd281', 'd255', 'd065', 'd074', 'd317', 'd265', 'd229', 'd275', 'd130', 'd021', 'd077', 'd152', 'd195', 'd052', 'd316', 'd038', 'd164', 'd024', 'd123', 'd136', 'd184']\n"
     ]
    }
   ],
   "source": [
    "def queries_evaluation(queries):\n",
    "    queries_rank = {}\n",
    "    for idq, query in queries.items():\n",
    "        query_doc_bow = dictionary.doc2bow(process(query))\n",
    "        sims = index[tfidf[query_doc_bow]]\n",
    "        sorted_vals = sorted(list(enumerate(sims)), key=lambda x: x[1], reverse=True)\n",
    "        clean_query_scores = [ \"d{0:0=3d}\".format(id+1) for id,v in sorted_vals if v != 0]\n",
    "        queries_rank[idq] = clean_query_scores\n",
    "    return queries_rank\n",
    "\n",
    "\n",
    "queries_ranking = queries_evaluation(queries)\n",
    "print(queries_ranking[\"q01\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d136': '2', 'd139': '2', 'd143': '4', 'd147': '2', 'd149': '2', 'd164': '4', 'd228': '4', 'd283': '4', 'd291': '4', 'd293': '4', 'd318': '2'}\n"
     ]
    }
   ],
   "source": [
    "def read_judgemnts_file():\n",
    "    \"\"\"\n",
    "    DocString\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    document_path = os.path.join(os.getcwd(), 'docs/relevance-judgments.tsv')\n",
    "    tsv_file = open(document_path)\n",
    "    read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "    relevance = {}\n",
    "    for row in read_tsv:\n",
    "        documents = row[1].split(',')\n",
    "        query_relevance = {pair.split(':')[0] : pair.split(':')[1] for pair in documents }\n",
    "        query_relevance = dict(sorted(query_relevance.items(), key=lambda item: item[0]))\n",
    "        relevance[row[0]] = query_relevance\n",
    "    return relevance\n",
    "\n",
    "\n",
    "relevance = read_judgemnts_file()\n",
    "print(relevance['q02'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_binary_result(results, relevant_res):\n",
    "    bin_relevant = {}\n",
    "    rel_scale_repr = {}\n",
    "    for query, relevant_docs in relevant_res.items():\n",
    "        bin_repr = []\n",
    "        scaled_repr = []\n",
    "        M = len(relevant_docs)\n",
    "        for doc_id, rel_scale in relevant_docs.items():\n",
    "            bin = 1 if doc_id in results[query][:M] else 0\n",
    "            bin_repr.append(bin)\n",
    "            scaled_repr.append(bin * int(rel_scale))\n",
    "        bin_relevant[query] = bin_repr\n",
    "        rel_scale_repr[query] = scaled_repr\n",
    "    return bin_relevant, rel_scale_repr\n",
    "\n",
    "bin_results, scaled_results = make_binary_result(queries_ranking, relevance)\n",
    "bin_results['q02']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of IR metrics functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "0.35468253968253965"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision_at_k(relevance: list, k: int):\n",
    "    \"\"\"\n",
    "    DocString\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return 0\n",
    "    l = np.array(relevance[:k]).sum()/k\n",
    "    return l\n",
    "\n",
    "def recall_at_k(relevance: list, nr_relevant: int, k: int):\n",
    "    \"\"\"\n",
    "    DocString\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    l = np.array(relevance[:k]).sum()/nr_relevant\n",
    "    return l\n",
    "\n",
    "def average_precision(relevance):\n",
    "    \"\"\"\n",
    "    DocString\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    length = len(relevance)\n",
    "    sum = 0\n",
    "    for i in range(length):\n",
    "        if relevance[i]:\n",
    "            sum += precision_at_k(relevance, i+1)\n",
    "    if np.array(relevance).sum()==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sum / np.array(relevance).sum()\n",
    "\n",
    "def mean_avg_precision(l):\n",
    "    \"\"\"\n",
    "    DocString\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    average = 0\n",
    "    for lista in l:\n",
    "        #print(average_precision(lista))\n",
    "        average+= average_precision(lista)\n",
    "\n",
    "    mean = average / len(l)\n",
    "    return mean\n",
    "\n",
    "mean_avg_precision([[0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 1], [0, 1, 0, 1, 1, 1, 1]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7424602308163405"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dcg_at_k(relevance, k: int):\n",
    "    \"\"\"\n",
    "    DocString\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    sum = 0\n",
    "    i =  0\n",
    "    for rel_i in relevance[: k]:\n",
    "        i+= 1\n",
    "        sum += rel_i/np.log2(max(i, 2))\n",
    "\n",
    "    return sum\n",
    "\n",
    "dcg_at_k([4, 4, 3, 0, 0, 1, 3, 3, 3, 0], 6)\n",
    "\n",
    "def ndcg_at_k(relevance, k):\n",
    "    \"\"\"\n",
    "    DocString\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    rel_sorted = sorted(relevance, reverse=True)\n",
    "    max = dcg_at_k(rel_sorted, k)\n",
    "    real = dcg_at_k(relevance, k)\n",
    "\n",
    "    return real/ max\n",
    "\n",
    "\n",
    "ndcg_at_k([4, 4, 3, 0, 0, 1, 3, 3, 3, 0], 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(recall_at_k(bin_results['q01'], 3, 3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute Evaluation Metrics for each query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-b9586156bffd>:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return real/ max\n"
     ]
    },
    {
     "data": {
      "text/plain": "          P@M       R@M    NDCG@M\nq01  0.666667  0.666667  0.815465\nq02  0.363636  0.363636  0.428656\nq03  0.500000  0.500000  0.567635\nq04  0.714286  0.714286  0.928210\nq06  0.666667  0.666667  0.691704\nq07  0.500000  0.500000  0.800000\nq08  0.666667  0.666667  0.837297\nq09  0.833333  0.833333  0.880115\nq10  0.375000  0.375000  0.577633\nq12  1.000000  1.000000  0.989111",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P@M</th>\n      <th>R@M</th>\n      <th>NDCG@M</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>q01</th>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.815465</td>\n    </tr>\n    <tr>\n      <th>q02</th>\n      <td>0.363636</td>\n      <td>0.363636</td>\n      <td>0.428656</td>\n    </tr>\n    <tr>\n      <th>q03</th>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.567635</td>\n    </tr>\n    <tr>\n      <th>q04</th>\n      <td>0.714286</td>\n      <td>0.714286</td>\n      <td>0.928210</td>\n    </tr>\n    <tr>\n      <th>q06</th>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.691704</td>\n    </tr>\n    <tr>\n      <th>q07</th>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <th>q08</th>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.837297</td>\n    </tr>\n    <tr>\n      <th>q09</th>\n      <td>0.833333</td>\n      <td>0.833333</td>\n      <td>0.880115</td>\n    </tr>\n    <tr>\n      <th>q10</th>\n      <td>0.375000</td>\n      <td>0.375000</td>\n      <td>0.577633</td>\n    </tr>\n    <tr>\n      <th>q12</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.989111</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation_metric(bin_queries, query_index, scaled_results):\n",
    "    COLUMNS = ['P@M', 'R@M', 'NDCG@M']\n",
    "    records = []\n",
    "    for query, bin_vec in bin_queries.items():\n",
    "        scaled = scaled_results[query]\n",
    "        M = len(bin_vec)\n",
    "        pm = precision_at_k(bin_vec, M)\n",
    "        rm = recall_at_k(bin_vec, M, M)\n",
    "        ndcg = ndcg_at_k(scaled, M)\n",
    "        records.append([pm, rm, ndcg])\n",
    "\n",
    "    return pd.DataFrame.from_records(records, index=query_index, columns=COLUMNS)\n",
    "\n",
    "metrics = evaluation_metric(bin_results, query_index, scaled_results)\n",
    "metrics.head(10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MAP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP resultante de todas las queries: 0.6533977342259655\n"
     ]
    }
   ],
   "source": [
    "def overall_map(bin_results):\n",
    "    matrix = [vector for key, vector in bin_results.items() ]\n",
    "    return mean_avg_precision(matrix)\n",
    "\n",
    "print(f'MAP resultante de todas las queries: {overall_map(bin_results)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}