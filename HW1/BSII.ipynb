{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T22:03:13.008025Z",
     "start_time": "2021-03-13T22:03:05.618489Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import spacy\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo correr este capitulo si no hay inverted index creado. Se asume que se tiene spark 2.4.5 instalado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización de SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T22:49:59.744629Z",
     "start_time": "2021-03-13T22:49:59.472628Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "localizacion_spark = '/opt/spark-2.4.5' # lugar donde tenga instalado spark\n",
    "findspark.init(localizacion_spark)\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# numero de cores: 4, memoria ram que se le permite a spark usar: 7GB\n",
    "spark_configurations = SparkConf()\\\n",
    "    .setMaster('local[4]')\\\n",
    "    .setAppName('Tarea_1')\\\n",
    "    .set(\"spark.driver.memory\", \"7g\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf = spark_configurations)\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[4]')\\\n",
    "    .appName(\"Tarea_1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T22:51:37.390143Z",
     "start_time": "2021-03-13T22:51:37.376107Z"
    }
   },
   "outputs": [],
   "source": [
    "def documentReaderSpark(data_path, sparkContext):\n",
    "    \"\"\"\n",
    "    Reads the documents using the RDD format of Spark. Each partition of the database \n",
    "    is a single document.\n",
    "    :param data_path: path of the folder where all the documents are located\n",
    "    :param sparkContext: object SparkContext() initialized\n",
    "    :return: RDD of the documents\n",
    "    \"\"\"\n",
    "    documents = sc\\\n",
    "        .wholeTextFiles(data_path,\n",
    "                        minPartitions=None, \n",
    "                        use_unicode=True)\\\n",
    "        .map(lambda s: (re.search('<public publicId=\"(.*?)\" uri=\"(.*?)\" />',s[1]).group(1),\n",
    "                        s[1].replace(\"\\n\",\"\")\\\n",
    "                            .replace(\"\\xa0\",\" \"))\n",
    "            )\\\n",
    "        .map(lambda s: (int(s[0].replace('d','')),re.search('<raw><!\\[CDATA\\[(.*?)\\]\\]></raw>',s[1]).group(1)))\n",
    "    # print(documents.collect()[0])\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T22:51:37.912884Z",
     "start_time": "2021-03-13T22:51:37.882060Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizationSpark(documents_rdd, use_spacy=False):\n",
    "    \"\"\"\n",
    "    Tokenizes, removes stop words, normalizes and lemmatizes the documents\n",
    "    :param documents_rdd:RDD of the documents\n",
    "    :param use_spacy: Boolean used to specify the use of the package Spacy. By default uses\n",
    "    nltk\n",
    "    :return: RDD of term and corresponding document\n",
    "    \"\"\"\n",
    "    if use_spacy:\n",
    "        nlp_spacy_en = spacy.load('en_core_web_sm')\n",
    "        nltk_lemmaList = documents_rdd\\\n",
    "            .map(lambda s : (s[0], nlp_spacy_en(s[1])))\\\n",
    "            .flatMap(lambda s : [(lemma,s[0]) for lemma in [token.lemma_ for token in s[1]]\n",
    "                                 if nlp_spacy_en.vocab[lemma].is_stop == False\n",
    "                                 and nlp_spacy_en.vocab[lemma].is_punct == False])\\\n",
    "            .map(lambda t : ((t[0], t[1]),1))\n",
    "    else:\n",
    "        nltk_stop_words_en = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "        p_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "        wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        nltk_lemmaList = documents_rdd\\\n",
    "            .map(lambda s : (s[0], [word for word in nltk.word_tokenize(s[1]) \n",
    "                                    if word.isalnum()]))\\\n",
    "            .flatMap(lambda s : [(token,s[0]) for token in s[1] \n",
    "                                 if token not in nltk_stop_words_en])\\\n",
    "            .map(lambda s : ((wordnet_lemmatizer.lemmatize(s[0]), s[1]),1))\n",
    "    \n",
    "    # print(nltk_lemmaList.filter(lambda x : 223==x[0][1]).collect())\n",
    "    return nltk_lemmaList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T22:51:41.057430Z",
     "start_time": "2021-03-13T22:51:41.022618Z"
    }
   },
   "outputs": [],
   "source": [
    "def makeInvertedIndexSpark(tokenized_documents_rdd):\n",
    "    \"\"\"\n",
    "    Creates and saves the inverted index in pickle format. Additionally this implementation calculates\n",
    "    term frecuency.\n",
    "    :param tokenized_documents_rdd:RDD of the terms\n",
    "    :return: Dictionary of the inverted index\n",
    "    \"\"\"\n",
    "    inverted_index = tokenized_documents_rdd\\\n",
    "        .reduceByKey(lambda a, b : a+b )\\\n",
    "        .map(lambda s : (s[0][0], [[s[0][1],s[1]]]))\\\n",
    "        .reduceByKey(lambda a, b : sorted(a+b) )\\\n",
    "        .sortBy(lambda s : s[0])\\\n",
    "        .map(lambda s :  { s[0]: {'freq' : len(s[1]), 'posting':s[1]}})\\\n",
    "        .collect()\n",
    "    respuesta = {}\n",
    "    for item in inverted_index:\n",
    "        respuesta.update(item)\n",
    "    with open(os.path.join('docs','inverted_index_lemma.pkl'), 'wb') as handle:\n",
    "        pickle.dump(respuesta, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return respuesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T22:52:00.057675Z",
     "start_time": "2021-03-13T22:51:43.181255Z"
    }
   },
   "outputs": [],
   "source": [
    "# se crea el indice\n",
    "documents_path = os.path.join('docs', 'docs-raw-texts')\n",
    "documents = documentReaderSpark(documents_path, sc)\n",
    "tokenized_docs = tokenizationSpark(documents)\n",
    "inverted_index = makeInvertedIndexSpark(tokenized_docs)\n",
    "# se termina la sesion de spark\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Binary Search using Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T22:03:45.367096Z",
     "start_time": "2021-03-13T22:03:45.351401Z"
    }
   },
   "outputs": [],
   "source": [
    "def lectura_inverted_index(path):\n",
    "    \"\"\"\n",
    "    Reads the inverted index given in the path\n",
    "    :param path: path of the inverted index in pickle format\n",
    "    :return: Dictionary of the inverted index\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as handle:\n",
    "        unserialized_data = pickle.load(handle)\n",
    "    return unserialized_data\n",
    "def obtener_total_documentos(inverted_index):\n",
    "    \"\"\"\n",
    "    Calculates the number of documents used in the inverted index\n",
    "    :param inverted_index: Dictionary of the inverted index \n",
    "    :return: Number of documents\n",
    "    \"\"\"\n",
    "    res = max([max([id_doc[0] for id_doc in inverted_index[term]['posting']]) \n",
    "     for term in inverted_index.keys()])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T22:04:12.114265Z",
     "start_time": "2021-03-13T22:04:12.059411Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_and(term1_docs, freq1, term2_docs, freq2):\n",
    "    \"\"\"\n",
    "    Calculates the intersection between two lists of documents using the 'merge' algorithm.\n",
    "    :param term1_docs: Sorted ascending list of integers which represents the list of \n",
    "    documents of the first term in the inverted index\n",
    "    :param freq1: Number of documents the first term appears \n",
    "    :param term2_docs: Sorted ascending list of integers which represents the list of \n",
    "    documents of the second term\n",
    "    :param freq2: Number of documents the second term appears  \n",
    "    :return: Sorted ascending list of integers of the intersection.\n",
    "    \"\"\"\n",
    "    ndocs_term1 = freq1\n",
    "    ndocs_term2 = freq2\n",
    "    answer = []\n",
    "    i1 = 0\n",
    "    i2 = 0\n",
    "    while((i1<ndocs_term1) and (i2<ndocs_term2)):\n",
    "        if term1_docs[i1]==term2_docs[i2]:\n",
    "            answer.append(term1_docs[i1])\n",
    "            i1+=1\n",
    "            i2+=1\n",
    "        elif term1_docs[i1]<term2_docs[i2]:\n",
    "            i1+=1\n",
    "        else:\n",
    "            i2+=1\n",
    "    return answer\n",
    "def merge_or(term1_docs, freq1, term2_docs, freq2):\n",
    "    \"\"\"\n",
    "    Calculates the union between two lists of documents using a variant of the\n",
    "    'merge' algorithm.\n",
    "    :param term1_docs: Sorted ascending list of integers which represents the list of \n",
    "    documents of the first term in the inverted index\n",
    "    :param freq1: Number of documents the first term appears \n",
    "    :param term2_docs: Sorted ascending list of integers which represents the list of \n",
    "    documents of the second term\n",
    "    :param freq2: Number of documents the second term appears  \n",
    "    :return: Sorted ascending list of integers without duplicates of the union.\n",
    "    \"\"\"\n",
    "    ndocs_term1 = freq1\n",
    "    ndocs_term2 = freq2\n",
    "    answer = []\n",
    "    i1 = 0\n",
    "    i2 = 0\n",
    "    while((i1<ndocs_term1) and (i2<ndocs_term2)):\n",
    "        if term1_docs[i1]==term2_docs[i2]:\n",
    "            answer.append(term1_docs[i1])\n",
    "            i1+=1\n",
    "            i2+=1\n",
    "        elif term1_docs[i1]<term2_docs[i2]:\n",
    "            answer.append(term1_docs[i1])\n",
    "            i1+=1\n",
    "        else:\n",
    "            answer.append(term2_docs[i2])\n",
    "            i2+=1\n",
    "    while( i1<ndocs_term1):\n",
    "        answer.append(term1_docs[i1])\n",
    "        i1+=1\n",
    "    while( i2<ndocs_term2):\n",
    "        answer.append(term2_docs[i2])\n",
    "        i2+=1\n",
    "    return answer\n",
    "\n",
    "def merge_not(term_docs, freq, num_documentos):\n",
    "    \"\"\"\n",
    "    Calculates the complement of the given list of documents of a term using a variant of the\n",
    "    'merge' algorithm.\n",
    "    :param term_docs: Sorted ascending list of integers which represents the list of \n",
    "    documents of the term in the inverted index\n",
    "    :param freq: Number of documents where the term appears \n",
    "    :param num_documentos: Number of documents used in the creation of the inverted index\n",
    "    :return: Sorted ascending list of integers of the documents not included in term_docs.\n",
    "    \"\"\"\n",
    "    ndocs_term = freq\n",
    "    answer = []\n",
    "    i1 = 0\n",
    "    i2 = 1\n",
    "    while((i1<ndocs_term) and (i2<=num_documentos)):\n",
    "        if term_docs[i1]==i2:\n",
    "            i1+=1\n",
    "            i2+=1\n",
    "        elif term_docs[i1]<i2:\n",
    "            i1+=1\n",
    "        else:\n",
    "            answer.append(i2)\n",
    "            i2+=1\n",
    "    while( i2<=num_documentos):\n",
    "        answer.append(i2)\n",
    "        i2+=1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T23:28:08.807263Z",
     "start_time": "2021-03-13T23:28:08.788512Z"
    }
   },
   "outputs": [],
   "source": [
    "def leer_query(path):\n",
    "    \"\"\"\n",
    "    Reads a query\n",
    "    :param path: Path of the file of the query\n",
    "    :return: List of each line of a query\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "def leer_queries():\n",
    "    \"\"\"\n",
    "    Creates a dictionary with the terms of each query after applying the same process of tokenization, \n",
    "    stop words, normalization and lemmatization done to the inverted index.\n",
    "    :param :None\n",
    "    :return: Dictionary where each key is the id of a query and each value is \n",
    "    the list of terms of the query\n",
    "    \"\"\"\n",
    "    queries = {}\n",
    "    archivos_queries = glob.glob(os.path.join(*['docs', 'queries-raw-texts','*.naf']))\n",
    "    nltk_stop_words_en = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    p_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    for archivo in archivos_queries:\n",
    "        query_content = ' '.join(leer_query(archivo)).replace('\\n','')\n",
    "        id_query = re.search('<public publicId=\"(.*?)\" uri=\"(.*?)\"',query_content).group(1)\n",
    "        text_query = re.search('<raw><!\\[CDATA\\[(.*?)\\]\\]></raw>',query_content).group(1)\n",
    "        queries[id_query] = [wordnet_lemmatizer.lemmatize(token) \n",
    "                             for token in nltk.word_tokenize(text_query) \n",
    "                             if (token.isalnum()) and (token not in nltk_stop_words_en)]\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T23:28:09.740221Z",
     "start_time": "2021-03-13T23:28:09.709147Z"
    }
   },
   "outputs": [],
   "source": [
    "def conjunction_queries(inverted_index, queries):\n",
    "    \"\"\"\n",
    "    Calculates the conjunction binary query for each record in the dictionary of queries. \n",
    "    :param inverted_index: Dictionary of the inverted index\n",
    "    :param queries: Dictionary of the queries\n",
    "    :return: Dictionary with the result of the conjunction for each query\n",
    "    \"\"\"\n",
    "    respuesta = {}\n",
    "    for query in queries:\n",
    "        first_term = queries[query][0]\n",
    "        respuesta_query = []\n",
    "        respuesta_query_freq = 0\n",
    "        try:\n",
    "            respuesta_query = [item[0] for item in inverted_index[first_term]['posting']]\n",
    "            respuesta_query_freq = inverted_index[queries[query][0]]['freq']\n",
    "        except:\n",
    "            pass\n",
    "        for term in queries[query][1:]:\n",
    "            term_docs = []\n",
    "            term_freq = 0\n",
    "            try:\n",
    "                term_docs = [item[0] for item in inverted_index[term]['posting']]\n",
    "                term_freq = inverted_index[term]['freq']\n",
    "            except:\n",
    "                pass\n",
    "            respuesta_query = merge_and(respuesta_query, respuesta_query_freq, term_docs, term_freq ) \n",
    "            respuesta_query_freq = len(respuesta_query)\n",
    "            if respuesta_query_freq==0:\n",
    "                break\n",
    "        respuesta[query] = ','.join([ f\"d{doc:03d}\" for doc in respuesta_query])\n",
    "    respuesta = {i : respuesta[i] for i in sorted(respuesta)}\n",
    "    return respuesta\n",
    "def disjunction_queries(inverted_index, queries):\n",
    "    \"\"\"\n",
    "    Calculates the disjunctive binary query for each record in the dictionary of queries. \n",
    "    :param inverted_index: Dictionary of the inverted index\n",
    "    :param queries: Dictionary of the queries\n",
    "    :return: Dictionary with the result of the disjunction for each query\n",
    "    \"\"\"\n",
    "    respuesta = {}\n",
    "    for query in queries:\n",
    "        first_term = queries[query][0]\n",
    "        respuesta_query = []\n",
    "        respuesta_query_freq = 0\n",
    "        try:\n",
    "            respuesta_query = [item[0] for item in inverted_index[first_term]['posting']]\n",
    "            respuesta_query_freq = inverted_index[queries[query][0]]['freq']\n",
    "        except:\n",
    "            pass\n",
    "        for term in queries[query][1:]:\n",
    "            term_docs = []\n",
    "            term_freq = 0\n",
    "            try:\n",
    "                term_docs = [item[0] for item in inverted_index[term]['posting']]\n",
    "                term_freq = inverted_index[term]['freq']\n",
    "            except:\n",
    "                pass\n",
    "            respuesta_query = merge_or(respuesta_query, respuesta_query_freq, term_docs, term_freq ) \n",
    "            respuesta_query_freq = len(respuesta_query)\n",
    "\n",
    "        respuesta[query] = ','.join([ f\"d{doc:03d}\" for doc in respuesta_query])\n",
    "    respuesta = {i : respuesta[i] for i in sorted(respuesta)}\n",
    "    return respuesta\n",
    "def guardar_file(path, dict_resultado):\n",
    "    \"\"\"\n",
    "    Saves the given dictionary in tsv format \n",
    "    :param path: Path of the name fo the file\n",
    "    :param dict_resultado: Dictionary to be saved\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with open(path, \"w\") as record_file:\n",
    "        for key in dict_resultado:\n",
    "            record_file.write(f'{key}\\t{dict_resultado[key]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T23:28:13.022954Z",
     "start_time": "2021-03-13T23:28:10.987605Z"
    }
   },
   "outputs": [],
   "source": [
    "# se lee el inverted index\n",
    "ii = lectura_inverted_index(os.path.join('docs','inverted_index_lemma.pkl'))\n",
    "total_documentos = obtener_total_documentos(ii)\n",
    "# se leen las queries\n",
    "queries = leer_queries()\n",
    "# se calcula los queries binarios usando AND y OR.\n",
    "disj = disjunction_queries(ii, queries)\n",
    "conj = conjunction_queries(ii, queries)\n",
    "# se guardan los resultados\n",
    "guardar_file(os.path.join('docs', 'BSII-AND-queries_results.tsv'), conj)\n",
    "guardar_file(os.path.join('docs', 'BSII-OR-queries_results.tsv'), disj)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
