{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reddit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import ciso8601\n",
    "import requests #Pushshift accesses Reddit via an url so this is needed\n",
    "import json #JSON manipulation\n",
    "import datetime\n",
    "import time\n",
    "import jsonpickle\n",
    "import praw\n",
    "import sys\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## First Approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authentication"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('credentials/reddit_credentials') as f:\n",
    "  credentials2 = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=credentials2[\"client_id\"], client_secret=credentials2[\"client_secret\"], user_agent=credentials2['user_agent'])\n",
    "fName = 'data/reddit10000.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "\n",
    "hot_posts = reddit.subreddit('Covid').top(limit=10000)\n",
    "posts = 0\n",
    "comments = 0\n",
    "with open(fName, 'w') as f:\n",
    "    for post in hot_posts:\n",
    "        post.comments.replace_more(limit=None)\n",
    "        #print(\"Title\",post.title)\n",
    "        f.write(jsonpickle.encode(post.title, unpicklable=False) +'\\n')\n",
    "        #print(\"Self Test\",post.selftext)\n",
    "        f.write(jsonpickle.encode(post.selftext, unpicklable=False) +'\\n')\n",
    "        posts = posts +1\n",
    "        for top_level_comment in post.comments:\n",
    "            #print(\"C:\",top_level_comment.body)\n",
    "            f.write(jsonpickle.encode(top_level_comment.body, unpicklable=False) +'\\n')\n",
    "            comments = comments + 1\n",
    "\n",
    "\n",
    "print (\"Downloaded {0} posts and {1} comments, Saved to {2}\".format(posts,comments, fName))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=credentials2[\"client_id\"], client_secret=credentials2[\"client_secret\"], user_agent=credentials2['user_agent'])\n",
    "fName = 'data/reddit1.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "\n",
    "hot_posts = reddit.subreddit('Covid').top(limit=10000)\n",
    "posts = 0\n",
    "comments = 0\n",
    "with open(fName, 'w') as f:\n",
    "    for post in hot_posts:\n",
    "        post.comments.replace_more(limit=None)\n",
    "        #print(\"Title\",post.title)\n",
    "        f.write(jsonpickle.encode(post.title, unpicklable=False) +'\\n')\n",
    "        #print(\"Self Test\",post.selftext)\n",
    "        f.write(jsonpickle.encode(post.selftext, unpicklable=False) +'\\n')\n",
    "        posts = posts +1\n",
    "        for top_level_comment in post.comments:\n",
    "            #print(\"C:\",top_level_comment.body)\n",
    "            f.write(jsonpickle.encode(top_level_comment.body, unpicklable=False) +'\\n')\n",
    "            comments = comments + 1\n",
    "\n",
    "\n",
    "print (\"Downloaded {0} posts and {1} comments, Saved to {2}\".format(posts,comments, fName))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Second Approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Pushshift Module to extract Submissions Data from Reddit via Python"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pushshift get URL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "#Adapted from this https://gist.github.com/dylankilkenny/3dbf6123527260165f8c5c3bc3ee331b\n",
    "#This function builds an Pushshift URL, accesses the webpage and stores JSON data in a nested list\n",
    "def getPushshiftData(query, after, before, sub):\n",
    "    #Build URL\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    #Print URL to show user\n",
    "    print(url)\n",
    "    #Request URL\n",
    "    r = requests.get(url)\n",
    "    #Load JSON data from webpage into data variable\n",
    "    data = json.loads(r.text)\n",
    "    #return the data element which contains all the submissions data\n",
    "    return data['data']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract key information from Submissions\n",
    "\n",
    "We want key data for further analysis including:\n",
    "* Submission Title\n",
    "* Author\n",
    "* Submission post ID\n",
    "* Upload Time\n",
    "* Text\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "#This function will be used to extract the key data points from each JSON result\n",
    "def collectSubData(subm,id):\n",
    "    #subData was created at the start to hold all the data which is then added to our global subStats dictionary.\n",
    "    subData = {} #list to store data points\n",
    "    subData['id'] = id\n",
    "    subData['internal_id'] = subm['id']\n",
    "    subData['publication_date'] = int(subm['created_utc'])#1520561700.0\n",
    "    subData['source'] = \"Reddit\"\n",
    "    subData['author'] = subm['author']\n",
    "    subData['title'] = subm['title']\n",
    "    subData['text'] = subm['selftext']\n",
    "    #Put all data points into a tuple and append to subData\n",
    "    #Create a dictionary entry of current submission data and store all data related to it\n",
    "    subStats[id] = subData"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Search Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "#Create your timestamps and queries for your search URL\n",
    "#https://www.unixtimestamp.com/index.php > Use this to create your timestamps\n",
    "after = \"1577836800\" #int(time.mktime(ciso8601.parse_datetime(date_after).timetuple())) #Submissions after this timestamp\n",
    "before = \"1607040000\" #int(time.mktime(ciso8601.parse_datetime(date_after).timetuple())) #Submissions after this timestamp\n",
    "query = \"COVID\" #Keyword(s) to look for in submissions\n",
    "sub = \"COVID\" #Which Subreddit to search in\n",
    "\n",
    "#subCount tracks the no. of total submissions we collect\n",
    "subCount = 0\n",
    "#subStats is the dictionary where we will store our data.\n",
    "subStats = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1577836800&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-03-26 20:59:00\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1585274340&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-01 21:53:37\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1585796017&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-08 04:55:42\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1586339742&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-12 07:54:14\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1586696054&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-17 12:47:55\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1587145675&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-23 15:58:54\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1587675534&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-29 09:02:01\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1588168921&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-05-06 15:52:44\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1588798364&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-05-13 22:07:09\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1589425629&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-05-20 01:53:05\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1589957585&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-05-28 14:07:09\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1590692829&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-06-09 20:53:29\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1591754009&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-06-22 12:22:22\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1592846542&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-07-01 14:15:44\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1593630944&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-07-09 00:34:40\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1594272880&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-07-14 17:58:55\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1594767535&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-07-20 20:15:58\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1595294158&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-07-27 03:09:52\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1595837392&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-08-03 18:50:36\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1596498636&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-08-10 18:24:54\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1597101894&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-08-19 09:14:10\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1597846450&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-08-28 23:46:51\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1598676411&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-09-07 04:57:05\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1599472625&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-09-14 20:06:28\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1600131988&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-09-23 13:06:01\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1600884361&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-10-05 00:17:30\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1601875050&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-10-13 18:18:43\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1602631123&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-10-23 00:25:37\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1603430737&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-10-31 23:29:14\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1604204954&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-11-09 13:30:51\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1604946651&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-11-16 12:51:29\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1605549089&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-11-21 17:30:08\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1605997808&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-11-28 00:03:00\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1606539780&before=1607040000&subreddit=COVID\n",
      "98\n",
      "2020-12-03 11:15:13\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1607012113&before=1607040000&subreddit=COVID\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# We need to run this function outside the loop first to get the updated after variable\n",
    "data = getPushshiftData(query, after, before, sub)\n",
    "# Will run until all posts have been gathered i.e. When the length of data variable = 0\n",
    "# from the 'after' date up until before date\n",
    "id = 0\n",
    "while len(data) > 0: #The length of data is the number submissions (data[0], data[1] etc), once it hits zero (after and before vars are the same) end\n",
    "    for submission in data:\n",
    "        collectSubData(submission, id)\n",
    "        id+=1\n",
    "        subCount+=1\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    print(len(data))\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    #update after variable to last created date of submission\n",
    "    after = data[-1]['created_utc']\n",
    "    #data has changed due to the new after variable provided by above code\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "\n",
    "print(len(data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check your Submission Extraction was successful"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3398 submissions have added to list\n",
      "1st entry is:\n",
      "Coronavirus disease named Covid-19\n",
      "Last entry is:\n",
      "Covid Help - Testing - How To Properly Quarantine?\n"
     ]
    }
   ],
   "source": [
    "num_submissions = len(subStats)\n",
    "print(str(num_submissions) + \" submissions have added to list\")\n",
    "print(\"1st entry is:\")\n",
    "print(list(subStats.values())[0][\"title\"])\n",
    "print(\"Last entry is:\")\n",
    "print(list(subStats.values())[-1][\"title\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save data to JSON file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON created\n"
     ]
    }
   ],
   "source": [
    "def updateSubs_file():\n",
    "    ts = str(round(time.time() * 1000))\n",
    "    file = \"data/reddit_\" + str(num_submissions) +\"_\" + ts + \".JSON\"\n",
    "    with open(file, 'w') as file:\n",
    "        json.dump(subStats,file)\n",
    "    print(\"JSON created\")\n",
    "\n",
    "updateSubs_file()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comments\n",
    "\n",
    "(Not yet implemented)\n",
    "\n",
    "https://reddit-api.readthedocs.io/en/latest/index.html#comments-search\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subreddits = ['Coronavirus']\n",
    "ignored_users = ['[deleted]', 'automoderator']\n",
    "lookback_days = 180\n",
    "min_comments_per_sub = 1\n",
    "file_name = \"/data/reddint1.txt\"\n",
    "# either author, comments or leave it empty\n",
    "sort_by = \"\"\n",
    "\n",
    "url = \"https://api.pushshift.io/reddit/comment/search?&limit=1000&sort=desc&subreddit={}&before=\"\n",
    "\n",
    "startTime = datetime.utcnow()\n",
    "startEpoch = int(startTime.timestamp())\n",
    "endTime = startTime - timedelta(days=lookback_days)\n",
    "endEpoch = int(endTime.timestamp())\n",
    "totalSeconds = startEpoch - endEpoch\n",
    "\n",
    "\n",
    "def countCommenters(subreddit):\n",
    "\tcount = 0\n",
    "\tcommenters = defaultdict(int)\n",
    "\tpreviousEpoch = startEpoch\n",
    "\tprint(f\"Counting commenters in: {subreddit}\")\n",
    "\tbreakOut = False\n",
    "\twhile True:\n",
    "\t\tnewUrl = url.format(subreddit)+str(previousEpoch)\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(newUrl, headers={'User-Agent': \"Overlap counter by /u/Watchful1\"})\n",
    "\t\texcept requests.exceptions.ReadTimeout:\n",
    "\t\t\tprint(f\"Pushshift timeout, this usually means pushshift is down. Waiting 5 seconds and trying again: {newUrl}\")\n",
    "\t\t\ttime.sleep(5)\n",
    "\t\t\tcontinue\n",
    "\t\ttry:\n",
    "\t\t\tobjects = response.json()['data']\n",
    "\t\texcept json.decoder.JSONDecodeError:\n",
    "\t\t\tprint(f\"Decoding error, this usually means pushshift is down. Waiting 5 seconds and trying again: {newUrl}\")\n",
    "\t\t\ttime.sleep(5)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\ttime.sleep(1)  # pushshift is ratelimited. If we go too fast we'll get errors\n",
    "\n",
    "\t\tif len(objects) == 0:\n",
    "\t\t\tbreak\n",
    "\t\tfor object in objects:\n",
    "\t\t\tpreviousEpoch = object['created_utc'] - 1\n",
    "\t\t\tif object['author'] not in ignored_users:\n",
    "\t\t\t\tcommenters[object['author']] += 1\n",
    "\t\t\tcount += 1\n",
    "\t\t\tif count % 1000 == 0:\n",
    "\t\t\t\tprint(\"r/{0} comments: {1}, {2}, {3:.2f}%\".format(\n",
    "\t\t\t\t\tsubreddit,\n",
    "\t\t\t\t\tcount,\n",
    "\t\t\t\t\tdatetime.fromtimestamp(previousEpoch).strftime(\"%Y-%m-%d\"),\n",
    "\t\t\t\t\t((startEpoch - previousEpoch) / totalSeconds) * 100))\n",
    "\t\t\tif previousEpoch < endEpoch:\n",
    "\t\t\t\tbreakOut = True\n",
    "\t\t\t\tbreak\n",
    "\t\tif breakOut:\n",
    "\t\t\tbreak\n",
    "\tprint(f\"Comments: {count}, commenters: {len(commenters)}\")\n",
    "\treturn commenters\n",
    "\n",
    "\n",
    "totalCommenters = defaultdict(int)\n",
    "overlapCommenters = defaultdict(int)\n",
    "for subreddit in subreddits:\n",
    "\tcommenters = countCommenters(subreddit)\n",
    "\n",
    "\tif not len(overlapCommenters):\n",
    "\t\tprint(\"Building first\")\n",
    "\t\tfor commenter in commenters:\n",
    "\t\t\tif commenters[commenter] >= min_comments_per_sub:\n",
    "\t\t\t\toverlapCommenters[commenter] += commenters[commenter]\n",
    "\t\t\t\ttotalCommenters[commenter] += commenters[commenter]\n",
    "\t\tprint(f\"Done, size: {len(overlapCommenters)}\")\n",
    "\telse:\n",
    "\t\tprint(f\"Building, current size: {len(overlapCommenters)}\")\n",
    "\t\tnewOverlap = defaultdict(int)\n",
    "\t\tfor commenter in commenters:\n",
    "\t\t\tif commenters[commenter] >= min_comments_per_sub:\n",
    "\t\t\t\ttotalCommenters[commenter] += commenters[commenter]\n",
    "\t\t\t\tif commenter in overlapCommenters:\n",
    "\t\t\t\t\tnewOverlap[commenter] += commenters[commenter]\n",
    "\t\toverlapCommenters = newOverlap\n",
    "\t\tprint(f\"Done, new size: {len(overlapCommenters)}, total commenters: {len(totalCommenters)}\")\n",
    "\n",
    "\n",
    "print(f\"{len(overlapCommenters)} of {len(totalCommenters)} total commenters commented in all subreddits, that's {round((len(overlapCommenters) / len(totalCommenters)) * 100, 2)} percent\")\n",
    "\n",
    "with open(file_name, 'w') as txt:\n",
    "\tif sort_by == 'author':\n",
    "\t\tprint(f\"Printing to {file_name} sorted by author\")\n",
    "\t\tfor user in sorted(overlapCommenters.keys(), key=str.lower):\n",
    "\t\t\ttxt.write(f\"{user}: {overlapCommenters[user]}\\n\")\n",
    "\telif sort_by == 'comments':\n",
    "\t\tprint(f\"Printing to {file_name} sorted by number of comments\")\n",
    "\t\tfor user, comments in sorted(overlapCommenters.items(), key=lambda item: item[1], reverse=True):\n",
    "\t\t\ttxt.write(f\"{user}: {comments}\\n\")\n",
    "\telse:\n",
    "\t\tprint(f\"Printing to {file_name} unsorted\")\n",
    "\t\tfor user in overlapCommenters:\n",
    "\t\t\ttxt.write(f\"{user}: {overlapCommenters[user]}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}