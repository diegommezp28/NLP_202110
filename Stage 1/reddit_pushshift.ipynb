{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reddit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import ciso8601\n",
    "import requests #Pushshift accesses Reddit via an url so this is needed\n",
    "import json #JSON manipulation\n",
    "import datetime\n",
    "import time\n",
    "import jsonpickle\n",
    "import praw\n",
    "import sys\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## First Approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authentication"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('credentials/reddit_credentials') as f:\n",
    "  credentials2 = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=credentials2[\"client_id\"], client_secret=credentials2[\"client_secret\"], user_agent=credentials2['user_agent'])\n",
    "fName = 'data/reddit10000.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "\n",
    "hot_posts = reddit.subreddit('Covid').top(limit=10000)\n",
    "posts = 0\n",
    "comments = 0\n",
    "with open(fName, 'w') as f:\n",
    "    for post in hot_posts:\n",
    "        post.comments.replace_more(limit=None)\n",
    "        #print(\"Title\",post.title)\n",
    "        f.write(jsonpickle.encode(post.title, unpicklable=False) +'\\n')\n",
    "        #print(\"Self Test\",post.selftext)\n",
    "        f.write(jsonpickle.encode(post.selftext, unpicklable=False) +'\\n')\n",
    "        posts = posts +1\n",
    "        for top_level_comment in post.comments:\n",
    "            #print(\"C:\",top_level_comment.body)\n",
    "            f.write(jsonpickle.encode(top_level_comment.body, unpicklable=False) +'\\n')\n",
    "            comments = comments + 1\n",
    "\n",
    "\n",
    "print (\"Downloaded {0} posts and {1} comments, Saved to {2}\".format(posts,comments, fName))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=credentials2[\"client_id\"], client_secret=credentials2[\"client_secret\"], user_agent=credentials2['user_agent'])\n",
    "fName = 'data/reddit1.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "\n",
    "hot_posts = reddit.subreddit('Covid').top(limit=10000)\n",
    "posts = 0\n",
    "comments = 0\n",
    "with open(fName, 'w') as f:\n",
    "    for post in hot_posts:\n",
    "        post.comments.replace_more(limit=None)\n",
    "        #print(\"Title\",post.title)\n",
    "        f.write(jsonpickle.encode(post.title, unpicklable=False) +'\\n')\n",
    "        #print(\"Self Test\",post.selftext)\n",
    "        f.write(jsonpickle.encode(post.selftext, unpicklable=False) +'\\n')\n",
    "        posts = posts +1\n",
    "        for top_level_comment in post.comments:\n",
    "            #print(\"C:\",top_level_comment.body)\n",
    "            f.write(jsonpickle.encode(top_level_comment.body, unpicklable=False) +'\\n')\n",
    "            comments = comments + 1\n",
    "\n",
    "\n",
    "print (\"Downloaded {0} posts and {1} comments, Saved to {2}\".format(posts,comments, fName))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Second Approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Pushshift Module to extract Submissions Data from Reddit via Python"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pushshift get URL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "#Adapted from this https://gist.github.com/dylankilkenny/3dbf6123527260165f8c5c3bc3ee331b\n",
    "#This function builds an Pushshift URL, accesses the webpage and stores JSON data in a nested list\n",
    "def getPushshiftData(query, after, before, sub):\n",
    "    #Build URL\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    #Print URL to show user\n",
    "    print(url)\n",
    "    #Request URL\n",
    "    r = requests.get(url)\n",
    "    #Load JSON data from webpage into data variable\n",
    "    data = json.loads(r.text)\n",
    "    #return the data element which contains all the submissions data\n",
    "    return data['data']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract key information from Submissions\n",
    "\n",
    "We want key data for further analysis including:\n",
    "* Submission Title\n",
    "* Author\n",
    "* Submission post ID\n",
    "* Upload Time\n",
    "* Text\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "#This function will be used to extract the key data points from each JSON result\n",
    "def collectSubData(subm,id):\n",
    "    #subData was created at the start to hold all the data which is then added to our global subStats dictionary.\n",
    "    subData = {} #list to store data points\n",
    "    subData['id'] = id\n",
    "    subData['internal_id'] = subm['id']\n",
    "    subData['publication_date'] = int(subm['created_utc'])#1520561700.0\n",
    "    subData['source'] = \"Reddit\"\n",
    "    subData['author'] = subm['author']\n",
    "    subData['title'] = subm['title']\n",
    "    subData['text'] = subm['selftext']\n",
    "    #Put all data points into a tuple and append to subData\n",
    "    #Create a dictionary entry of current submission data and store all data related to it\n",
    "    subStats[id] = subData"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Search Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "#Create your timestamps and queries for your search URL\n",
    "#https://www.unixtimestamp.com/index.php > Use this to create your timestamps\n",
    "after = \"1577836800\" #int(time.mktime(ciso8601.parse_datetime(date_after).timetuple())) #Submissions after this timestamp\n",
    "before = \"1607040000\" #int(time.mktime(ciso8601.parse_datetime(date_after).timetuple())) #Submissions after this timestamp\n",
    "query = \"COVID\" #Keyword(s) to look for in submissions\n",
    "sub = \"COVID\" #Which Subreddit to search in\n",
    "\n",
    "#subCount tracks the no. of total submissions we collect\n",
    "subCount = 0\n",
    "#subStats is the dictionary where we will store our data.\n",
    "subStats = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1577836800&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-03-26 20:59:00\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1585274340&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-01 21:53:37\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1585796017&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-08 04:55:42\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1586339742&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-12 07:54:14\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1586696054&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-17 12:47:55\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1587145675&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-23 15:58:54\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1587675534&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-04-29 09:02:01\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1588168921&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-05-06 15:52:44\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1588798364&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-05-13 22:07:09\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1589425629&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-05-20 01:53:05\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1589957585&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-05-28 14:07:09\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1590692829&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-06-09 20:53:29\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1591754009&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-06-22 12:22:22\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1592846542&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-07-01 14:15:44\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1593630944&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-07-09 00:34:40\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1594272880&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-07-14 17:58:55\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1594767535&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-07-20 20:15:58\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1595294158&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-07-27 03:09:52\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1595837392&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-08-03 18:50:36\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1596498636&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-08-10 18:24:54\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1597101894&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-08-19 09:14:10\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1597846450&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-08-28 23:46:51\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1598676411&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-09-07 04:57:05\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1599472625&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-09-14 20:06:28\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1600131988&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-09-23 13:06:01\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1600884361&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-10-05 00:17:30\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1601875050&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-10-13 18:18:43\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1602631123&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-10-23 00:25:37\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1603430737&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-10-31 23:29:14\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1604204954&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-11-09 13:30:51\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1604946651&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-11-16 12:51:29\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1605549089&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-11-21 17:30:08\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1605997808&before=1607040000&subreddit=COVID\n",
      "100\n",
      "2020-11-28 00:03:00\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1606539780&before=1607040000&subreddit=COVID\n",
      "98\n",
      "2020-12-03 11:15:13\n",
      "https://api.pushshift.io/reddit/search/submission/?title=COVID&size=1000&after=1607012113&before=1607040000&subreddit=COVID\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# We need to run this function outside the loop first to get the updated after variable\n",
    "data = getPushshiftData(query, after, before, sub)\n",
    "# Will run until all posts have been gathered i.e. When the length of data variable = 0\n",
    "# from the 'after' date up until before date\n",
    "id = 0\n",
    "while len(data) > 0: #The length of data is the number submissions (data[0], data[1] etc), once it hits zero (after and before vars are the same) end\n",
    "    for submission in data:\n",
    "        collectSubData(submission, id)\n",
    "        id+=1\n",
    "        subCount+=1\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    print(len(data))\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    #update after variable to last created date of submission\n",
    "    after = data[-1]['created_utc']\n",
    "    #data has changed due to the new after variable provided by above code\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "\n",
    "print(len(data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check your Submission Extraction was successful"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3398 submissions have added to list\n",
      "1st entry is:\n",
      "Coronavirus disease named Covid-19\n",
      "Last entry is:\n",
      "Covid Help - Testing - How To Properly Quarantine?\n"
     ]
    }
   ],
   "source": [
    "num_submissions = len(subStats)\n",
    "print(str(num_submissions) + \" submissions have added to list\")\n",
    "print(\"1st entry is:\")\n",
    "print(list(subStats.values())[0][\"title\"])\n",
    "print(\"Last entry is:\")\n",
    "print(list(subStats.values())[-1][\"title\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save data to JSON file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON created\n"
     ]
    }
   ],
   "source": [
    "def updateSubs_file():\n",
    "    ts = str(round(time.time() * 1000))\n",
    "    file = \"data/reddit_\" + str(num_submissions) +\"_\" + ts + \".JSON\"\n",
    "    with open(file, 'w') as file:\n",
    "        json.dump(subStats,file)\n",
    "    print(\"JSON created\")\n",
    "\n",
    "updateSubs_file()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Second approach\n",
    "Taken from: https://github.com/Watchful1/Sketchpad/blob/master/postDownloader.py\n",
    "https://www.reddit.com/r/pushshift/comments/ffzyjj/get_all_comments_from_specific_subreddit/\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving submissions to data/reddit_italian_posts.txt\n",
      "Saved 100 submissions through 2020-11-24\n",
      "Saved 200 submissions through 2020-05-14\n",
      "Saved 300 submissions through 2020-04-29\n",
      "Saved 400 submissions through 2020-04-20\n",
      "Saved 500 submissions through 2020-04-07\n",
      "Saved 600 submissions through 2020-04-01\n",
      "Saved 700 submissions through 2020-03-25\n",
      "Saved 800 submissions through 2020-03-22\n",
      "Saved 900 submissions through 2020-03-18\n",
      "Saved 1000 submissions through 2020-03-14\n",
      "Saved 1100 submissions through 2020-03-10\n",
      "Saved 1200 submissions through 2020-03-06\n",
      "Saved 1300 submissions through 2020-02-28\n",
      "Saved 1400 submissions through 2020-02-24\n",
      "Saved 1500 submissions through 2020-02-21\n",
      "Saved 1600 submissions through 2020-02-11\n",
      "Saved 1660 submissions through 2020-02-06\n",
      "Saved 1660 submissions\n",
      "Saving comments to data/reddit_italian_comments.txt\n",
      "Saved 100 comments through 2021-02-01\n",
      "Saved 200 comments through 2021-01-03\n",
      "Saved 300 comments through 2020-11-30\n",
      "Saved 400 comments through 2020-11-13\n",
      "Saved 500 comments through 2020-10-28\n",
      "Saved 600 comments through 2020-10-17\n",
      "Saved 700 comments through 2020-05-17\n",
      "Saved 800 comments through 2020-05-03\n",
      "Saved 900 comments through 2020-04-28\n",
      "Saved 1000 comments through 2020-04-18\n",
      "Saved 1100 comments through 2020-04-10\n",
      "Saved 1200 comments through 2020-04-05\n",
      "Saved 1300 comments through 2020-04-01\n",
      "Saved 1400 comments through 2020-03-29\n",
      "Saved 1500 comments through 2020-03-27\n",
      "Saved 1600 comments through 2020-03-25\n",
      "Saved 1700 comments through 2020-03-23\n",
      "Saved 1800 comments through 2020-03-22\n",
      "Saved 1900 comments through 2020-03-21\n",
      "Saved 2000 comments through 2020-03-20\n",
      "Saved 2100 comments through 2020-03-18\n",
      "Saved 2200 comments through 2020-03-16\n",
      "Saved 2300 comments through 2020-03-14\n",
      "Saved 2400 comments through 2020-03-12\n",
      "Saved 2500 comments through 2020-03-11\n",
      "Saved 2600 comments through 2020-03-09\n",
      "Saved 2700 comments through 2020-03-08\n",
      "Saved 2800 comments through 2020-03-07\n",
      "Saved 2900 comments through 2020-03-06\n",
      "Saved 3000 comments through 2020-03-04\n",
      "Saved 3100 comments through 2020-02-29\n",
      "Saved 3200 comments through 2020-02-28\n",
      "Saved 3300 comments through 2020-02-26\n",
      "Saved 3400 comments through 2020-02-25\n",
      "Saved 3500 comments through 2020-02-24\n",
      "Saved 3600 comments through 2020-02-22\n",
      "Saved 3700 comments through 2020-02-21\n",
      "Saved 3800 comments through 2020-02-15\n",
      "Saved 3891 comments through 2020-02-06\n",
      "Saved 3891 comments\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "username = \"\"  # put the username you want to download in the quotes\n",
    "subreddit = \"CoronaVirus_ITALIA\"  # put the subreddit you want to download in the quotes\n",
    "# English Coronavirus\n",
    "# Spanish covidmx\n",
    "# Italian CoronaVirus_ITALIA\n",
    "\n",
    "# leave either one blank to download an entire user's or subreddit's history\n",
    "# or fill in both to download a specific users history from a specific subreddit\n",
    "\n",
    "filter_string = None\n",
    "if username == \"\" and subreddit == \"\":\n",
    "\tprint(\"Fill in either username or subreddit\")\n",
    "\tsys.exit(0)\n",
    "elif username == \"\" and subreddit != \"\":\n",
    "\tfilter_string = f\"subreddit={subreddit}\"\n",
    "elif username != \"\" and subreddit == \"\":\n",
    "\tfilter_string = f\"author={username}\"\n",
    "else:\n",
    "\tfilter_string = f\"author={username}&subreddit={subreddit}\"\n",
    "\n",
    "url = \"https://api.pushshift.io/reddit/{}/search?limit=1000&sort=desc&{}&before=\"\n",
    "\n",
    "start_time = datetime.utcnow()\n",
    "\n",
    "\n",
    "def downloadFromUrl(filename, object_type):\n",
    "\tprint(f\"Saving {object_type}s to {filename}\")\n",
    "\n",
    "\tcount = 0\n",
    "\thandle = open(filename, 'w')\n",
    "\tprevious_epoch = int(start_time.timestamp())\n",
    "\twhile True:\n",
    "\t\tnew_url = url.format(object_type, filter_string)+str(previous_epoch)\n",
    "\t\tjson_text = requests.get(new_url, headers={'User-Agent': \"Post downloader by /u/Watchful1\"})\n",
    "\t\ttime.sleep(1)  # pushshift has a rate limit, if we send requests too fast it will start returning error messages\n",
    "\t\ttry:\n",
    "\t\t\tjson_data = json_text.json()\n",
    "\t\texcept json.decoder.JSONDecodeError:\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif 'data' not in json_data:\n",
    "\t\t\tbreak\n",
    "\t\tobjects = json_data['data']\n",
    "\t\tif len(objects) == 0:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tfor object in objects:\n",
    "\t\t\tprevious_epoch = object['created_utc'] - 1\n",
    "\t\t\tcount += 1\n",
    "\t\t\tif object_type == 'comment':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\thandle.write(str(object['score']))\n",
    "\t\t\t\t\thandle.write(\" : \")\n",
    "\t\t\t\t\thandle.write(datetime.fromtimestamp(object['created_utc']).strftime(\"%Y-%m-%d\"))\n",
    "\t\t\t\t\thandle.write(\"\\n\")\n",
    "\t\t\t\t\thandle.write(object['body'].encode(encoding='ascii', errors='ignore').decode())\n",
    "\t\t\t\t\thandle.write(\"\\n-------------------------------\\n\")\n",
    "\t\t\t\texcept Exception as err:\n",
    "\t\t\t\t\tprint(f\"Couldn't print comment: https://www.reddit.com{object['permalink']}\")\n",
    "\t\t\t\t\tprint(traceback.format_exc())\n",
    "\t\t\telif object_type == 'submission':\n",
    "\t\t\t\tif object['is_self']:\n",
    "\t\t\t\t\tif 'selftext' not in object:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\thandle.write(str(object['score']))\n",
    "\t\t\t\t\t\thandle.write(\" : \")\n",
    "\t\t\t\t\t\thandle.write(datetime.fromtimestamp(object['created_utc']).strftime(\"%Y-%m-%d\"))\n",
    "\t\t\t\t\t\thandle.write(\"\\n\")\n",
    "\t\t\t\t\t\thandle.write(object['selftext'].encode(encoding='ascii', errors='ignore').decode())\n",
    "\t\t\t\t\t\thandle.write(\"\\n-------------------------------\\n\")\n",
    "\t\t\t\t\texcept Exception as err:\n",
    "\t\t\t\t\t\tprint(f\"Couldn't print post: {object['url']}\")\n",
    "\t\t\t\t\t\tprint(traceback.format_exc())\n",
    "\n",
    "\t\tprint(\"Saved {} {}s through {}\".format(count, object_type, datetime.fromtimestamp(previous_epoch).strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\tprint(f\"Saved {count} {object_type}s\")\n",
    "\thandle.close()\n",
    "\n",
    "\n",
    "downloadFromUrl(\"data/reddit_italian_posts.txt\", \"submission\")\n",
    "downloadFromUrl(\"data/reddit_italian_comments.txt\", \"comment\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}