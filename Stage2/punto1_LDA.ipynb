{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Punto 1 - LDA Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\diego\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "import ssl\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "italian_path = 'datasets/italian/italian_out_hash_bien.json'\n",
    "spanish_path = 'datasets/spanish/spanish_out_hash_bien.json'\n",
    "english_path = 'datasets/english/english_out_hash_bien.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/isabelasarmiento/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/isabelasarmiento/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/isabelasarmiento/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "     _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "     pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')#%% md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784932, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                    id  publication_date   source  \\\n0  1295929115770593287      1.597809e+09  twitter   \n1  1296738518216011777      1.598002e+09  twitter   \n2  1252450676015198210      1.587442e+09  twitter   \n3  1380684968880406528      1.618016e+09  twitter   \n4  1368958702150156290      1.615220e+09  twitter   \n5  1317169175203401735      1.602873e+09  twitter   \n6  1288154256449708032      1.595955e+09  twitter   \n7  1283867253222502400      1.594933e+09  twitter   \n8  1286909343909240832      1.595658e+09  twitter   \n9  1235895985009811461      1.583496e+09  twitter   \n\n                                                text  \n0                                      Info Source:   \n1  #PostponeJEE_NEETSept #ProtestAgainstExamsInCO...  \n2  Coronavirus-spreader Chris Cuomo got a lecture...  \n3  Any military member that refuses to get vaccin...  \n4  #Covid19 is staying around for a while.  your ...  \n5                                 LIES!!!   LIES!!!   \n6  @GregMannarino Deborah BirxWhite House \"Expert\"    \n7  Kayleigh McEnany: ‘Science Should Not Stand in...  \n8  Amazing effort from the guys! Please donate if...  \n9  @SulaiOdus They said it was suspended due to c...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>publication_date</th>\n      <th>source</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1295929115770593287</td>\n      <td>1.597809e+09</td>\n      <td>twitter</td>\n      <td>Info Source:</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1296738518216011777</td>\n      <td>1.598002e+09</td>\n      <td>twitter</td>\n      <td>#PostponeJEE_NEETSept #ProtestAgainstExamsInCO...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1252450676015198210</td>\n      <td>1.587442e+09</td>\n      <td>twitter</td>\n      <td>Coronavirus-spreader Chris Cuomo got a lecture...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1380684968880406528</td>\n      <td>1.618016e+09</td>\n      <td>twitter</td>\n      <td>Any military member that refuses to get vaccin...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1368958702150156290</td>\n      <td>1.615220e+09</td>\n      <td>twitter</td>\n      <td>#Covid19 is staying around for a while.  your ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1317169175203401735</td>\n      <td>1.602873e+09</td>\n      <td>twitter</td>\n      <td>LIES!!!   LIES!!!</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1288154256449708032</td>\n      <td>1.595955e+09</td>\n      <td>twitter</td>\n      <td>@GregMannarino Deborah BirxWhite House \"Expert\"</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1283867253222502400</td>\n      <td>1.594933e+09</td>\n      <td>twitter</td>\n      <td>Kayleigh McEnany: ‘Science Should Not Stand in...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1286909343909240832</td>\n      <td>1.595658e+09</td>\n      <td>twitter</td>\n      <td>Amazing effort from the guys! Please donate if...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1235895985009811461</td>\n      <td>1.583496e+09</td>\n      <td>twitter</td>\n      <td>@SulaiOdus They said it was suspended due to c...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# english_path\n",
    "file = open(english_path, 'r')\n",
    "data = []\n",
    "for line in file:\n",
    "    data.append(json.loads(line))\n",
    "english_raw = pd.json_normalize(data)\n",
    "\n",
    "print(english_raw.shape)\n",
    "english_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Info Source: '"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Turn text to Numpy Array\n",
    "'''\n",
    "texts_column = english_raw.loc[:,'text']\n",
    "raw_texts = texts_column.values\n",
    "raw_texts[0]\n",
    "\n",
    "\n",
    "## Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in raw_texts:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['info', 'sourc'], ['work'], ['coronavirus', 'spreader', 'chris', 'cuomo', 'lectur', 'break', 'quarantin', 'upset'], ['militari', 'member', 'refus', 'vaccin', 'baffl', 'shit', 'pump'], ['covid', 'stay', 'busi', 'covidsaf'], ['lie', 'lie'], ['gregmannarino', 'deborah', 'birxwhit', 'hous', 'expert'], ['kayleigh', 'mcenani', 'scienc', 'stand', 'reopen', 'school'], ['amaz', 'effort', 'guy', 'donat'], ['sulaiodus', 'say', 'suspend', 'coronavirus', 'media']]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview 'processed_docs'\n",
    "'''\n",
    "print(processed_docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears\n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# LDA multicore\n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 8,\n",
    "                                   id2word = dictionary,\n",
    "                                   passes = 10,\n",
    "                                   workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "\n",
    "with open(\"results_punto1/eng_lda\", \"wb\") as output_file:\n",
    "    pk.dump(lda_model, output_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.040*\"mask\" + 0.030*\"spread\" + 0.020*\"wear\" + 0.019*\"face\" + 0.015*\"social\" + 0.012*\"hous\" + 0.011*\"question\" + 0.011*\"white\" + 0.010*\"hand\" + 0.010*\"stop\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.047*\"trump\" + 0.023*\"peopl\" + 0.021*\"realdonaldtrump\" + 0.017*\"know\" + 0.014*\"think\" + 0.013*\"presid\" + 0.013*\"say\" + 0.012*\"go\" + 0.012*\"american\" + 0.010*\"die\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.024*\"like\" + 0.018*\"thank\" + 0.018*\"good\" + 0.016*\"virus\" + 0.014*\"time\" + 0.014*\"look\" + 0.014*\"peopl\" + 0.012*\"come\" + 0.011*\"news\" + 0.009*\"hope\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.018*\"pandem\" + 0.015*\"school\" + 0.013*\"work\" + 0.010*\"stay\" + 0.009*\"home\" + 0.009*\"chang\" + 0.008*\"student\" + 0.008*\"communiti\" + 0.008*\"impact\" + 0.008*\"safe\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.018*\"pandem\" + 0.013*\"busi\" + 0.011*\"help\" + 0.010*\"govern\" + 0.009*\"crisi\" + 0.009*\"support\" + 0.009*\"need\" + 0.008*\"economi\" + 0.008*\"relief\" + 0.008*\"fund\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.073*\"vaccin\" + 0.020*\"health\" + 0.014*\"patient\" + 0.012*\"say\" + 0.011*\"studi\" + 0.010*\"hospit\" + 0.010*\"effect\" + 0.010*\"medic\" + 0.008*\"expert\" + 0.008*\"develop\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.083*\"case\" + 0.050*\"death\" + 0.033*\"report\" + 0.024*\"state\" + 0.024*\"updat\" + 0.018*\"number\" + 0.013*\"total\" + 0.013*\"confirm\" + 0.012*\"record\" + 0.012*\"daili\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.074*\"test\" + 0.035*\"posit\" + 0.012*\"home\" + 0.010*\"lockdown\" + 0.010*\"quarantin\" + 0.010*\"travel\" + 0.010*\"nurs\" + 0.009*\"south\" + 0.009*\"news\" + 0.008*\"close\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(845125, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                    id  publication_date   source  \\\n0  1304391319972790274      1.599826e+09  twitter   \n1  1308823479077175297      1.600883e+09  twitter   \n2  1255929719893221377      1.588272e+09  twitter   \n3  1367024221319286784      1.614759e+09  twitter   \n4  1278689023087849480      1.593698e+09  twitter   \n5  1374234992969015297      1.616478e+09  twitter   \n6  1296604758732570625      1.597970e+09  twitter   \n7  1344926278592438272      1.609490e+09  twitter   \n8  1311046706809786372      1.601413e+09  twitter   \n9  1257004539875667971      1.588528e+09  twitter   \n\n                                                text  \n0  ORACIÓN DIARIAViernes 11 de Septiembre 2020#or...  \n1  Se pudrió todo. Acá la FIFA debe castigar a la...  \n2   Última  Publicación  en la Prensaldia -   @ca...  \n3  Australia = 0 positivos por coronavirus.¿Vacun...  \n4                   coronavirus esto ya es personal   \n5        #LadyZopilota, zopiloteando en la noticia.   \n6  La noticia que esperaban los mercados. Gracias...  \n7  No caigamos en la trampa.En México ya iniciaro...  \n8  El coronavirus se ha confirmado ya en más de 2...  \n9  @JaimeChincha @RPPNoticias Mi padre acaba de m...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>publication_date</th>\n      <th>source</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1304391319972790274</td>\n      <td>1.599826e+09</td>\n      <td>twitter</td>\n      <td>ORACIÓN DIARIAViernes 11 de Septiembre 2020#or...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1308823479077175297</td>\n      <td>1.600883e+09</td>\n      <td>twitter</td>\n      <td>Se pudrió todo. Acá la FIFA debe castigar a la...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1255929719893221377</td>\n      <td>1.588272e+09</td>\n      <td>twitter</td>\n      <td>Última  Publicación  en la Prensaldia -   @ca...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1367024221319286784</td>\n      <td>1.614759e+09</td>\n      <td>twitter</td>\n      <td>Australia = 0 positivos por coronavirus.¿Vacun...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1278689023087849480</td>\n      <td>1.593698e+09</td>\n      <td>twitter</td>\n      <td>coronavirus esto ya es personal</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1374234992969015297</td>\n      <td>1.616478e+09</td>\n      <td>twitter</td>\n      <td>#LadyZopilota, zopiloteando en la noticia.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1296604758732570625</td>\n      <td>1.597970e+09</td>\n      <td>twitter</td>\n      <td>La noticia que esperaban los mercados. Gracias...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1344926278592438272</td>\n      <td>1.609490e+09</td>\n      <td>twitter</td>\n      <td>No caigamos en la trampa.En México ya iniciaro...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1311046706809786372</td>\n      <td>1.601413e+09</td>\n      <td>twitter</td>\n      <td>El coronavirus se ha confirmado ya en más de 2...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1257004539875667971</td>\n      <td>1.588528e+09</td>\n      <td>twitter</td>\n      <td>@JaimeChincha @RPPNoticias Mi padre acaba de m...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spanish_path\n",
    "file = open(spanish_path, 'r')\n",
    "data = []\n",
    "for line in file:\n",
    "    data.append(json.loads(line))\n",
    "text_raw = pd.json_normalize(data)\n",
    "\n",
    "print(text_raw.shape)\n",
    "text_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'ORACIÓN DIARIAViernes 11 de Septiembre 2020#oraciondiaria #11DeSeptiembre #BuenosDias #BuenosDiasATodos… '"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Turn text to Numpy Array\n",
    "'''\n",
    "texts_column = text_raw.loc[:,'text']\n",
    "raw_texts = texts_column.values\n",
    "raw_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-d2c412fb643d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mraw_texts\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mprocessed_docs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpreprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-18-4aa83b457db2>\u001B[0m in \u001B[0;36mpreprocess\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mpreprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0mresult\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mgensim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msimple_preprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mgensim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparsing\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSTOPWORDS\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m             \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlemmatize_stemming\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pythonProject/venv/lib/python3.8/site-packages/gensim/utils.py\u001B[0m in \u001B[0;36msimple_preprocess\u001B[0;34m(doc, deacc, min_len, max_len)\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m     \"\"\"\n\u001B[0;32m--> 310\u001B[0;31m     tokens = [\n\u001B[0m\u001B[1;32m    311\u001B[0m         \u001B[0mtoken\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlower\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdeacc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdeacc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'ignore'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    312\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmin_len\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mmax_len\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mtoken\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'_'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pythonProject/venv/lib/python3.8/site-packages/gensim/utils.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m     \"\"\"\n\u001B[0;32m--> 310\u001B[0;31m     tokens = [\n\u001B[0m\u001B[1;32m    311\u001B[0m         \u001B[0mtoken\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlower\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdeacc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdeacc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'ignore'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    312\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmin_len\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mmax_len\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mtoken\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'_'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pythonProject/venv/lib/python3.8/site-packages/gensim/utils.py\u001B[0m in \u001B[0;36msimple_tokenize\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m    282\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m     \"\"\"\n\u001B[0;32m--> 284\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mmatch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mPAT_ALPHABETIC\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinditer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    285\u001B[0m         \u001B[0;32myield\u001B[0m \u001B[0mmatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    286\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in raw_texts:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Preview 'processed_docs'\n",
    "'''\n",
    "print(processed_docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears\n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# LDA multicore\n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "\n",
    "num_topics = 8\n",
    "\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = num_topics,\n",
    "                                   id2word = dictionary,\n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "'''\n",
    "Guardarlo en  una variable especifica\n",
    "'''\n",
    "def savePickle(file,obj):\n",
    "    outfile = open(file,'wb')\n",
    "    pickle.dump(obj,outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Guardarlo en  una variable especifica\n",
    "'''\n",
    "\n",
    "ita_lda_model = lda_model\n",
    "savePickle(\"results_punto1/italian_lda\", ita_lda_model )\n",
    "\n",
    "ita_bow_corpus = bow_corpus\n",
    "savePickle(\"results_punto1/ita_bow_corpus\", ita_bow_corpus )\n",
    "\n",
    "ita_dictionary = dictionary\n",
    "savePickle(\"results_punto1/ita_dictionary\", ita_dictionary )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  Graphical Representantion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pyLDAvis\n",
    "import os\n",
    "import pyLDAvis.gensim_models\n",
    "#import pyLDAvis.gensim\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "num_topics = 8\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results_punto1/ldavis_tuned_'+str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results_punto1/ldavis_tuned_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# spanish_path\n",
    "file = open(italian_path, 'r')\n",
    "data = []\n",
    "for line in file:\n",
    "    data.append(json.loads(line))\n",
    "text_raw = pd.json_normalize(data)\n",
    "\n",
    "print(text_raw.shape)\n",
    "text_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Turn text to Numpy Array\n",
    "'''\n",
    "texts_column = text_raw.loc[:,'text']\n",
    "raw_texts = texts_column.values\n",
    "raw_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"italian\")\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in raw_texts:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Preview 'processed_docs'\n",
    "'''\n",
    "print(processed_docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears\n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# LDA multicore\n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 8,\n",
    "                                   id2word = dictionary,\n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPickle(path):\n",
    "    with open(path, \"rb\") as output_file:\n",
    "        return pk.load(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = loadPickle('./results_punto1/italian_lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 \nWords: 0.046*\"coronavirus\" + 0.020*\"com\" + 0.018*\"per\" + 0.016*\"hac\" + 0.015*\"par\" + 0.015*\"esta\" + 0.014*\"tod\" + 0.013*\"esto\" + 0.012*\"tien\" + 0.012*\"pas\"\n\n\nTopic: 1 \nWords: 0.059*\"par\" + 0.051*\"cov\" + 0.016*\"med\" + 0.016*\"salud\" + 0.011*\"ante\" + 0.010*\"coronavirus\" + 0.009*\"trabaj\" + 0.009*\"prueb\" + 0.009*\"sanitari\" + 0.008*\"com\"\n\n\nTopic: 2 \nWords: 0.085*\"coronavirus\" + 0.027*\"cov\" + 0.015*\"chin\" + 0.015*\"pandemi\" + 0.012*\"nuev\" + 0.011*\"estad\" + 0.010*\"unid\" + 0.009*\"españ\" + 0.008*\"crisis\" + 0.008*\"mund\"\n\n\nTopic: 3 \nWords: 0.057*\"cov\" + 0.024*\"coronavirus\" + 0.022*\"pacient\" + 0.020*\"hospital\" + 0.020*\"posit\" + 0.015*\"medic\" + 0.010*\"president\" + 0.009*\"sintom\" + 0.009*\"mur\" + 0.008*\"años\"\n\n\nTopic: 4 \nWords: 0.072*\"vacun\" + 0.068*\"cov\" + 0.064*\"contr\" + 0.033*\"par\" + 0.022*\"coronavirus\" + 0.010*\"millon\" + 0.009*\"primer\" + 0.008*\"luch\" + 0.008*\"recib\" + 0.007*\"investig\"\n\n\nTopic: 5 \nWords: 0.029*\"tod\" + 0.024*\"cov\" + 0.021*\"esta\" + 0.013*\"nuestr\" + 0.013*\"graci\" + 0.012*\"par\" + 0.012*\"este\" + 0.011*\"coronavirus\" + 0.011*\"vid\" + 0.010*\"com\"\n\n\nTopic: 6 \nWords: 0.030*\"coronavirus\" + 0.018*\"cov\" + 0.014*\"buen\" + 0.013*\"niñ\" + 0.010*\"despues\" + 0.008*\"mes\" + 0.008*\"clas\" + 0.008*\"estudi\" + 0.007*\"social\" + 0.007*\"llev\"\n\n\nTopic: 7 \nWords: 0.077*\"cas\" + 0.075*\"cov\" + 0.057*\"coronavirus\" + 0.042*\"nuev\" + 0.034*\"muert\" + 0.025*\"contagi\" + 0.018*\"confirm\" + 0.016*\"fallec\" + 0.016*\"ultim\" + 0.014*\"registr\"\n\n\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd06c70104609522f67cb30e8200dfa6f77a5bec0e1b1538a23f62dccdf26f51f37",
   "display_name": "Python 3.7.10 64-bit ('TF-Keras': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}