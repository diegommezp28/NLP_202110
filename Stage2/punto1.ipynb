{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Punto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/isabelasarmiento/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/isabelasarmiento/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/isabelasarmiento/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "     _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "     pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')#%% md\n",
    "\n",
    "## Reading Files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "italian_path = './datasets/italian/italianV1.json'\n",
    "spanish_path = './datasets/spanish/spanishV1.json'\n",
    "english_path = './datasets/english/englishV1.json'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(817046, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                         id  publication_date   source  \\\n0       1295929115770593287      1.597809e+09  twitter   \n1       1296738518216011777      1.598002e+09  twitter   \n2       1252450676015198210      1.587442e+09  twitter   \n3       1380684968880406528      1.618016e+09  twitter   \n4       1368958702150156290      1.615220e+09  twitter   \n...                     ...               ...      ...   \n817041  1270423398120935426      1.591728e+09  twitter   \n817042  1283969389117792258      1.594957e+09  twitter   \n817043  1309834472053903360      1.601124e+09  twitter   \n817044  1279062456468492289      1.593787e+09  twitter   \n817045  1274642711623188480      1.592733e+09  twitter   \n\n                                                     text  \n0                                           Info Source:   \n1       #PostponeJEE_NEETSept #ProtestAgainstExamsInCO...  \n2       Coronavirus-spreader Chris Cuomo got a lecture...  \n3       Any military member that refuses to get vaccin...  \n4       #Covid19 is staying around for a while.  your ...  \n...                                                   ...  \n817041  Children receiving free school meals, from sin...  \n817042  L.A. County reports highest number of coronavi...  \n817043           Progress ?Covid19 Political Development   \n817044  @WeeklyEconPod @AyeishaTS @JCWI_UK @ZoeJardini...  \n817045  @mskathleenquinn @Babak_Javid_Lab @UCSF @Bob_W...  \n\n[817046 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>publication_date</th>\n      <th>source</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1295929115770593287</td>\n      <td>1.597809e+09</td>\n      <td>twitter</td>\n      <td>Info Source:</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1296738518216011777</td>\n      <td>1.598002e+09</td>\n      <td>twitter</td>\n      <td>#PostponeJEE_NEETSept #ProtestAgainstExamsInCO...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1252450676015198210</td>\n      <td>1.587442e+09</td>\n      <td>twitter</td>\n      <td>Coronavirus-spreader Chris Cuomo got a lecture...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1380684968880406528</td>\n      <td>1.618016e+09</td>\n      <td>twitter</td>\n      <td>Any military member that refuses to get vaccin...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1368958702150156290</td>\n      <td>1.615220e+09</td>\n      <td>twitter</td>\n      <td>#Covid19 is staying around for a while.  your ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>817041</th>\n      <td>1270423398120935426</td>\n      <td>1.591728e+09</td>\n      <td>twitter</td>\n      <td>Children receiving free school meals, from sin...</td>\n    </tr>\n    <tr>\n      <th>817042</th>\n      <td>1283969389117792258</td>\n      <td>1.594957e+09</td>\n      <td>twitter</td>\n      <td>L.A. County reports highest number of coronavi...</td>\n    </tr>\n    <tr>\n      <th>817043</th>\n      <td>1309834472053903360</td>\n      <td>1.601124e+09</td>\n      <td>twitter</td>\n      <td>Progress ?Covid19 Political Development</td>\n    </tr>\n    <tr>\n      <th>817044</th>\n      <td>1279062456468492289</td>\n      <td>1.593787e+09</td>\n      <td>twitter</td>\n      <td>@WeeklyEconPod @AyeishaTS @JCWI_UK @ZoeJardini...</td>\n    </tr>\n    <tr>\n      <th>817045</th>\n      <td>1274642711623188480</td>\n      <td>1.592733e+09</td>\n      <td>twitter</td>\n      <td>@mskathleenquinn @Babak_Javid_Lab @UCSF @Bob_W...</td>\n    </tr>\n  </tbody>\n</table>\n<p>817046 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(english_path, 'r')\n",
    "data = []\n",
    "for line in file:\n",
    "    data.append(json.loads(line))\n",
    "\n",
    "english_raw = pd.json_normalize(data)\n",
    "print(english_raw.shape)\n",
    "english_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "'''\n",
    "Turn text to Numpy Array\n",
    "'''\n",
    "texts_column = english_raw.loc[:,'text']\n",
    "raw_texts = texts_column.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "Crop for testing\n",
    "'''\n",
    "#raw_texts = raw_texts[:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nCrop for testing\\n'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in raw_texts:\n",
    "    processed_docs.append(preprocess(doc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['info', 'sourc'], ['work'], ['coronavirus', 'spreader', 'chris', 'cuomo', 'lectur', 'break', 'quarantin', 'upset'], ['militari', 'member', 'refus', 'vaccin', 'baffl', 'shit', 'pump'], ['covid', 'stay', 'busi', 'covidsaf'], ['lie', 'lie'], ['gregmannarino', 'deborah', 'birxwhit', 'hous', 'expert'], ['kayleigh', 'mcenani', 'scienc', 'stand', 'reopen', 'school'], ['amaz', 'effort', 'guy', 'donat'], ['sulaiodus', 'say', 'suspend', 'coronavirus', 'media']]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview 'processed_docs'\n",
    "'''\n",
    "print(processed_docs[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears\n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# LDA multicore\n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 8,\n",
    "                                   id2word = dictionary,\n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*\"vaccin\" + 0.013*\"patient\" + 0.012*\"hous\" + 0.012*\"studi\" + 0.010*\"hospit\" + 0.010*\"research\" + 0.009*\"expert\" + 0.009*\"say\" + 0.009*\"doctor\" + 0.008*\"post\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.028*\"mask\" + 0.019*\"pandem\" + 0.016*\"china\" + 0.014*\"face\" + 0.014*\"wear\" + 0.012*\"govern\" + 0.009*\"market\" + 0.008*\"sign\" + 0.007*\"world\" + 0.007*\"wuhan\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.068*\"peopl\" + 0.039*\"like\" + 0.019*\"look\" + 0.018*\"think\" + 0.013*\"right\" + 0.011*\"kill\" + 0.011*\"fuck\" + 0.011*\"get\" + 0.009*\"long\" + 0.009*\"go\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.040*\"vaccin\" + 0.020*\"health\" + 0.017*\"help\" + 0.012*\"home\" + 0.012*\"pandem\" + 0.011*\"stay\" + 0.011*\"busi\" + 0.010*\"work\" + 0.010*\"worker\" + 0.009*\"care\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.067*\"case\" + 0.046*\"test\" + 0.041*\"death\" + 0.028*\"report\" + 0.022*\"posit\" + 0.020*\"updat\" + 0.014*\"number\" + 0.013*\"state\" + 0.010*\"news\" + 0.010*\"infect\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.029*\"know\" + 0.020*\"good\" + 0.019*\"come\" + 0.018*\"year\" + 0.016*\"go\" + 0.011*\"time\" + 0.010*\"hope\" + 0.010*\"great\" + 0.009*\"love\" + 0.009*\"school\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.028*\"spread\" + 0.027*\"thank\" + 0.019*\"read\" + 0.014*\"check\" + 0.014*\"social\" + 0.013*\"latest\" + 0.011*\"nurs\" + 0.010*\"share\" + 0.010*\"articl\" + 0.009*\"symptom\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.070*\"trump\" + 0.030*\"realdonaldtrump\" + 0.020*\"presid\" + 0.019*\"american\" + 0.018*\"say\" + 0.017*\"state\" + 0.011*\"america\" + 0.011*\"biden\" + 0.010*\"respons\" + 0.009*\"vote\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}