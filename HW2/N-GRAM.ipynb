{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "N-GRAM.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "XZDIme-MFl2w"
      },
      "source": [
        "# N-Gram Language Models Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% imports\n"
        },
        "id": "4-edlegAFl27"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import nltk\n",
        "import nltk.data\n",
        "import random\n",
        "import math\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xznKACce5n_h",
        "outputId": "b14508f2-1961-4842-8c64-ed66409b1a3b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FMvfImqMFl29"
      },
      "source": [
        "## Reading 20N dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KrnKrO-BFl2-",
        "outputId": "cf07535d-e642-4f39-cb20-701375e9663c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "def read_document_20N():\n",
        "    \"\"\"\n",
        "    This method reads and writes the 20N Dataset\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    new_file = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/20news_file')\n",
        "    if os.path.exists(new_file):\n",
        "        os.remove(new_file)\n",
        "\n",
        "    folder_path = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/20news-18828')\n",
        "    inner_dirs = os.listdir(folder_path)\n",
        "    for dir in inner_dirs:\n",
        "        if not dir.startswith('.'):\n",
        "            #print(dir)\n",
        "            dir_path = os.path.join(folder_path,dir)\n",
        "            filenames = os.listdir(dir_path)\n",
        "            for file in filenames:\n",
        "                cur_path = os.path.join(dir_path,file)\n",
        "                #print(\"Copying \"+file)\n",
        "                with open(cur_path,'r', errors=\"ignore\") as firstfile, open(new_file,'a') as secondfile:\n",
        "                    for line in firstfile:\n",
        "                        secondfile.write(line)\n",
        "    print(\"Archivo terminado\")\n",
        "\n",
        "\n",
        "read_document_20N()\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-97b5f884f6b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mread_document_20N\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-97b5f884f6b0>\u001b[0m in \u001b[0;36mread_document_20N\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drive/MyDrive/datasets/20news-18828'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0minner_dirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minner_dirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/datasets/20news-18828'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "iBwOA9NKFl3B"
      },
      "source": [
        "## Reading BAC dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "umxi62QSFl3B"
      },
      "source": [
        "def read_document_BAC():\n",
        "    \"\"\"\n",
        "    This method reads and writes the BAC Dataset\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    new_file = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/bac_file')\n",
        "    if os.path.exists(new_file):\n",
        "        os.remove(new_file)\n",
        "\n",
        "    folder_path = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/blogs')\n",
        "    files = os.listdir(folder_path)\n",
        "    #files.sort()\n",
        "    for file in files:\n",
        "        #print(file)\n",
        "        cur_path = os.path.join(folder_path,file)\n",
        "        with open(cur_path,'r', errors=\"ignore\") as firstfile, open(new_file,'a') as secondfile:\n",
        "            for line in firstfile:\n",
        "                if len(line)>8 and not len(line)==28:\n",
        "                    #print(len(line))\n",
        "                    #print(line.strip())\n",
        "                    secondfile.write(line.strip())\n",
        "    print(\"Archivo terminado\")\n",
        "\n",
        "\n",
        "read_document_BAC()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "TcTIi3gYFl3D"
      },
      "source": [
        "## Tokenize by sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-i_FXy9lFl3D",
        "outputId": "a31dd851-4296-4a0e-a076-e37010b33294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "source": [
        "def sentences(path):\n",
        "    \"\"\"\n",
        "    Divides the document in a List of sentences.\n",
        "    :return: a list with sentences\n",
        "    \"\"\"\n",
        "    news_file = os.path.join(os.getcwd(), path)\n",
        "    text = open(news_file).read()\n",
        "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    sentences = sent_detector.tokenize(text.strip())\n",
        "    return sentences\n",
        "\n",
        "sentencesf1 = sentences(\"drive/MyDrive/datasets/20news_file\")\n",
        "sentencesf2 = sentences(\"drive/MyDrive/datasets/bac_file\")\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-9d002accecdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msentencesf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/datasets/20news_file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0msentencesf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/datasets/bac_file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-9d002accecdc>\u001b[0m in \u001b[0;36msentences\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnews_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msent_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "rg2p9ZuzFl3E"
      },
      "source": [
        "## Reduce data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9Im5kw7iFl3F"
      },
      "source": [
        "def reduce_data(sentences):\n",
        "    \"\"\"\n",
        "    Reduces randomly the sentences in order to speed up the\n",
        "    processing time according to a specific percentage\n",
        "    :param sentences: the complete list of sentences\n",
        "    :return: the reduces sentences\n",
        "    \"\"\"\n",
        "    percentage = math.floor(len(sentences)*0.001)\n",
        "    sentences = random.sample(sentences, percentage)\n",
        "    return sentences\n",
        "\n",
        "sentencesf1 = reduce_data(sentencesf1)\n",
        "sentencesf2 = reduce_data(sentencesf2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bNchFewpFl3F"
      },
      "source": [
        "def tokenization(sentences):\n",
        "    \"\"\"\n",
        "    Tokenization of the sentences array.\n",
        "    :return: tokens for each sentence\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    i = 0\n",
        "    for sentence in sentences:\n",
        "        #print(sentence)\n",
        "        # Normalize, but DO NOT eliminate stop words.\n",
        "        lower_text = sentence.lower()\n",
        "        sentence_tok = [token for token in nltk.word_tokenize(lower_text) if (token.isalnum())]\n",
        "        if len(sentence_tok)== 0:\n",
        "            continue\n",
        "        # Replace numbers with a token named NUM.\n",
        "        sentence_tok = [\"NUM\" if token.isnumeric() else token for token in sentence_tok]\n",
        "        # Add sentence start and end tags <s></s>.\n",
        "        sentence_tok.insert(0,\"<s>\")\n",
        "        sentence_tok.append(\"</s>\")\n",
        "        #print(sentence_tok)\n",
        "        tokens.extend(sentence_tok)\n",
        "        #print(tokens)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "tokensf1 = tokenization(sentencesf1)\n",
        "tokensf2 = tokenization(sentencesf2)\n",
        "\n",
        "#print(tokensf1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NIu5MSdcFl3G"
      },
      "source": [
        "def unit_frequency(tokens):\n",
        "    \"\"\"\n",
        "    Identifies tokens that apear only once in the data set\n",
        "    and replaces them with <UKN> tag\n",
        "    :param tokens:\n",
        "    :return: tokens with <UKN> tag\n",
        "    \"\"\"\n",
        "    tokens_final = [\"<UKN>\" if tokens.count(token) == 1 else token for token in tokens]\n",
        "    return tokens_final\n",
        "\n",
        "\n",
        "#tokensf1 = unit_frequency(tokensf1)\n",
        "#tokensf2 = unit_frequency(tokensf2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "d6GDl_RoFl3G"
      },
      "source": [
        "## Training and Testing Division"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "w1YOk7kmFl3H"
      },
      "source": [
        "def sentence_selection(tokens):\n",
        "    \"\"\"\n",
        "    Dvidides the document in test and training set\n",
        "    :param tokens: list of tokens\n",
        "    :return: sentences of training and test sets\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    i = 0\n",
        "    for token in tokens:\n",
        "        if \"<s>\" in token:\n",
        "            new_sentence = [token]\n",
        "        elif \"</s>\" in token:\n",
        "            new_sentence.append(token)\n",
        "            sentences.append(new_sentence)\n",
        "        else:\n",
        "            new_sentence.append(token)\n",
        "    percentage = math.floor(len(sentences)*0.8)\n",
        "    training = random.sample(sentences, percentage)\n",
        "    test = [ sentence for sentence in sentences if sentence not in training ]\n",
        "\n",
        "    return training, test\n",
        "\n",
        "\n",
        "(training1, test1) = sentence_selection(tokensf1)\n",
        "(training2, test2) = sentence_selection(tokensf2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ure6H5RGFl3H"
      },
      "source": [
        "#lista = [['hola','amigo'],['hola1','amigo1'],['hola2','amigo2'],['hola3','amigo3']]\n",
        "#porcentaje = math.floor(len(lista)*0.8)\n",
        "#training = random.sample(lista, porcentaje)\n",
        "#test = [ sentence for sentence in lista if sentence not in training ]\n",
        "\n",
        "#print(training)\n",
        "#print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FPRlHu5IFl3I"
      },
      "source": [
        "def writing_files(file, training, testing):\n",
        "    \"\"\"\n",
        "    Writes the correspondind file\n",
        "    :param file: 1 for 20N 2 for BAC\n",
        "    :param training: list of sentences of the training set\n",
        "    :param testing: list of sentences of the testing set\n",
        "    :return: nothing\n",
        "    \"\"\"\n",
        "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
        "    path_training = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_training\")\n",
        "    path_testing = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_testing\")\n",
        "    outfile = open(path_training,'wb')\n",
        "    pickle.dump(training,outfile)\n",
        "    outfile.close()\n",
        "\n",
        "    outfile = open(path_testing,'wb')\n",
        "    pickle.dump(testing,outfile)\n",
        "    outfile.close()\n",
        "\n",
        "writing_files(1, training1, test1)\n",
        "writing_files(2, training2, test2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Xg4N0lJWFl3J"
      },
      "source": [
        "def reading_files(file):\n",
        "    \"\"\"\n",
        "    Reads the testing and trianing file for a specific dataset\n",
        "    :param file: 1 for 20N, 2 for BAC\n",
        "    :return: the training and testing sets\n",
        "    \"\"\"\n",
        "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
        "    path_training = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_training\")\n",
        "    path_testing = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_testing\")\n",
        "    with open(path_training, 'rb') as training:\n",
        "        training = pickle.load(training)\n",
        "    with open(path_testing, 'rb') as testing:\n",
        "        testing = pickle.load(testing)\n",
        "    return training, testing\n",
        "\n",
        "\n",
        "training1, test1 = reading_files(1)\n",
        "training2, test2 = reading_files(2)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TzBQFfTsFl3K"
      },
      "source": [
        "## N-grams Modelling with Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ps0XWyYYFl3L"
      },
      "source": [
        "#This code was adapted from https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853\n",
        "\n",
        "def ngrams_generation(training):\n",
        "    \"\"\"\n",
        "    Generates the ngrams models\n",
        "    :param training: the traingin list of sentences\n",
        "    :return: a dictionary containing the unigram\n",
        "    bigram and trigram probabilities and ngrams\n",
        "    \"\"\"\n",
        "    tokenized_text = training\n",
        "    ngrams_all = {1:[], 2:[], 3:[]}\n",
        "    for i in range(3):\n",
        "        for each in tokenized_text:\n",
        "            for j in ngrams(each, i+1):\n",
        "                ngrams_all[i+1].append(j);\n",
        "\n",
        "    ngrams_voc = {1:set([]), 2:set([]), 3:set([])}\n",
        "\n",
        "    for i in range(3):\n",
        "        for gram in ngrams_all[i+1]:\n",
        "            if gram not in ngrams_voc[i+1]:\n",
        "                ngrams_voc[i+1].add(gram)\n",
        "    total_ngrams = {1:-1, 2:-1, 3:-1}\n",
        "    total_voc = {1:-1, 2:-1, 3:-1}\n",
        "\n",
        "    for i in range(3):\n",
        "        total_ngrams[i+1] = len(ngrams_all[i+1])\n",
        "        total_voc[i+1] = len(ngrams_voc[i+1])\n",
        "\n",
        "    ngrams_prob = {1:[], 2:[], 3:[]}\n",
        "    for i in range(3):\n",
        "        for ngram in ngrams_voc[i+1]:\n",
        "            tlist = [ngram]\n",
        "            tlist.append(ngrams_all[i+1].count(ngram))\n",
        "            ngrams_prob[i+1].append(tlist)\n",
        "\n",
        "    for i in range(3):\n",
        "        for ngram in ngrams_prob[i+1]:\n",
        "            ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])\n",
        "    return ngrams_prob\n",
        "\n",
        "ngrams_prob_1 = ngrams_generation(training1)\n",
        "ngrams_prob_2 = ngrams_generation(training2)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qXLATWmgFl3N"
      },
      "source": [
        "## File Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7tsnbwESFl3N"
      },
      "source": [
        "def writing_models(file, ngrams_prob):\n",
        "    \"\"\"\n",
        "    Taking the ngram model, writes the corresponding files for\n",
        "    each one of them for the specific dataset\n",
        "    :param file: 1 for 20N 2 for BAC\n",
        "    :param ngrams_prob: the ngram dictionary that contains\n",
        "    each model\n",
        "    :return: nothing\n",
        "    \"\"\"\n",
        "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
        "    path_unigram = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_unigram\")\n",
        "    path_bigram = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_bigram\")\n",
        "    path_trigram = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_trigram\")\n",
        "    outfile = open(path_unigram,'wb')\n",
        "    pickle.dump(ngrams_prob[1],outfile)\n",
        "    outfile.close()\n",
        "\n",
        "    outfile = open(path_bigram,'wb')\n",
        "    pickle.dump(ngrams_prob[2],outfile)\n",
        "    outfile.close()\n",
        "\n",
        "    outfile = open(path_trigram,'wb')\n",
        "    pickle.dump(ngrams_prob[3],outfile)\n",
        "    outfile.close()\n",
        "\n",
        "writing_models(1, ngrams_prob_1)\n",
        "writing_models(2, ngrams_prob_2)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ewNNO9RhFl3O"
      },
      "source": [
        "def reading_models(file):\n",
        "    \"\"\"\n",
        "    Reads the n-grams model for each dataset\n",
        "    :param file: 1 for 20N or 2 for BAC\n",
        "    :return: unigram, bigram, trigram models\n",
        "    \"\"\"\n",
        "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
        "    path_unigram = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_unigram\")\n",
        "    path_bigram = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_bigram\")\n",
        "    path_trigram = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_trigram\")\n",
        "    with open(path_unigram, 'rb') as unigram:\n",
        "        unigram = pickle.load(unigram)\n",
        "    with open(path_bigram, 'rb') as bigram:\n",
        "        bigram = pickle.load(bigram)\n",
        "    with open(path_trigram, 'rb') as trigram:\n",
        "        trigram = pickle.load(trigram)\n",
        "    return unigram, bigram, trigram\n",
        "\n",
        "\n",
        "unigram_1, bigram_1, trigram_1  = reading_models(1)\n",
        "unigram_2, bigram_2, trigram_2  = reading_models(2)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oJyE37l9Fl3M",
        "outputId": "48b70f1a-291c-4fdf-a26a-6e7562adb6c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Prints top 10 unigram, bigram, trigram after smoothing\n",
        "print(\"Most common n-grams and with Laplace smoothing: \\n\")\n",
        "  \n",
        "unigram_1 = sorted(unigram_1, key = lambda x:x[1], reverse = True)\n",
        "bigram_1 = sorted(bigram_1, key = lambda x:x[1], reverse = True)\n",
        "trigram_1 = sorted(trigram_1, key = lambda x:x[1], reverse = True)\n",
        "\n",
        "unigram_2 = sorted(unigram_2, key = lambda x:x[1], reverse = True)\n",
        "bigram_2 = sorted(bigram_2, key = lambda x:x[1], reverse = True)\n",
        "trigram_2 = sorted(trigram_2, key = lambda x:x[1], reverse = True)\n",
        "\n",
        "print(\"Most common unigrams: \", str(unigram_1[:10]))\n",
        "print(\"\\nMost common bigrams: \", str(bigram_1[:10]))\n",
        "print(\"\\nMost common trigrams: \", str(trigram_1[:10]))\n",
        "\n",
        "print(\"Most common unigrams: \", str(unigram_2[:10]))\n",
        "print(\"\\nMost common bigrams: \", str(bigram_2[:10]))\n",
        "print(\"\\nMost common trigrams: \", str(trigram_2[:10]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common n-grams and with Laplace smoothing: \n",
            "\n",
            "Most common unigrams:  [[('</s>',), 0.04005572971090213], [('<s>',), 0.04005572971090213], [('the',), 0.03692093347265761], [('of',), 0.017938000696621385], [('to',), 0.01654475792406827], [('and',), 0.014280738418669453], [('a',), 0.014280738418669453], [('NUM',), 0.012365029606408917], [('i',), 0.012365029606408917], [('in',), 0.01079763148728666]]\n",
            "\n",
            "Most common bigrams:  [[('<s>', 'i'), 0.0041812786619908286], [('o', 'o'), 0.003506878877798759], [('of', 'the'), 0.002967359050445104], [('<s>', 'the'), 0.00283247909360669], [('in', 'the'), 0.0022929592662530346], [('NUM', '</s>'), 0.0016185594820609657], [('to', 'the'), 0.001483679525222552], [('for', 'the'), 0.001483679525222552], [('it', 'is'), 0.001348799568384138], [('on', 'the'), 0.0012139196115457244]]\n",
            "\n",
            "Most common trigrams:  [[('o', 'o', 'o'), 0.00337108953613808], [('NUM', 'to', 'NUM'), 0.000674217907227616], [('NUM', 'NUM', '</s>'), 0.0005393743257820927], [('<s>', 'i', 'do'), 0.0005393743257820927], [('<s>', 'i', 'am'), 0.0005393743257820927], [('the', 'case', '</s>'), 0.0005393743257820927], [('into', 'a', 'NUM'), 0.0004045307443365696], [('anas', 'omran', 'writes'), 0.0004045307443365696], [('we', 'had', 'a'), 0.0004045307443365696], [('NUM', 'NUM', 'NUM'), 0.0004045307443365696]]\n",
            "Most common unigrams:  [[('</s>',), 0.0499828340274357], [('<s>',), 0.0499828340274357], [('i',), 0.031928708969593836], [('the',), 0.031033093009717433], [('to',), 0.02330840535578344], [('and',), 0.021644052363679788], [('a',), 0.01691967817533175], [('of',), 0.01421790336303793], [('it',), 0.011628080545728657], [('that',), 0.011225053363784276]]\n",
            "\n",
            "Most common bigrams:  [[('<s>', 'i'), 0.006498758505228984], [('date', 'NUM'), 0.0023868373274209148], [('in', 'the'), 0.002047476569967704], [('of', 'the'), 0.0020191965068466033], [('and', 'i'), 0.0015723715095332092], [('<s>', 'the'), 0.0015101553706667874], [('<s>', 'it'), 0.0014535952444245856], [('i', 'have'), 0.001340474991940182], [('to', 'the'), 0.001244322777328439], [('i', 'was'), 0.001216042714207338]]\n",
            "\n",
            "Most common trigrams:  [[('<s>', 'i', 'have'), 0.0003111910218920414], [('<s>', 'it', 'was'), 0.00030131194183197663], [('<s>', 'i', 'was'), 0.0002914328617719118], [('date', 'NUM', 'i'), 0.0002716747016517822], [('<s>', 'i', 'think'), 0.00026673516162174977], [('<s>', 'date', 'NUM'), 0.00023709792144155535], [('<s>', 'i', 'am'), 0.00022721884138149055], [('<s>', 'i', 'do'), 0.00022721884138149055], [('<s>', 'NUM', '</s>'), 0.00022227930135145814], [('date', 'NUM', 'urllink'), 0.00022227930135145814]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uGy_cmMHFl3P"
      },
      "source": [
        "## Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sXmPnhFBFl3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c2ce11-2b71-401f-8832-f5966d167cb3"
      },
      "source": [
        "def perplexity(test, n, ngram):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity for each model\n",
        "    :param test: the test set\n",
        "    :param n: the number of words to consider in the n gram\n",
        "    :param ngram: the model\n",
        "    :return: the value of the perplexity\n",
        "    \"\"\"\n",
        "    perp = 1;\n",
        "    N = 0\n",
        "    for sentence in test:\n",
        "        for j in range(len(sentence)):\n",
        "            for gram in ngram:\n",
        "                for i in range(n):\n",
        "                    encontrado = True\n",
        "                    if j-i >= 0 and sentence[j-i] == gram[0][-1-i]:\n",
        "                       encontrado = encontrado and True\n",
        "                    else:\n",
        "                       encontrado = encontrado and False\n",
        "                       break\n",
        "                    if i == n-1 and encontrado:\n",
        "                        perp = perp + math.log(gram[1])\n",
        "                        #print(perp)\n",
        "                        N = N+1\n",
        "    #print(perp)\n",
        "    #print(N)\n",
        "    perplexity = math.exp((-1/N)*perp)\n",
        "    return perplexity\n",
        "\n",
        "uni_perp_1 = perplexity(test1,1,unigram_1)\n",
        "print(\"Unigram Perplexity 20N\", uni_perp_1)\n",
        "bi_perp_1 = perplexity(test1,2,bigram_1)\n",
        "print(\"Bigram Perplexity 20N\", bi_perp_1)\n",
        "tri_perp_1 = perplexity(test1,3,trigram_1)\n",
        "print(\"Trigram Perplexity 20N\", tri_perp_1)\n",
        "\n",
        "uni_perp_2 = perplexity(test2,1,unigram_2)\n",
        "print(\"Unigram Perplexity BAC\", uni_perp_2)\n",
        "bi_perp_2 = perplexity(test2,2,bigram_2)\n",
        "print(\"Bigram Perplexity BAC\", bi_perp_2)\n",
        "tri_perp_2 = perplexity(test2,3,trigram_2)\n",
        "print(\"Trigram Perplexity BAC\", tri_perp_2)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigram Perplexity 20N 229.33528861559785\n",
            "Bigram Perplexity 20N 1929.6016359072366\n",
            "Trigram Perplexity 20N 3245.5179654821177\n",
            "Unigram Perplexity BAC 494.79174973486306\n",
            "Bigram Perplexity BAC 14062.236366452424\n",
            "Trigram Perplexity BAC 50923.97332367625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "czpAILckFl3R"
      },
      "source": [
        "## Linear Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "pXORfiQSFl3R"
      },
      "source": [
        "def linear_interpolation(unigrams, bigrams, trigrams):\n",
        "    \"\"\"\n",
        "    Generates the linear interpolation model\n",
        "    :param ngrams_prob: dictiornary of ngrams models.\n",
        "    :return: the linear interpolation model\n",
        "    \"\"\"\n",
        "    linear = []\n",
        "    lambda1 = 1/3\n",
        "    lambda2 = 1/3\n",
        "    lambda3 = 1/3\n",
        "\n",
        "    for trigram in trigrams:\n",
        "        for bigram in bigrams:\n",
        "            for unigram in unigrams:\n",
        "                if trigram[0][-1] == bigram[0][-1] and trigram[0][-2] == trigram[0][-2] and trigram[0][-1] == unigram[0][0]:\n",
        "                    prob = lambda1*trigram[1] + lambda2*bigram[1] + lambda3*unigram[1]\n",
        "                    linear.append([trigram[0],prob])\n",
        "\n",
        "    return linear\n",
        "\n",
        "linear_1 = linear_interpolation(unigram_1, bigram_1, trigram_1)\n",
        "linear_2 = linear_interpolation(unigram_2, bigram_2, trigram_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "K4MvhFotFl3S"
      },
      "source": [
        "#Prints top 10 trigrams after smothing\n",
        "print(\"Most common n-grams with linear interpolation: \\n\")\n",
        "linear_1 = sorted(linear_1, key = lambda x:x[1], reverse = True)\n",
        "\n",
        "print (\"\\nMost common trigrams: \", str(linear_1[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "rg9fEPpJFl3U"
      },
      "source": [
        "## Perplexity Comparrison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "Zi3p_bHrFl3V"
      },
      "source": [
        "# Caclulates the perplexity of linear interpolation\n",
        "linear_perp_1 = perplexity(test1,3,linear_1)\n",
        "linear_perp_2 = perplexity(test2,3,linear_1)\n",
        "print(\"Linear Interpolation Perplexity 20N\", linear_perp_1)\n",
        "print(\"Linear Interpolation Perplexity BAC\", linear_perp_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "CP5TYjBLFl3V"
      },
      "source": [
        "# Generates the corresponding table\n",
        "\n",
        "raw_data = {'Model': [\"Unigram\", \"Bigram\", \"Trigram\",\"Linear Interpolation\"],\n",
        "        'Perplexities_20N': [uni_perp_1, bi_perp_1, tri_perp_1, linear_perp_1],\n",
        "        'Perplexities_BAC': [uni_perp_2, bi_perp_2, tri_perp_2, linear_perp_2],}\n",
        "\n",
        "df = pd.DataFrame(raw_data, columns = ['Model', 'Perplexities_20N', 'Perplexities_BAC'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0lAiDC4GFl3W"
      },
      "source": [
        "## Sentence Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "baX_6-B2Fl3X"
      },
      "source": [
        "def sentence_generator(word, ngram):\n",
        "    \"\"\"\n",
        "    Funcion that given an initial word, generates\n",
        "    a sentence\n",
        "    :param word: the initial (or current word) that is beeing considered\n",
        "    :param ngram: the ngram model beeing used\n",
        "    :param sentence: the given sentence till now. At first is just \"\"\n",
        "    :return: the final sentence\n",
        "    \"\"\"\n",
        "    max = 10\n",
        "    count = 0\n",
        "    sentence = word + \"\"\n",
        "    encontrado = True\n",
        "\n",
        "    p_word = word\n",
        "    pp_word = word\n",
        "    while word != \"</s>\" and count < max and encontrado:\n",
        "      encontrado = False\n",
        "      for gram in ngram:\n",
        "          if count < 1 and word == gram[0][-1]:\n",
        "              sentence = sentence + \" \" + gram[0][-2]\n",
        "              p_word = word\n",
        "              word = gram[0][-2]\n",
        "              count = count +1\n",
        "              encontrado = True\n",
        "              #print(\"Entra\")\n",
        "              break\n",
        "          elif count < 2 and word == gram[0][-1] and p_word == gram[0][-2]:\n",
        "              sentence = sentence + \" \" + gram[0][-2]\n",
        "              pp_word = p_word\n",
        "              p_word = word\n",
        "              word = gram[0][-2]\n",
        "              count = count +1\n",
        "              encontrado = True\n",
        "              #print(\"Entra2\")\n",
        "              break\n",
        "          elif count == 1 and word == gram[0][-1]:\n",
        "              sentence = sentence + \" \" + gram[0][-2]\n",
        "              pp_word = p_word\n",
        "              p_word = word\n",
        "              word = gram[0][-2]\n",
        "              count = count +1\n",
        "              encontrado = True\n",
        "              #print(\"Entra2\")\n",
        "              break\n",
        "          elif count >= 2 and word == gram[0][-1] and p_word == gram[0][-2] and pp_word == gram[0][-3]:\n",
        "              sentence = sentence + \" \" + gram[0][-2]\n",
        "              pp_word = p_word\n",
        "              p_word = word\n",
        "              word = gram[0][-2]\n",
        "              count = count +1\n",
        "              encontrado = True\n",
        "              #print(\"Entra3\")\n",
        "              break\n",
        "          elif count >= 2 and word == gram[0][-1] and p_word == gram[0][-2]:\n",
        "              sentence = sentence + \" \" + gram[0][-2]\n",
        "              pp_word = p_word\n",
        "              p_word = word\n",
        "              word = gram[0][-2]\n",
        "              count = count +1\n",
        "              encontrado = True\n",
        "              #print(\"Entra4\")\n",
        "              break\n",
        "          elif count >= 2 and word == gram[0][-1]:\n",
        "              sentence = sentence + \" \" + gram[0][-2]\n",
        "              p_word = word\n",
        "              word = gram[0][-2]\n",
        "              count = count +1\n",
        "              encontrado = True\n",
        "              #print(\"Entra5\")\n",
        "              break\n",
        "          \n",
        "            \n",
        "    return sentence\n",
        "\n",
        "trigram_1 = sorted(trigram_1, key = lambda x:x[1], reverse = True)\n",
        "\n",
        "# Different tests with different words.\n",
        "\n",
        "test_sentence = sentence_generator(\"friend\", trigram_1)\n",
        "print(\"Test Sentence 1 (20N):\", test_sentence)\n",
        "\n",
        "test_sentence = sentence_generator(\"friend\", trigram_2)\n",
        "print(\"Test Sentence 1 (BAC):\", test_sentence)\n",
        "\n",
        "test_sentence = sentence_generator(\"the\", trigram_1)\n",
        "print(\"Test Sentence 2 (20N):\", test_sentence)\n",
        "\n",
        "test_sentence = sentence_generator(\"the\", trigram_2)\n",
        "print(\"Test Sentence 2 (BAC):\", test_sentence)\n",
        "\n",
        "test_sentence = sentence_generator(\"i\", trigram_1)\n",
        "print(\"Test Sentence 3 (20N):\", test_sentence)\n",
        "\n",
        "test_sentence = sentence_generator(\"i\", trigram_2)\n",
        "print(\"Test Sentence 3 (BAC):\", test_sentence)\n",
        "\n",
        "test_sentence = sentence_generator(\"you\", trigram_1)\n",
        "print(\"Test Sentence 4 (20N):\", test_sentence)\n",
        "\n",
        "test_sentence = sentence_generator(\"you\", trigram_2)\n",
        "print(\"Test Sentence 4 (BAC):\", test_sentence)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5T-qvhlZ3ZW"
      },
      "source": [
        ""
      ]
    }
  ]
}