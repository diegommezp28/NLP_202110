{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# N-Gram Language Models Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "import nltk.data\n",
    "import random\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% imports\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading 20N dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo terminado\n"
     ]
    }
   ],
   "source": [
    "def read_document_20N():\n",
    "    \"\"\"\n",
    "    This method reads and writes the 20N Dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_file = os.path.join(os.getcwd(), 'datasets/20news_file')\n",
    "    if os.path.exists(new_file):\n",
    "        os.remove(new_file)\n",
    "\n",
    "    folder_path = os.path.join(os.getcwd(), 'datasets/20news-18828')\n",
    "    inner_dirs = os.listdir(folder_path)\n",
    "    for dir in inner_dirs:\n",
    "        if not dir.startswith('.'):\n",
    "            #print(dir)\n",
    "            dir_path = os.path.join(folder_path,dir)\n",
    "            filenames = os.listdir(dir_path)\n",
    "            for file in filenames:\n",
    "                cur_path = os.path.join(dir_path,file)\n",
    "                #print(\"Copying \"+file)\n",
    "                with open(cur_path,'r', errors=\"ignore\") as firstfile, open(new_file,'a') as secondfile:\n",
    "                    for line in firstfile:\n",
    "                        secondfile.write(line)\n",
    "    print(\"Archivo terminado\")\n",
    "\n",
    "\n",
    "read_document_20N()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading BAC dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-39-1f1e08efdbc5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m \u001B[0mread_document_BAC\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-39-1f1e08efdbc5>\u001B[0m in \u001B[0;36mread_document_BAC\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0mcur_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfolder_path\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcur_path\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'r'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"ignore\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mfirstfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_file\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msecondfile\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfirstfile\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m>\u001B[0m\u001B[0;36m8\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0;36m28\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m                     \u001B[0;31m#print(len(line))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/codecs.py\u001B[0m in \u001B[0;36mdecode\u001B[0;34m(self, input, final)\u001B[0m\n\u001B[1;32m    317\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mNotImplementedError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 319\u001B[0;31m     \u001B[0;32mdef\u001B[0m \u001B[0mdecode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfinal\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    320\u001B[0m         \u001B[0;31m# decode input (taking the buffer into account)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    321\u001B[0m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuffer\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def read_document_BAC():\n",
    "    \"\"\"\n",
    "    This method reads and writes the BAC Dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_file = os.path.join(os.getcwd(), 'datasets/bac_file')\n",
    "    if os.path.exists(new_file):\n",
    "        os.remove(new_file)\n",
    "\n",
    "    folder_path = os.path.join(os.getcwd(), 'datasets/blogs')\n",
    "    files = os.listdir(folder_path)\n",
    "    #files.sort()\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        cur_path = os.path.join(folder_path,file)\n",
    "        with open(cur_path,'r', errors=\"ignore\") as firstfile, open(new_file,'a') as secondfile:\n",
    "            for line in firstfile:\n",
    "                if len(line)>8 and not len(line)==28:\n",
    "                    #print(len(line))\n",
    "                    #print(line.strip())\n",
    "                    secondfile.write(line.strip())\n",
    "    print(\"Archivo terminado\")\n",
    "\n",
    "\n",
    "read_document_BAC()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize by sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentences(path):\n",
    "    \"\"\"\n",
    "    Divides the document in a List of sentences.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    news_file = os.path.join(os.getcwd(), path)\n",
    "    text = open(news_file).read()\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(text.strip())\n",
    "    return sentences\n",
    "\n",
    "sentencesf1 = sentences(\"datasets/20news_file\")\n",
    "sentencesf2 = sentences(\"datasets/bac_file\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenization(sentences):\n",
    "    \"\"\"\n",
    "    Tokenization of the sentences array.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        #print(sentence)\n",
    "        # Normalize, but DO NOT eliminate stop words.\n",
    "        lower_text = sentence.lower()\n",
    "        sentence_tok = [token for token in nltk.word_tokenize(lower_text) if (token.isalnum())]\n",
    "        if len(sentence_tok)== 0:\n",
    "            continue\n",
    "        # Replace numbers with a token named NUM.\n",
    "        sentence_tok = [\"NUM\" if token.isnumeric() else token for token in sentence_tok]\n",
    "        # Add ta Add sentence start and end tags <s></s>.\n",
    "        sentence_tok[0] = \"<s>\" + sentence_tok[0]\n",
    "        sentence_tok[len(sentence_tok)-1] = sentence_tok[len(sentence_tok)-1] +  \"</s>\"\n",
    "        #print(sentence_tok)\n",
    "        tokens.extend(sentence_tok)\n",
    "        #print(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokensf1_pre = tokenization(sentencesf1)\n",
    "tokensf2_pre = tokenization(sentencesf2)\n",
    "\n",
    "#print(tokensf1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def unit_frequency(tokens):\n",
    "    \"\"\"\n",
    "\n",
    "    :param tokens:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens_final = [\"<UKN>\" if tokens.count(token) == 1 else token for token in tokens]\n",
    "    return tokens_final\n",
    "\n",
    "\n",
    "tokensf1 = unit_frequency(tokensf1_pre)\n",
    "tokensf2 = unit_frequency(tokensf2_pre)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Testing Division"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentence_selection(tokens):\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    for token in tokens:\n",
    "        if \"<s>\" in token:\n",
    "            new_sentence = [token]\n",
    "        elif \"</s>\" in token:\n",
    "            new_sentence.append(token)\n",
    "            sentences.append(new_sentence)\n",
    "        else:\n",
    "            new_sentence.append(token)\n",
    "    percentage = math.floor(len(sentences)*0.8)\n",
    "    training = random.sample(sentences, percentage)\n",
    "    test = [ sentence for sentence in sentences if sentence not in training ]\n",
    "\n",
    "    return training, test\n",
    "\n",
    "\n",
    "(training1, test1) = sentence_selection(tokensf1)\n",
    "#(training2, test2) = sentence_selection(tokensf2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lista = [['hola','amigo'],['hola1','amigo1'],['hola2','amigo2'],['hola3','amigo3']]\n",
    "porcentaje = math.floor(len(lista)*0.8)\n",
    "training = random.sample(lista, porcentaje)\n",
    "test = [ sentence for sentence in lista if sentence not in training ]\n",
    "\n",
    "print(training)\n",
    "print(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def writing_files(file, training, testing):\n",
    "    file_name = \"20N\" if file == 1 else BAC\n",
    "    path_training = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_training\")\n",
    "    path_testing = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_testing\")\n",
    "    outfile = open(path_training,'wb')\n",
    "    pickle.dump(training,outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    outfile = open(path_testing,'wb')\n",
    "    pickle.dump(testing,outfile)\n",
    "    outfile.close()\n",
    "\n",
    "writing_files(1, training1, test1)\n",
    "writing_files(2, training2, test2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}