{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "N-GRAM.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "XZDIme-MFl2w"
      },
      "source": [
        "# N-Gram Language Models Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% imports\n"
        },
        "id": "4-edlegAFl27"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import nltk\n",
        "import nltk.data\n",
        "import random\n",
        "import math\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FMvfImqMFl29"
      },
      "source": [
        "## Reading 20N dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KrnKrO-BFl2-",
        "outputId": "d5dbb880-f754-4a24-810f-adceda1df6c9"
      },
      "source": [
        "def read_document_20N():\n",
        "    \"\"\"\n",
        "    This method reads and writes the 20N Dataset\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    new_file = os.path.join(os.getcwd(), 'datasets/20news_file')\n",
        "    if os.path.exists(new_file):\n",
        "        os.remove(new_file)\n",
        "\n",
        "    folder_path = os.path.join(os.getcwd(), 'datasets/20news-18828')\n",
        "    inner_dirs = os.listdir(folder_path)\n",
        "    for dir in inner_dirs:\n",
        "        if not dir.startswith('.'):\n",
        "            #print(dir)\n",
        "            dir_path = os.path.join(folder_path,dir)\n",
        "            filenames = os.listdir(dir_path)\n",
        "            for file in filenames:\n",
        "                cur_path = os.path.join(dir_path,file)\n",
        "                #print(\"Copying \"+file)\n",
        "                with open(cur_path,'r', errors=\"ignore\") as firstfile, open(new_file,'a') as secondfile:\n",
        "                    for line in firstfile:\n",
        "                        secondfile.write(line)\n",
        "    print(\"Archivo terminado\")\n",
        "\n",
        "\n",
        "read_document_20N()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archivo terminado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "iBwOA9NKFl3B"
      },
      "source": [
        "## Reading BAC dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "umxi62QSFl3B",
        "outputId": "9300628a-71e2-4491-a9ba-a802bcc13f98"
      },
      "source": [
        "def read_document_BAC():\n",
        "    \"\"\"\n",
        "    This method reads and writes the BAC Dataset\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    new_file = os.path.join(os.getcwd(), 'datasets/bac_file')\n",
        "    if os.path.exists(new_file):\n",
        "        os.remove(new_file)\n",
        "\n",
        "    folder_path = os.path.join(os.getcwd(), 'datasets/blogs')\n",
        "    files = os.listdir(folder_path)\n",
        "    #files.sort()\n",
        "    for file in files:\n",
        "        #print(file)\n",
        "        cur_path = os.path.join(folder_path,file)\n",
        "        with open(cur_path,'r', errors=\"ignore\") as firstfile, open(new_file,'a') as secondfile:\n",
        "            for line in firstfile:\n",
        "                if len(line)>8 and not len(line)==28:\n",
        "                    #print(len(line))\n",
        "                    #print(line.strip())\n",
        "                    secondfile.write(line.strip())\n",
        "    print(\"Archivo terminado\")\n",
        "\n",
        "\n",
        "read_document_BAC()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archivo terminado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "TcTIi3gYFl3D"
      },
      "source": [
        "## Tokenize by sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-i_FXy9lFl3D"
      },
      "source": [
        "def sentences(path):\n",
        "    \"\"\"\n",
        "    Divides the document in a List of sentences.\n",
        "    :return: a list with sentences\n",
        "    \"\"\"\n",
        "    news_file = os.path.join(os.getcwd(), path)\n",
        "    text = open(news_file).read()\n",
        "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    sentences = sent_detector.tokenize(text.strip())\n",
        "    return sentences\n",
        "\n",
        "sentencesf1 = sentences(\"datasets/20news_file\")\n",
        "sentencesf2 = sentences(\"datasets/bac_file\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "rg2p9ZuzFl3E"
      },
      "source": [
        "## Reduce data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9Im5kw7iFl3F"
      },
      "source": [
        "def reduce_data(sentences):\n",
        "    \"\"\"\n",
        "    Reduces randomly the sentences in order to speed up the\n",
        "    processing time according to a specific percentage\n",
        "    :param sentences: the complete list of sentences\n",
        "    :return: the reduces sentences\n",
        "    \"\"\"\n",
        "    percentage = math.floor(len(sentences)*0.001)\n",
        "    sentences = random.sample(sentences, percentage)\n",
        "    return sentences\n",
        "\n",
        "sentencesf1 = reduce_data(sentencesf1)\n",
        "sentencesf2 = reduce_data(sentencesf2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bNchFewpFl3F"
      },
      "source": [
        "def tokenization(sentences):\n",
        "    \"\"\"\n",
        "    Tokenization of the sentences array.\n",
        "    :return: tokens for each sentence\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    i = 0\n",
        "    for sentence in sentences:\n",
        "        #print(sentence)\n",
        "        # Normalize, but DO NOT eliminate stop words.\n",
        "        lower_text = sentence.lower()\n",
        "        sentence_tok = [token for token in nltk.word_tokenize(lower_text) if (token.isalnum())]\n",
        "        if len(sentence_tok)== 0:\n",
        "            continue\n",
        "        # Replace numbers with a token named NUM.\n",
        "        sentence_tok = [\"NUM\" if token.isnumeric() else token for token in sentence_tok]\n",
        "        # Add sentence start and end tags <s></s>.\n",
        "        sentence_tok.insert(0,\"<s>\")\n",
        "        sentence_tok.append(\"</s>\")\n",
        "        #print(sentence_tok)\n",
        "        tokens.extend(sentence_tok)\n",
        "        #print(tokens)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "tokensf1 = tokenization(sentencesf1)\n",
        "tokensf2 = tokenization(sentencesf2)\n",
        "\n",
        "#print(tokensf1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NIu5MSdcFl3G"
      },
      "source": [
        "def unit_frequency(tokens):\n",
        "    \"\"\"\n",
        "    Identifies tokens that apear only once in the data set\n",
        "    and replaces them with <UKN> tag\n",
        "    :param tokens:\n",
        "    :return: tokens with <UKN> tag\n",
        "    \"\"\"\n",
        "    tokens_final = [\"<UKN>\" if tokens.count(token) == 1 else token for token in tokens]\n",
        "    return tokens_final\n",
        "\n",
        "\n",
        "#tokensf1 = unit_frequency(tokensf1)\n",
        "#tokensf2 = unit_frequency(tokensf2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "d6GDl_RoFl3G"
      },
      "source": [
        "## Training and Testing Division"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "w1YOk7kmFl3H"
      },
      "source": [
        "def sentence_selection(tokens):\n",
        "    \"\"\"\n",
        "    Dvidides the document in test and training set\n",
        "    :param tokens: list of tokens\n",
        "    :return: sentences of training and test sets\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    i = 0\n",
        "    for token in tokens:\n",
        "        if \"<s>\" in token:\n",
        "            new_sentence = [token]\n",
        "        elif \"</s>\" in token:\n",
        "            new_sentence.append(token)\n",
        "            sentences.append(new_sentence)\n",
        "        else:\n",
        "            new_sentence.append(token)\n",
        "    percentage = math.floor(len(sentences)*0.8)\n",
        "    training = random.sample(sentences, percentage)\n",
        "    test = [ sentence for sentence in sentences if sentence not in training ]\n",
        "\n",
        "    return training, test\n",
        "\n",
        "\n",
        "(training1, test1) = sentence_selection(tokensf1)\n",
        "(training2, test2) = sentence_selection(tokensf2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ure6H5RGFl3H"
      },
      "source": [
        "#lista = [['hola','amigo'],['hola1','amigo1'],['hola2','amigo2'],['hola3','amigo3']]\n",
        "#porcentaje = math.floor(len(lista)*0.8)\n",
        "#training = random.sample(lista, porcentaje)\n",
        "#test = [ sentence for sentence in lista if sentence not in training ]\n",
        "\n",
        "#print(training)\n",
        "#print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FPRlHu5IFl3I"
      },
      "source": [
        "def writing_files(file, training, testing):\n",
        "    \"\"\"\n",
        "    Writes the correspondind file\n",
        "    :param file: 1 for 20N 2 for BAC\n",
        "    :param training: list of sentences of the training set\n",
        "    :param testing: list of sentences of the testing set\n",
        "    :return: nothing\n",
        "    \"\"\"\n",
        "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
        "    path_training = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_training\")\n",
        "    path_testing = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_testing\")\n",
        "    outfile = open(path_training,'wb')\n",
        "    pickle.dump(training,outfile)\n",
        "    outfile.close()\n",
        "\n",
        "    outfile = open(path_testing,'wb')\n",
        "    pickle.dump(testing,outfile)\n",
        "    outfile.close()\n",
        "\n",
        "writing_files(1, training1, test1)\n",
        "writing_files(2, training2, test2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Xg4N0lJWFl3J"
      },
      "source": [
        "def reading_files(file):\n",
        "    \"\"\"\n",
        "    Reads the testing and trianing file for a specific dataset\n",
        "    :param file: 1 for 20N, 2 for BAC\n",
        "    :return: the training and testing sets\n",
        "    \"\"\"\n",
        "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
        "    path_training = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_training\")\n",
        "    path_testing = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_testing\")\n",
        "    with open(path_training, 'rb') as training:\n",
        "        training = pickle.load(training)\n",
        "    with open(path_testing, 'rb') as testing:\n",
        "        testing = pickle.load(testing)\n",
        "    return training, testing\n",
        "\n",
        "\n",
        "training1, test1 = reading_files(1)\n",
        "training2, test2 = reading_files(2)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TzBQFfTsFl3K"
      },
      "source": [
        "## N-grams Modelling with Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ps0XWyYYFl3L"
      },
      "source": [
        "#This code was adapted from https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853\n",
        "\n",
        "def ngrams_generation(training):\n",
        "    \"\"\"\n",
        "    Generates the ngrams models\n",
        "    :param training: the traingin list of sentences\n",
        "    :return: a dictionary containing the unigram\n",
        "    bigram and trigram probabilities and ngrams\n",
        "    \"\"\"\n",
        "    tokenized_text = training\n",
        "    ngrams_all = {1:[], 2:[], 3:[]}\n",
        "    for i in range(3):\n",
        "        for each in tokenized_text:\n",
        "            for j in ngrams(each, i+1):\n",
        "                ngrams_all[i+1].append(j);\n",
        "\n",
        "    ngrams_voc = {1:set([]), 2:set([]), 3:set([])}\n",
        "\n",
        "    for i in range(3):\n",
        "        for gram in ngrams_all[i+1]:\n",
        "            if gram not in ngrams_voc[i+1]:\n",
        "                ngrams_voc[i+1].add(gram)\n",
        "    total_ngrams = {1:-1, 2:-1, 3:-1}\n",
        "    total_voc = {1:-1, 2:-1, 3:-1}\n",
        "\n",
        "    for i in range(3):\n",
        "        total_ngrams[i+1] = len(ngrams_all[i+1])\n",
        "        total_voc[i+1] = len(ngrams_voc[i+1])\n",
        "\n",
        "    ngrams_prob = {1:[], 2:[], 3:[]}\n",
        "    for i in range(3):\n",
        "        for ngram in ngrams_voc[i+1]:\n",
        "            tlist = [ngram]\n",
        "            tlist.append(ngrams_all[i+1].count(ngram))\n",
        "            ngrams_prob[i+1].append(tlist)\n",
        "\n",
        "    for i in range(3):\n",
        "        for ngram in ngrams_prob[i+1]:\n",
        "            ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])\n",
        "    return ngrams_prob\n",
        "\n",
        "ngrams_prob_1 = ngrams_generation(training1)\n",
        "ngrams_prob_2 = ngrams_generation(training2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oJyE37l9Fl3M"
      },
      "source": [
        "#Prints top 10 unigram, bigram, trigram after smoothing\n",
        "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
        "for i in range(3):\n",
        "    ngrams_prob_1[i+1] = sorted(ngrams_prob_1[i+1], key = lambda x:x[1], reverse = True)\n",
        "\n",
        "print (\"Most common unigrams: \", str(ngrams_prob_1[1][:10]))\n",
        "print (\"\\nMost common bigrams: \", str(ngrams_prob_1[2][:10]))\n",
        "print (\"\\nMost common trigrams: \", str(ngrams_prob_1[3][:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qXLATWmgFl3N"
      },
      "source": [
        "## File Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7tsnbwESFl3N"
      },
      "source": [
        "def writing_models(file, ngrams_prob):\n",
        "    \"\"\"\n",
        "    Taking the ngram model, writes the corresponding files for\n",
        "    each one of them for the specific dataset\n",
        "    :param file: 1 for 20N 2 for BAC\n",
        "    :param ngrams_prob: the ngram dictionary that contains\n",
        "    each model\n",
        "    :return: nothing\n",
        "    \"\"\"\n",
        "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
        "    path_unigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_unigram\")\n",
        "    path_bigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_bigram\")\n",
        "    path_trigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_trigram\")\n",
        "    outfile = open(path_unigram,'wb')\n",
        "    pickle.dump(ngrams_prob[1],outfile)\n",
        "    outfile.close()\n",
        "\n",
        "    outfile = open(path_bigram,'wb')\n",
        "    pickle.dump(ngrams_prob[2],outfile)\n",
        "    outfile.close()\n",
        "\n",
        "    outfile = open(path_trigram,'wb')\n",
        "    pickle.dump(ngrams_prob[3],outfile)\n",
        "    outfile.close()\n",
        "\n",
        "writing_models(1, ngrams_prob_1)\n",
        "writing_models(2, ngrams_prob_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN4eUAt_GBDj"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ewNNO9RhFl3O"
      },
      "source": [
        "def reading_models(file):\n",
        "    \"\"\"\n",
        "    Reads the n-grams model for each dataset\n",
        "    :param file: 1 for 20N or 2 for BAC\n",
        "    :return: unigram, bigram, trigram models\n",
        "    \"\"\"\n",
        "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
        "    path_unigram = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_unigram\")\n",
        "    path_bigram = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_bigram\")\n",
        "    path_trigram = os.path.join(os.getcwd(), 'drive/MyDrive/datasets/'+file_name+\"_2_trigram\")\n",
        "    with open(path_unigram, 'rb') as unigram:\n",
        "        unigram = pickle.load(unigram)\n",
        "    with open(path_bigram, 'rb') as bigram:\n",
        "        bigram = pickle.load(bigram)\n",
        "    with open(path_trigram, 'rb') as trigram:\n",
        "        trigram = pickle.load(trigram)\n",
        "    return unigram, bigram, trigram\n",
        "\n",
        "\n",
        "unigram_1, bigram_1, trigram_1  = reading_models(1)\n",
        "unigram_2, bigram_2, trigram_2  = reading_models(2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uGy_cmMHFl3P"
      },
      "source": [
        "## Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sXmPnhFBFl3Q"
      },
      "source": [
        "def perplexity(test, n, ngram):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity for each model\n",
        "    :param test: the test set\n",
        "    :param n: the number of words to consider in the n gram\n",
        "    :param ngram: the model\n",
        "    :return: the value of the perplexity\n",
        "    \"\"\"\n",
        "    perp = 1;\n",
        "    N = 0\n",
        "    for sentence in test:\n",
        "        for j in range(len(sentence)):\n",
        "            for gram in ngram:\n",
        "                for i in range(n):\n",
        "                    encontrado = True\n",
        "                    if j-i >= 0 and sentence[j-i] == gram[0][n-i-1]:\n",
        "                       encontrado = encontrado and True\n",
        "                    else:\n",
        "                       encontrado = encontrado and False\n",
        "                       break\n",
        "                    if i == n-1 and encontrado:\n",
        "                        perp = perp + math.log(gram[1])\n",
        "                        #print(perp)\n",
        "                        N = N+1\n",
        "    #print(perp)\n",
        "    #print(N)\n",
        "    perplexity = math.exp((-1/N)*perp)\n",
        "    return perp**(1/N)\n",
        "\n",
        "uni_perp_1 = perplexity(test1,1,unigram_1)\n",
        "print(\"Unigram Perplexity 20N\", uni_perp_1)\n",
        "bi_perp_1 = perplexity(test1,2,bigram_1)\n",
        "print(\"Bigram Perplexity 20N\", bi_perp_1)\n",
        "tri_perp_1 = perplexity(test1,3,trigram_1)\n",
        "print(\"Trigram Perplexity 20N\", tri_perp_1)\n",
        "\n",
        "uni_perp_2 = perplexity(test2,1,unigram_2)\n",
        "print(\"Unigram Perplexity 20N\", uni_perp_2)\n",
        "bi_perp_2 = perplexity(test2,2,bigram_2)\n",
        "print(\"Bigram Perplexity 20N\", bi_perp_2)\n",
        "tri_perp_2 = perplexity(test2,3,trigram_2)\n",
        "print(\"Trigram Perplexity 20N\", tri_perp_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "czpAILckFl3R"
      },
      "source": [
        "## Linear Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "pXORfiQSFl3R"
      },
      "source": [
        "def linear_interpolation(ngrams_prob):\n",
        "    \"\"\"\n",
        "    Generates the linear interpolation model\n",
        "    :param ngrams_prob: dictiornary of ngrams models.\n",
        "    :return: the linear interpolation model\n",
        "    \"\"\"\n",
        "    linear = []\n",
        "    lambda1 = 1/3\n",
        "    lambda2 = 1/3\n",
        "    lambda3 = 1/3\n",
        "\n",
        "    for trigram in ngrams_prob[3]:\n",
        "        for bigram in ngrams_prob[2]:\n",
        "            for unigram in ngrams_prob[1]:\n",
        "                if trigram[0][-1] == bigram[0][-1] and trigram[0][-2] == trigram[0][-2] and trigram[0][-1] == unigram[0][0]:\n",
        "                    prob = lambda1*trigram[1] + lambda2*bigram[1] + lambda3*unigram[1]\n",
        "                    linear.append([trigram[0],prob])\n",
        "\n",
        "    return linear\n",
        "\n",
        "linear_1 = linear_interpolation(training1)\n",
        "linear_2 = linear_interpolation(training1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "K4MvhFotFl3S"
      },
      "source": [
        "#Prints top 10 trigrams after smothing\n",
        "print(\"Most common n-grams with linear interpolation: \\n\")\n",
        "linear_1 = sorted(linear_1, key = lambda x:x[1], reverse = True)\n",
        "\n",
        "print (\"\\nMost common trigrams: \", str(linear_1[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "rg9fEPpJFl3U"
      },
      "source": [
        "## Perplexity Comparrison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "Zi3p_bHrFl3V"
      },
      "source": [
        "# Caclulates the perplexity of linear interpolation\n",
        "linear_perp_1 = perplexity(test1,3,linear_1)\n",
        "linear_perp_2 = perplexity(test2,3,linear_1)\n",
        "print(\"Linear Interpolation Perplexity 20N\", linear_perp_1)\n",
        "print(\"Linear Interpolation Perplexity BAC\", linear_perp_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "CP5TYjBLFl3V"
      },
      "source": [
        "# Generates the corresponding table\n",
        "\n",
        "raw_data = {'Model': [\"Unigram\", \"Bigram\", \"Trigram\",\"Linear Interpolation\"],\n",
        "        'Perplexities_20N': [uni_perp_1, bi_perp_1, tri_perp_1, linear_perp_1],\n",
        "        'Perplexities_BAC': [uni_perp_2, bi_perp_2, tri_perp_2, linear_perp_2],}\n",
        "\n",
        "df = pd.DataFrame(raw_data, columns = ['Model', 'Perplexities_20N', 'Perplexities_BAC'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0lAiDC4GFl3W"
      },
      "source": [
        "## Sentence Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "baX_6-B2Fl3X"
      },
      "source": [
        "def sentence_generator(word, ngram):\n",
        "    \"\"\"\n",
        "    Funcion that given an initial word, generates\n",
        "    a sentence\n",
        "    :param word: the initial (or current word) that is beeing considered\n",
        "    :param ngram: the ngram model beeing used\n",
        "    :param sentence: the given sentence till now. At first is just \"\"\n",
        "    :return: the final sentence\n",
        "    \"\"\"\n",
        "    max = 100\n",
        "    count = 0\n",
        "    sentence = \"\"\n",
        "    while word != \"</s>\" and count < max :\n",
        "      for gram in ngram:\n",
        "          if word == gram[0][-1]:\n",
        "              sentence = sentence + \" \" + gram[0][-2]\n",
        "              word = gram[0][-2]\n",
        "              count = count +1\n",
        "              break\n",
        "    return sentece\n",
        "\n",
        "linear_1 = sorted(linear_1[i+1], key = lambda x:x[1], reverse = True)\n",
        "\n",
        "# Different tests with different words.\n",
        "\n",
        "test_sentence = sentence_generator(\"friend\", linear_1, \"\")\n",
        "print(\"Test Sentence 1\", test_sentence)\n",
        "\n",
        "test_sentence = sentence_generator(\"politics\", linear_1, \"\")\n",
        "print(\"Test Sentence 2\", test_sentence)\n",
        "\n",
        "test_sentence = sentence_generator(\"i\", linear_1, \"\")\n",
        "print(\"Test Sentence 3\", test_sentence)\n",
        "\n",
        "test_sentence = sentence_generator(\"Once\", linear_1, \"\")\n",
        "print(\"Test Sentence 4\", test_sentence)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}