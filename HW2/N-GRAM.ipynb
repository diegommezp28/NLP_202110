{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# N-Gram Language Models Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "import nltk.data\n",
    "import random\n",
    "import math\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% imports\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading 20N dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo terminado\n"
     ]
    }
   ],
   "source": [
    "def read_document_20N():\n",
    "    \"\"\"\n",
    "    This method reads and writes the 20N Dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_file = os.path.join(os.getcwd(), 'datasets/20news_file')\n",
    "    if os.path.exists(new_file):\n",
    "        os.remove(new_file)\n",
    "\n",
    "    folder_path = os.path.join(os.getcwd(), 'datasets/20news-18828')\n",
    "    inner_dirs = os.listdir(folder_path)\n",
    "    for dir in inner_dirs:\n",
    "        if not dir.startswith('.'):\n",
    "            #print(dir)\n",
    "            dir_path = os.path.join(folder_path,dir)\n",
    "            filenames = os.listdir(dir_path)\n",
    "            for file in filenames:\n",
    "                cur_path = os.path.join(dir_path,file)\n",
    "                #print(\"Copying \"+file)\n",
    "                with open(cur_path,'r', errors=\"ignore\") as firstfile, open(new_file,'a') as secondfile:\n",
    "                    for line in firstfile:\n",
    "                        secondfile.write(line)\n",
    "    print(\"Archivo terminado\")\n",
    "\n",
    "\n",
    "read_document_20N()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading BAC dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo terminado\n"
     ]
    }
   ],
   "source": [
    "def read_document_BAC():\n",
    "    \"\"\"\n",
    "    This method reads and writes the BAC Dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_file = os.path.join(os.getcwd(), 'datasets/bac_file')\n",
    "    if os.path.exists(new_file):\n",
    "        os.remove(new_file)\n",
    "\n",
    "    folder_path = os.path.join(os.getcwd(), 'datasets/blogs')\n",
    "    files = os.listdir(folder_path)\n",
    "    #files.sort()\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        cur_path = os.path.join(folder_path,file)\n",
    "        with open(cur_path,'r', errors=\"ignore\") as firstfile, open(new_file,'a') as secondfile:\n",
    "            for line in firstfile:\n",
    "                if len(line)>8 and not len(line)==28:\n",
    "                    #print(len(line))\n",
    "                    #print(line.strip())\n",
    "                    secondfile.write(line.strip())\n",
    "    print(\"Archivo terminado\")\n",
    "\n",
    "\n",
    "read_document_BAC()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize by sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def sentences(path):\n",
    "    \"\"\"\n",
    "    Divides the document in a List of sentences.\n",
    "    :return: a list with sentences\n",
    "    \"\"\"\n",
    "    news_file = os.path.join(os.getcwd(), path)\n",
    "    text = open(news_file).read()\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(text.strip())\n",
    "    return sentences\n",
    "\n",
    "sentencesf1 = sentences(\"datasets/20news_file\")\n",
    "sentencesf2 = sentences(\"datasets/bac_file\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reduce data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def reduce_data(sentences):\n",
    "    \"\"\"\n",
    "    Reduces randomly the sentences in order to speed up the\n",
    "    processing time according to a specific percentage\n",
    "    :param sentences: the complete list of sentences\n",
    "    :return: the reduces sentences\n",
    "    \"\"\"\n",
    "    percentage = math.floor(len(sentences)*0.001)\n",
    "    sentences = random.sample(sentences, percentage)\n",
    "    return sentences\n",
    "\n",
    "sentencesf1 = reduce_data(sentencesf1)\n",
    "sentencesf2 = reduce_data(sentencesf2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def tokenization(sentences):\n",
    "    \"\"\"\n",
    "    Tokenization of the sentences array.\n",
    "    :return: tokens for each sentence\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        #print(sentence)\n",
    "        # Normalize, but DO NOT eliminate stop words.\n",
    "        lower_text = sentence.lower()\n",
    "        sentence_tok = [token for token in nltk.word_tokenize(lower_text) if (token.isalnum())]\n",
    "        if len(sentence_tok)== 0:\n",
    "            continue\n",
    "        # Replace numbers with a token named NUM.\n",
    "        sentence_tok = [\"NUM\" if token.isnumeric() else token for token in sentence_tok]\n",
    "        # Add sentence start and end tags <s></s>.\n",
    "        sentence_tok.insert(0,\"<s>\")\n",
    "        sentence_tok.append(\"</s>\")\n",
    "        #print(sentence_tok)\n",
    "        tokens.extend(sentence_tok)\n",
    "        #print(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokensf1 = tokenization(sentencesf1)\n",
    "tokensf2 = tokenization(sentencesf2)\n",
    "\n",
    "#print(tokensf1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def unit_frequency(tokens):\n",
    "    \"\"\"\n",
    "    Identifies tokens that apear only once in the data set\n",
    "    and replaces them with <UKN> tag\n",
    "    :param tokens:\n",
    "    :return: tokens with <UKN> tag\n",
    "    \"\"\"\n",
    "    tokens_final = [\"<UKN>\" if tokens.count(token) == 1 else token for token in tokens]\n",
    "    return tokens_final\n",
    "\n",
    "\n",
    "#tokensf1 = unit_frequency(tokensf1)\n",
    "#tokensf2 = unit_frequency(tokensf2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Testing Division"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def sentence_selection(tokens):\n",
    "    \"\"\"\n",
    "    Dvidides the document in test and training set\n",
    "    :param tokens: list of tokens\n",
    "    :return: sentences of training and test sets\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    for token in tokens:\n",
    "        if \"<s>\" in token:\n",
    "            new_sentence = [token]\n",
    "        elif \"</s>\" in token:\n",
    "            new_sentence.append(token)\n",
    "            sentences.append(new_sentence)\n",
    "        else:\n",
    "            new_sentence.append(token)\n",
    "    percentage = math.floor(len(sentences)*0.8)\n",
    "    training = random.sample(sentences, percentage)\n",
    "    test = [ sentence for sentence in sentences if sentence not in training ]\n",
    "\n",
    "    return training, test\n",
    "\n",
    "\n",
    "(training1, test1) = sentence_selection(tokensf1)\n",
    "(training2, test2) = sentence_selection(tokensf2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#lista = [['hola','amigo'],['hola1','amigo1'],['hola2','amigo2'],['hola3','amigo3']]\n",
    "#porcentaje = math.floor(len(lista)*0.8)\n",
    "#training = random.sample(lista, porcentaje)\n",
    "#test = [ sentence for sentence in lista if sentence not in training ]\n",
    "\n",
    "#print(training)\n",
    "#print(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def writing_files(file, training, testing):\n",
    "    \"\"\"\n",
    "    Writes the correspondind file\n",
    "    :param file: 1 for 20N 2 for BAC\n",
    "    :param training: list of sentences of the training set\n",
    "    :param testing: list of sentences of the testing set\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
    "    path_training = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_training\")\n",
    "    path_testing = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_testing\")\n",
    "    outfile = open(path_training,'wb')\n",
    "    pickle.dump(training,outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    outfile = open(path_testing,'wb')\n",
    "    pickle.dump(testing,outfile)\n",
    "    outfile.close()\n",
    "\n",
    "writing_files(1, training1, test1)\n",
    "writing_files(2, training2, test2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def reading_files(file):\n",
    "    \"\"\"\n",
    "    Reads the testing and trianing file for a specific dataset\n",
    "    :param file: 1 for 20N, 2 for BAC\n",
    "    :return: the training and testing sets\n",
    "    \"\"\"\n",
    "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
    "    path_training = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_training\")\n",
    "    path_testing = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_testing\")\n",
    "    with open(path_training, 'rb') as training:\n",
    "        training = pickle.load(training)\n",
    "    with open(path_testing, 'rb') as testing:\n",
    "        testing = pickle.load(testing)\n",
    "    return training, testing\n",
    "\n",
    "\n",
    "training1, test1 = reading_files(1)\n",
    "training2, test2 = reading_files(2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-grams Modelling with Laplace Smoothing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#This code was adapted from https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853\n",
    "\n",
    "def ngrams_generation(training):\n",
    "    \"\"\"\n",
    "    Generates the ngrams models\n",
    "    :param training: the traingin list of sentences\n",
    "    :return: a dictionary containing the unigram\n",
    "    bigram and trigram probabilities and ngrams\n",
    "    \"\"\"\n",
    "    tokenized_text = training\n",
    "    ngrams_all = {1:[], 2:[], 3:[]}\n",
    "    for i in range(3):\n",
    "        for each in tokenized_text:\n",
    "            for j in ngrams(each, i+1):\n",
    "                ngrams_all[i+1].append(j);\n",
    "\n",
    "    ngrams_voc = {1:set([]), 2:set([]), 3:set([])}\n",
    "\n",
    "    for i in range(3):\n",
    "        for gram in ngrams_all[i+1]:\n",
    "            if gram not in ngrams_voc[i+1]:\n",
    "                ngrams_voc[i+1].add(gram)\n",
    "    total_ngrams = {1:-1, 2:-1, 3:-1}\n",
    "    total_voc = {1:-1, 2:-1, 3:-1}\n",
    "\n",
    "    for i in range(3):\n",
    "        total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "        total_voc[i+1] = len(ngrams_voc[i+1])\n",
    "\n",
    "    ngrams_prob = {1:[], 2:[], 3:[]}\n",
    "    for i in range(3):\n",
    "        for ngram in ngrams_voc[i+1]:\n",
    "            tlist = [ngram]\n",
    "            tlist.append(ngrams_all[i+1].count(ngram))\n",
    "            ngrams_prob[i+1].append(tlist)\n",
    "\n",
    "    for i in range(3):\n",
    "        for ngram in ngrams_prob[i+1]:\n",
    "            ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])\n",
    "    return ngrams_prob\n",
    "\n",
    "ngrams_prob_1 = ngrams_generation(training1)\n",
    "ngrams_prob_2 = ngrams_generation(training2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and with add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [[('<s>',), 0.04005572971090213], [('</s>',), 0.04005572971090213], [('the',), 0.03692093347265761], [('of',), 0.017938000696621385], [('to',), 0.01654475792406827], [('a',), 0.014280738418669453], [('and',), 0.014280738418669453], [('i',), 0.012365029606408917], [('NUM',), 0.012365029606408917], [('in',), 0.01079763148728666]]\n",
      "\n",
      "Most common bigrams:  [[('<s>', 'i'), 0.0041812786619908286], [('o', 'o'), 0.003506878877798759], [('of', 'the'), 0.002967359050445104], [('<s>', 'the'), 0.00283247909360669], [('in', 'the'), 0.0022929592662530346], [('NUM', '</s>'), 0.0016185594820609657], [('to', 'the'), 0.001483679525222552], [('for', 'the'), 0.001483679525222552], [('it', 'is'), 0.001348799568384138], [('on', 'the'), 0.0012139196115457244]]\n",
      "\n",
      "Most common trigrams:  [[('o', 'o', 'o'), 0.00337108953613808], [('NUM', 'to', 'NUM'), 0.000674217907227616], [('<s>', 'i', 'am'), 0.0005393743257820927], [('<s>', 'i', 'do'), 0.0005393743257820927], [('the', 'case', '</s>'), 0.0005393743257820927], [('NUM', 'NUM', '</s>'), 0.0005393743257820927], [('a', 'sign', 'to'), 0.0004045307443365696], [('<s>', 'i', 'can'), 0.0004045307443365696], [('while', 'it', 'is'), 0.0004045307443365696], [('into', 'a', 'NUM'), 0.0004045307443365696]]\n"
     ]
    }
   ],
   "source": [
    "#Prints top 10 unigram, bigram, trigram after smoothing\n",
    "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
    "for i in range(3):\n",
    "    ngrams_prob_1[i+1] = sorted(ngrams_prob_1[i+1], key = lambda x:x[1], reverse = True)\n",
    "\n",
    "print (\"Most common unigrams: \", str(ngrams_prob_1[1][:10]))\n",
    "print (\"\\nMost common bigrams: \", str(ngrams_prob_1[2][:10]))\n",
    "print (\"\\nMost common trigrams: \", str(ngrams_prob_1[3][:10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## File Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def writing_models(file, ngrams_prob):\n",
    "    \"\"\"\n",
    "    Taking the ngram model, writes the corresponding files for\n",
    "    each one of them for the specific dataset\n",
    "    :param file: 1 for 20N 2 for BAC\n",
    "    :param ngrams_prob: the ngram dictionary that contains\n",
    "    each model\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
    "    path_unigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_unigram\")\n",
    "    path_bigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_bigram\")\n",
    "    path_trigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_trigram\")\n",
    "    outfile = open(path_unigram,'wb')\n",
    "    pickle.dump(ngrams_prob[1],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    outfile = open(path_bigram,'wb')\n",
    "    pickle.dump(ngrams_prob[2],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    outfile = open(path_trigram,'wb')\n",
    "    pickle.dump(ngrams_prob[3],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "writing_models(1, ngrams_prob_1)\n",
    "writing_models(2, ngrams_prob_2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def reading_models(file):\n",
    "    \"\"\"\n",
    "    Reads the n-grams model for each dataset\n",
    "    :param file: 1 for 20N or 2 for BAC\n",
    "    :return: unigram, bigram, trigram models\n",
    "    \"\"\"\n",
    "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
    "    path_unigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_unigram\")\n",
    "    path_bigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_bigram\")\n",
    "    path_trigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_trigram\")\n",
    "    with open(path_unigram, 'rb') as unigram:\n",
    "        unigram = pickle.load(unigram)\n",
    "    with open(path_bigram, 'rb') as bigram:\n",
    "        bigram = pickle.load(bigram)\n",
    "    with open(path_trigram, 'rb') as trigram:\n",
    "        trigram = pickle.load(trigram)\n",
    "    return unigram, bigram, trigram\n",
    "\n",
    "\n",
    "unigram_1, bigram_1, trigram_1  = reading_models(1)\n",
    "unigram_2, bigram_2, trigram_2  = reading_models(2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perplexity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766\n",
      "198\n",
      "24\n",
      "Unigram Perplexity 20N inf\n",
      "Bigram Perplexity 20N inf\n",
      "Trigram Perplexity 20N 3383.6047106519104\n",
      "27676\n",
      "15366\n",
      "4407\n",
      "Unigram Perplexity 20N inf\n",
      "Bigram Perplexity 20N inf\n",
      "Trigram Perplexity 20N inf\n"
     ]
    }
   ],
   "source": [
    "def perplexity(test, n, ngram):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity for each model\n",
    "    :param test: the test set\n",
    "    :param n: the number of words to consider in the n gram\n",
    "    :param ngram: the model\n",
    "    :return: the value of the perplexity\n",
    "    \"\"\"\n",
    "    perp = 1;\n",
    "    N = 0\n",
    "    for sentence in test:\n",
    "        for j in range(len(sentence)):\n",
    "            for gram in ngram:\n",
    "                for i in range(n):\n",
    "                    encontrado = True\n",
    "                    if j-i >= 0 and sentence[j-i] == gram[0][n-i-1]:\n",
    "                       encontrado = encontrado and True\n",
    "                    else:\n",
    "                       encontrado = encontrado and False\n",
    "                       break\n",
    "                    if i == n-1 and encontrado:\n",
    "                        perp = perp*(1/gram[1])\n",
    "                        #print(perp)\n",
    "                        N = N+1\n",
    "    #print(perp)\n",
    "    print(N)\n",
    "    return perp**(1/N)\n",
    "\n",
    "uni_perp_1 = perplexity(test1,1,unigram_1)\n",
    "bi_perp_1 = perplexity(test1,2,bigram_1)\n",
    "tri_perp_1 = perplexity(test1,3,trigram_1)\n",
    "print(\"Unigram Perplexity 20N\", uni_perp_1)\n",
    "print(\"Bigram Perplexity 20N\", bi_perp_1)\n",
    "print(\"Trigram Perplexity 20N\", tri_perp_1)\n",
    "\n",
    "uni_perp_2 = perplexity(test2,1,unigram_2)\n",
    "bi_perp_2 = perplexity(test2,2,bigram_2)\n",
    "tri_perp_2 = perplexity(test2,3,trigram_2)\n",
    "print(\"Unigram Perplexity 20N\", uni_perp_2)\n",
    "print(\"Bigram Perplexity 20N\", bi_perp_2)\n",
    "print(\"Trigram Perplexity 20N\", tri_perp_2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Interpolation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def linear_interpolation(training):\n",
    "    \"\"\"\n",
    "    Generates the linear interpolation model\n",
    "    :param training: training list of setences tokenized\n",
    "    :return: the linear interpolation model\n",
    "    \"\"\"\n",
    "    tokenized_text = training\n",
    "    ngrams_all = {1:[], 2:[], 3:[]}\n",
    "    for i in range(3):\n",
    "        for each in tokenized_text:\n",
    "            for j in ngrams(each, i+1):\n",
    "                ngrams_all[i+1].append(j);\n",
    "\n",
    "    ngrams_voc = {1:set([]), 2:set([]), 3:set([])}\n",
    "\n",
    "    for i in range(3):\n",
    "        for gram in ngrams_all[i+1]:\n",
    "            if gram not in ngrams_voc[i+1]:\n",
    "                ngrams_voc[i+1].add(gram)\n",
    "    total_ngrams = {1:-1, 2:-1, 3:-1}\n",
    "    total_voc = {1:-1, 2:-1, 3:-1}\n",
    "\n",
    "    for i in range(3):\n",
    "        total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "        total_voc[i+1] = len(ngrams_voc[i+1])\n",
    "\n",
    "    ngrams_prob = {1:[], 2:[], 3:[]}\n",
    "    for i in range(3):\n",
    "        for ngram in ngrams_voc[i+1]:\n",
    "            tlist = [ngram]\n",
    "            tlist.append(ngrams_all[i+1].count(ngram)/(total_ngrams[i+1]))\n",
    "            ngrams_prob[i+1].append(tlist)\n",
    "\n",
    "    linear = []\n",
    "    lambda1 = 1/3\n",
    "    lambda2 = 1/3\n",
    "    lambda3 = 1/3\n",
    "\n",
    "    for trigram in ngrams_prob[3]:\n",
    "        for bigram in ngrams_prob[2]:\n",
    "            for unigram in ngrams_prob[1]:\n",
    "                if trigram[0][-1] == bigram[0][-1] and trigram[0][-2] == trigram[0][-2] and trigram[0][-1] == unigram[0][0]:\n",
    "                    prob = lambda1*trigram[1] + lambda2*bigram[1] + lambda3*unigram[1]\n",
    "                    linear.append([trigram[0],prob])\n",
    "\n",
    "    return linear\n",
    "\n",
    "linear_1 = linear_interpolation(training1)\n",
    "linear_2 = linear_interpolation(training1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Prints top 10 trigrams after smothing\n",
    "print(\"Most common n-grams with linear interpolation: \\n\")\n",
    "linear_1 = sorted(linear_1, key = lambda x:x[1], reverse = True)\n",
    "\n",
    "print (\"\\nMost common trigrams: \", str(linear_1[:10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perplexity Comparrison"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Caclulates the perplexity of linear interpolation\n",
    "linear_perp_1 = perplexity(test1,3,linear_1)\n",
    "linear_perp_2 = perplexity(test2,3,linear_1)\n",
    "print(\"Linear Interpolation Perplexity 20N\", linear_perp_1)\n",
    "print(\"Linear Interpolation Perplexity BAC\", linear_perp_2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Generates the corresponding table\n",
    "\n",
    "raw_data = {'Model': [\"Unigram\", \"Bigram\", \"Trigram\",\"Linear Interpolation\"],\n",
    "        'Perplexities_20N': [uni_perp_1, bi_perp_1, tri_perp_1, linear_perp_1],\n",
    "        'Perplexities_BAC': [uni_perp_2, bi_perp_2, tri_perp_2, linear_perp_2],}\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns = ['Model', 'Perplexities_20N', 'Perplexities_BAC'])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentence_generator(word, ngram, sentence):\n",
    "    \"\"\"\n",
    "    Recursive funcion that given an initial word, generates\n",
    "    a sentence\n",
    "    :param word: the initial (or current word) that is beeing considered\n",
    "    :param ngram: the ngram model beeing used\n",
    "    :param sentence: the given sentence till now. At first is just \"\"\n",
    "    :return: the final sentence\n",
    "    \"\"\"\n",
    "    if word == \"</s>\":\n",
    "        return sentence\n",
    "    else:\n",
    "        for gram in ngram:\n",
    "            if word == gram[0][-1]:\n",
    "                sentence = sentence + \" \" + gram[0][-2]\n",
    "                word = gram[0][-2]\n",
    "                break\n",
    "        sentence_generator(word, ngram, sentence)\n",
    "\n",
    "linear_1 = sorted(linear_1[i+1], key = lambda x:x[1], reverse = True)\n",
    "\n",
    "# Different tests with different words.\n",
    "\n",
    "test_sentence = sentence_generator(\"friend\", linear_1, \"\")\n",
    "print(\"Test Sentence 1\", test_sentence)\n",
    "\n",
    "test_sentence = sentence_generator(\"politics\", linear_1, \"\")\n",
    "print(\"Test Sentence 2\", test_sentence)\n",
    "\n",
    "test_sentence = sentence_generator(\"i\", linear_1, \"\")\n",
    "print(\"Test Sentence 3\", test_sentence)\n",
    "\n",
    "test_sentence = sentence_generator(\"Once\", linear_1, \"\")\n",
    "print(\"Test Sentence 4\", test_sentence)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}