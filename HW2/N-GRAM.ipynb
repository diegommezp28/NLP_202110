{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# N-Gram Language Models Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "import nltk.data\n",
    "import random\n",
    "import math\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% imports\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading 20N dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo terminado\n"
     ]
    }
   ],
   "source": [
    "def read_document_20N():\n",
    "    \"\"\"\n",
    "    This method reads and writes the 20N Dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_file = os.path.join(os.getcwd(), 'datasets/20news_file')\n",
    "    if os.path.exists(new_file):\n",
    "        os.remove(new_file)\n",
    "\n",
    "    folder_path = os.path.join(os.getcwd(), 'datasets/20news-18828')\n",
    "    inner_dirs = os.listdir(folder_path)\n",
    "    for dir in inner_dirs:\n",
    "        if not dir.startswith('.'):\n",
    "            #print(dir)\n",
    "            dir_path = os.path.join(folder_path,dir)\n",
    "            filenames = os.listdir(dir_path)\n",
    "            for file in filenames:\n",
    "                cur_path = os.path.join(dir_path,file)\n",
    "                #print(\"Copying \"+file)\n",
    "                with open(cur_path,'r', errors=\"ignore\") as firstfile, open(new_file,'a') as secondfile:\n",
    "                    for line in firstfile:\n",
    "                        secondfile.write(line)\n",
    "    print(\"Archivo terminado\")\n",
    "\n",
    "\n",
    "read_document_20N()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading BAC dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo terminado\n"
     ]
    }
   ],
   "source": [
    "def read_document_BAC():\n",
    "    \"\"\"\n",
    "    This method reads and writes the BAC Dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_file = os.path.join(os.getcwd(), 'datasets/bac_file')\n",
    "    if os.path.exists(new_file):\n",
    "        os.remove(new_file)\n",
    "\n",
    "    folder_path = os.path.join(os.getcwd(), 'datasets/blogs')\n",
    "    files = os.listdir(folder_path)\n",
    "    #files.sort()\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        cur_path = os.path.join(folder_path,file)\n",
    "        with open(cur_path,'r', errors=\"ignore\") as firstfile, open(new_file,'a') as secondfile:\n",
    "            for line in firstfile:\n",
    "                if len(line)>8 and not len(line)==28:\n",
    "                    #print(len(line))\n",
    "                    #print(line.strip())\n",
    "                    secondfile.write(line.strip())\n",
    "    print(\"Archivo terminado\")\n",
    "\n",
    "\n",
    "read_document_BAC()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize by sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def sentences(path):\n",
    "    \"\"\"\n",
    "    Divides the document in a List of sentences.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    news_file = os.path.join(os.getcwd(), path)\n",
    "    text = open(news_file).read()\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(text.strip())\n",
    "    return sentences\n",
    "\n",
    "sentencesf1 = sentences(\"datasets/20news_file\")\n",
    "sentencesf2 = sentences(\"datasets/bac_file\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def tokenization(sentences):\n",
    "    \"\"\"\n",
    "    Tokenization of the sentences array.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        #print(sentence)\n",
    "        # Normalize, but DO NOT eliminate stop words.\n",
    "        lower_text = sentence.lower()\n",
    "        sentence_tok = [token for token in nltk.word_tokenize(lower_text) if (token.isalnum())]\n",
    "        if len(sentence_tok)== 0:\n",
    "            continue\n",
    "        # Replace numbers with a token named NUM.\n",
    "        sentence_tok = [\"NUM\" if token.isnumeric() else token for token in sentence_tok]\n",
    "        # Add sentence start and end tags <s></s>.\n",
    "        sentence_tok.insert(0,\"<s>\")\n",
    "        sentence_tok.append(\"</s>\")\n",
    "        #print(sentence_tok)\n",
    "        tokens.extend(sentence_tok)\n",
    "        #print(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokensf1 = tokenization(sentencesf1)\n",
    "tokensf2 = tokenization(sentencesf2)\n",
    "\n",
    "#print(tokensf1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-56-ec6872248187>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mtokensf1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0munit_frequency\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokensf1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mtokensf2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0munit_frequency\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokensf2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-56-ec6872248187>\u001B[0m in \u001B[0;36munit_frequency\u001B[0;34m(tokens)\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;34m:\u001B[0m\u001B[0;32mreturn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \"\"\"\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0mtokens_final\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"<UKN>\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtokens\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtokens\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtokens_final\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-56-ec6872248187>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;34m:\u001B[0m\u001B[0;32mreturn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \"\"\"\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0mtokens_final\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"<UKN>\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtokens\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtokens\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtokens_final\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def unit_frequency(tokens):\n",
    "    \"\"\"\n",
    "\n",
    "    :param tokens:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens_final = [\"<UKN>\" if tokens.count(token) == 1 else token for token in tokens]\n",
    "    return tokens_final\n",
    "\n",
    "\n",
    "tokensf1 = unit_frequency(tokensf1)\n",
    "tokensf2 = unit_frequency(tokensf2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Testing Division"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-57-e9e3639523b7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m \u001B[0;34m(\u001B[0m\u001B[0mtraining1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest1\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msentence_selection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokensf1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m \u001B[0;31m#(training2, test2) = sentence_selection(tokensf2)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-57-e9e3639523b7>\u001B[0m in \u001B[0;36msentence_selection\u001B[0;34m(tokens)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0mpercentage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;36m0.8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mtraining\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpercentage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m     \u001B[0mtest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msentences\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtraining\u001B[0m \u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-57-e9e3639523b7>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0mpercentage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;36m0.8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mtraining\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpercentage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m     \u001B[0mtest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msentences\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtraining\u001B[0m \u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def sentence_selection(tokens):\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    for token in tokens:\n",
    "        if \"<s>\" in token:\n",
    "            new_sentence = [token]\n",
    "        elif \"</s>\" in token:\n",
    "            new_sentence.append(token)\n",
    "            sentences.append(new_sentence)\n",
    "        else:\n",
    "            new_sentence.append(token)\n",
    "    percentage = math.floor(len(sentences)*0.8)\n",
    "    training = random.sample(sentences, percentage)\n",
    "    test = [ sentence for sentence in sentences if sentence not in training ]\n",
    "\n",
    "    return training, test\n",
    "\n",
    "\n",
    "(training1, test1) = sentence_selection(tokensf1)\n",
    "(training2, test2) = sentence_selection(tokensf2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#lista = [['hola','amigo'],['hola1','amigo1'],['hola2','amigo2'],['hola3','amigo3']]\n",
    "#porcentaje = math.floor(len(lista)*0.8)\n",
    "#training = random.sample(lista, porcentaje)\n",
    "#test = [ sentence for sentence in lista if sentence not in training ]\n",
    "\n",
    "#print(training)\n",
    "#print(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def writing_files(file, training, testing):\n",
    "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
    "    path_training = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_training\")\n",
    "    path_testing = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_testing\")\n",
    "    outfile = open(path_training,'wb')\n",
    "    pickle.dump(training,outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    outfile = open(path_testing,'wb')\n",
    "    pickle.dump(testing,outfile)\n",
    "    outfile.close()\n",
    "\n",
    "writing_files(1, training1, test1)\n",
    "writing_files(2, training2, test2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def reading_files(file):\n",
    "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
    "    path_training = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_training\")\n",
    "    path_testing = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_testing\")\n",
    "    with open(path_training, 'rb') as training:\n",
    "        training = pickle.load(training)\n",
    "    with open(path_testing, 'rb') as testing:\n",
    "        testing = pickle.load(testing)\n",
    "    return training, testing\n",
    "\n",
    "\n",
    "training1, test1 = reading_files(1)\n",
    "training2, test2 = reading_files(2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def crop_data(training, test):\n",
    "    percentage = 0.005\n",
    "\n",
    "    training_small = training[:round(len(training)*percentage)]\n",
    "    test_small = test[:round(len(test)*percentage)]\n",
    "    return training_small,test_small\n",
    "\n",
    "training1_small, test1_small =  crop_data(training1, test1)\n",
    "training2_small, test2_small =  crop_data(training2, test2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-grams Modelling with Laplace Smoothing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def ngrams_generation(training):\n",
    "    tokenized_text = training\n",
    "    ngrams_all = {1:[], 2:[], 3:[]}\n",
    "    for i in range(3):\n",
    "        for each in tokenized_text:\n",
    "            for j in ngrams(each, i+1):\n",
    "                ngrams_all[i+1].append(j);\n",
    "\n",
    "    ngrams_voc = {1:set([]), 2:set([]), 3:set([])}\n",
    "\n",
    "    for i in range(3):\n",
    "        for gram in ngrams_all[i+1]:\n",
    "            if gram not in ngrams_voc[i+1]:\n",
    "                ngrams_voc[i+1].add(gram)\n",
    "    total_ngrams = {1:-1, 2:-1, 3:-1}\n",
    "    total_voc = {1:-1, 2:-1, 3:-1}\n",
    "\n",
    "    for i in range(3):\n",
    "        total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "        total_voc[i+1] = len(ngrams_voc[i+1])\n",
    "\n",
    "    ngrams_prob = {1:[], 2:[], 3:[]}\n",
    "    for i in range(3):\n",
    "        for ngram in ngrams_voc[i+1]:\n",
    "            tlist = [ngram]\n",
    "            tlist.append(ngrams_all[i+1].count(ngram))\n",
    "            ngrams_prob[i+1].append(tlist)\n",
    "\n",
    "    for i in range(3):\n",
    "        for ngram in ngrams_prob[i+1]:\n",
    "            ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])\n",
    "    return ngrams_prob\n",
    "\n",
    "ngrams_prob_1 = ngrams_generation(training1_small)\n",
    "ngrams_prob_2 = ngrams_generation(training2_small)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and with add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [[('the',), 0.03246727410177854], [('to',), 0.018740301595511877], [('of',), 0.016512155333625114], [('a',), 0.016512155333625114], [('and',), 0.013806549158476903], [('NUM',), 0.012811841005848885], [('in',), 0.012772052679743764], [('is',), 0.010225599809016035], [('i',), 0.009668563243544344], [('that',), 0.009390044960808499]]\n",
      "\n",
      "Most common bigrams:  [[('of', 'the'), 0.0025376830647873545], [('in', 'the'), 0.00250710857003088], [('NUM', 'NUM'), 0.0017427462011190265], [('subject', 're'), 0.0016815972116060783], [('on', 'the'), 0.0010701073164765952], [('is', 'a'), 0.0010395328217201211], [('to', 'the'), 0.001008958326963647], [('to', 'be'), 0.0008866603479377503], [('for', 'the'), 0.0008866603479377503], [('with', 'the'), 0.0008560858531812762]]\n",
      "\n",
      "Most common trigrams:  [[('NUM', 'NUM', 'NUM'), 0.0007150518412584912], [('writes', 'in', 'article'), 0.00023835061375283043], [('NUM', 'NUM', 'NUM</s>'), 0.00020855678703372662], [('the', 'fact', 'that'), 0.00020855678703372662], [('one', 'of', 'the'), 0.0001787629603146228], [('r', 'r', 'oo'), 0.0001787629603146228], [('right', 'to', 'match'), 0.0001787629603146228], [('r', 'oo', 'oo'), 0.000148969133595519], [('the', 'number', 'of'), 0.000148969133595519], [('in', 'article', 'NUM'), 0.000148969133595519]]\n"
     ]
    }
   ],
   "source": [
    "#Prints top 10 unigram, bigram, trigram after smoothing\n",
    "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
    "for i in range(3):\n",
    "    ngrams_prob_1[i+1] = sorted(ngrams_prob_1[i+1], key = lambda x:x[1], reverse = True)\n",
    "\n",
    "print (\"Most common unigrams: \", str(ngrams_prob_1[1][:10]))\n",
    "print (\"\\nMost common bigrams: \", str(ngrams_prob_1[2][:10]))\n",
    "print (\"\\nMost common trigrams: \", str(ngrams_prob_1[3][:10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## File Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def writing_models(file, ngrams_prob):\n",
    "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
    "    path_unigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_unigram\")\n",
    "    path_bigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_bigram\")\n",
    "    path_trigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_trigram\")\n",
    "    outfile = open(path_unigram,'wb')\n",
    "    pickle.dump(ngrams_prob[1],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    outfile = open(path_bigram,'wb')\n",
    "    pickle.dump(ngrams_prob[2],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    outfile = open(path_trigram,'wb')\n",
    "    pickle.dump(ngrams_prob[3],outfile)\n",
    "    outfile.close()\n",
    "\n",
    "writing_models(1, ngrams_prob_1)\n",
    "writing_models(2, ngrams_prob_2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def reading_models(file):\n",
    "    file_name = \"20N\" if file == 1 else \"BAC\"\n",
    "    path_unigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_unigram\")\n",
    "    path_bigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_bigram\")\n",
    "    path_trigram = os.path.join(os.getcwd(), 'datasets/'+file_name+\"_2_trigram\")\n",
    "    with open(path_unigram, 'rb') as unigram:\n",
    "        unigram = pickle.load(unigram)\n",
    "    with open(path_bigram, 'rb') as bigram:\n",
    "        bigram = pickle.load(bigram)\n",
    "    with open(path_bigram, 'rb') as trigram:\n",
    "        trigram = pickle.load(trigram)\n",
    "    return unigram, bigram, trigram\n",
    "\n",
    "\n",
    "unigram_1, bigram_1, trigram_1  = reading_models(1)\n",
    "unigram_2, bigram_2, trigram_2  = reading_models(2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perplexity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity inf\n",
      "Bigram Perplexity inf\n",
      "Trigram Perplexity inf\n"
     ]
    }
   ],
   "source": [
    "def perplexity(test, n, ngram):\n",
    "    perp = 1;\n",
    "    N = 0\n",
    "    for sentence in test:\n",
    "        for j in range(len(sentence)):\n",
    "            for gram in ngram:\n",
    "                for i in range(n):\n",
    "                    encontrado = True\n",
    "                    if j-i >= 0 and sentence[j-i] == gram[0][n-i-1]:\n",
    "                       encontrado = encontrado and True\n",
    "                    else:\n",
    "                       encontrado = encontrado and False\n",
    "                       break\n",
    "                    if i == n-1 and encontrado:\n",
    "                        perp = perp*(1/gram[1])\n",
    "                        #print(perp)\n",
    "                        N = N+1\n",
    "    #print(perp)\n",
    "    print(N)\n",
    "    return perp**(1/N)\n",
    "\n",
    "uni_perp_1 = perplexity(test1_small,1,unigram_1)\n",
    "bi_perp_1 = perplexity(test1_small,2,bigram_1)\n",
    "tri_perp_1 = perplexity(test1_small,3,trigram_1)\n",
    "print(\"Unigram Perplexity 20N\", uni_perp_1)\n",
    "print(\"Bigram Perplexity 20N\", bi_perp_1)\n",
    "print(\"Trigram Perplexity 20N\", tri_perp_1)\n",
    "\n",
    "uni_perp_2 = perplexity(test2_small,1,unigram_2)\n",
    "bi_perp_2 = perplexity(test2_small,2,bigram_2)\n",
    "tri_perp_2 = perplexity(test2_small,3,trigram_2)\n",
    "print(\"Unigram Perplexity 20N\", uni_perp_2)\n",
    "print(\"Bigram Perplexity 20N\", bi_perp_2)\n",
    "print(\"Trigram Perplexity 20N\", tri_perp_2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Interpolation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-26-a2976fd8bfee>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     41\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mlinear\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m \u001B[0mlinear\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlinear_interpolation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining1_small\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-26-a2976fd8bfee>\u001B[0m in \u001B[0;36mlinear_interpolation\u001B[0;34m(training)\u001B[0m\n\u001B[1;32m     35\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mbigram\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mngrams_prob\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0munigram\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mngrams_prob\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m                 \u001B[0;32mif\u001B[0m \u001B[0mtrigram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mbigram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtrigram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mtrigram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtrigram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0munigram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     38\u001B[0m                     \u001B[0mprob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlambda1\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mtrigram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlambda2\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mbigram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlambda3\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0munigram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m                     \u001B[0mlinear\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtrigram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mprob\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def linear_interpolation(training):\n",
    "    tokenized_text = training\n",
    "    ngrams_all = {1:[], 2:[], 3:[]}\n",
    "    for i in range(3):\n",
    "        for each in tokenized_text:\n",
    "            for j in ngrams(each, i+1):\n",
    "                ngrams_all[i+1].append(j);\n",
    "\n",
    "    ngrams_voc = {1:set([]), 2:set([]), 3:set([])}\n",
    "\n",
    "    for i in range(3):\n",
    "        for gram in ngrams_all[i+1]:\n",
    "            if gram not in ngrams_voc[i+1]:\n",
    "                ngrams_voc[i+1].add(gram)\n",
    "    total_ngrams = {1:-1, 2:-1, 3:-1}\n",
    "    total_voc = {1:-1, 2:-1, 3:-1}\n",
    "\n",
    "    for i in range(3):\n",
    "        total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "        total_voc[i+1] = len(ngrams_voc[i+1])\n",
    "\n",
    "    ngrams_prob = {1:[], 2:[], 3:[]}\n",
    "    for i in range(3):\n",
    "        for ngram in ngrams_voc[i+1]:\n",
    "            tlist = [ngram]\n",
    "            tlist.append(ngrams_all[i+1].count(ngram)/(total_ngrams[i+1]))\n",
    "            ngrams_prob[i+1].append(tlist)\n",
    "\n",
    "    linear = []\n",
    "    lambda1 = 1/3\n",
    "    lambda2 = 1/3\n",
    "    lambda3 = 1/3\n",
    "\n",
    "    for trigram in ngrams_prob[3]:\n",
    "        for bigram in ngrams_prob[2]:\n",
    "            for unigram in ngrams_prob[1]:\n",
    "                if trigram[0][-1] == bigram[0][-1] and trigram[0][-2] == trigram[0][-2] and trigram[0][-1] == unigram[0][0]:\n",
    "                    prob = lambda1*trigram[1] + lambda2*bigram[1] + lambda3*unigram[1]\n",
    "                    linear.append([trigram[0],prob])\n",
    "\n",
    "    return linear\n",
    "\n",
    "linear_1 = linear_interpolation(training1_small)\n",
    "linear_2 = linear_interpolation(training1_small)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Prints top 10 trigrams after smothing\n",
    "print(\"Most common n-grams with linear interpolation: \\n\")\n",
    "linear_1 = sorted(linear_1, key = lambda x:x[1], reverse = True)\n",
    "\n",
    "print (\"\\nMost common trigrams: \", str(linear_1[:10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perplexity Comparrison"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "linear_perp_1 = perplexity(test1_small,3,linear_1)\n",
    "linear_perp_2 = perplexity(test2_small,3,linear_1)\n",
    "print(\"Linear Interpolation Perplexity 20N\", linear_perp_1)\n",
    "print(\"Linear Interpolation Perplexity BAC\", linear_perp_2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "_data = {'Model': [uni_perp_1, bi_perp_1, tri_perp_1, linear_perp_1],\n",
    "        'Perplexities_20N': [uni_perp_1, bi_perp_1, tri_perp_1, linear_perp_1],\n",
    "        'Perplexities_BAC': [uni_perp_2, bi_perp_2, tri_perp_2, linear_perp_2],\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns = ['Model', 'Perplexities_20N', 'Perplexities_BAC'])\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\nraw",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentence_generator(word, ngram, sentence):\n",
    "    if word == \"</s>\":\n",
    "        return sentence\n",
    "    else:\n",
    "        for gram in ngram:\n",
    "            if word == gram[0][-1]:\n",
    "                sentence = sentence + \" \" + gram[0][-2]\n",
    "                word = gram[0][-2]\n",
    "                break\n",
    "        sentence_generator(word, ngram, sentence)\n",
    "\n",
    "linear_1\n",
    "linear_1 = sorted(linear_1[i+1], key = lambda x:x[1], reverse = True)\n",
    "\n",
    "test_sentence = sentence_generator(\"friend\", linear_1, \"\")\n",
    "print(\"Test Sentence 1\", test_sentence)\n",
    "\n",
    "test_sentence = sentence_generator(\"politics\", linear_1, \"\")\n",
    "print(\"Test Sentence 2\", test_sentence)\n",
    "\n",
    "test_sentence = sentence_generator(\"i\", linear_1, \"\")\n",
    "print(\"Test Sentence 3\", test_sentence)\n",
    "\n",
    "test_sentence = sentence_generator(\"Once\", linear_1, \"\")\n",
    "print(\"Test Sentence 4\", test_sentence)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}