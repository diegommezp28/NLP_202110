{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web Scraping\n",
    "Isabela"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "\n",
    "import praw\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Twitter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authentication"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('credentials/twitter_credentials') as f:\n",
    "  credentials = json.load(f)\n",
    "#print(credentials)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "consumer_key = credentials[\"API_key\"]\n",
    "consumer_secret = credentials[\"API_secret_key\"]\n",
    "access_token = credentials[\"access_token\"]\n",
    "access_token_secret = credentials[\"access_token_secret\"]\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "text_query = 'COVID'\n",
    "count = 150\n",
    "try:\n",
    " # Creation of query method using parameters\n",
    " tweets = tweepy.Cursor(api.search,q=text_query + \"-filter:retweets\", lang=\"IT\", locale=\"IT\").items(count)\n",
    "\n",
    " # Pulling information from tweets iterable object\n",
    " tweets_list = [[tweet.created_at, tweet.id, tweet.text] for tweet in tweets]\n",
    "\n",
    " # Creation of dataframe from tweets list\n",
    " # Add or remove columns as you remove tweet information\n",
    " tweets_df = pd.DataFrame(tweets_list)\n",
    "\n",
    "except BaseException as e:\n",
    "    print('failed on_status,',str(e))\n",
    "    time.sleep(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                    0                    1  \\\n0 2021-04-04 11:10:49  1378666361459597312   \n1 2021-04-02 15:04:47  1378000464147546112   \n2 2021-04-01 04:19:42  1377475736672423936   \n3 2021-03-30 10:28:01  1376843650873884674   \n\n                                                   2  \n0  not saying there aren't any young people dying...  \n1  L’Azienda Careggi sta contattando direttamente...  \n2  i know y'all are enjoying pranking one another...  \n3  Tre tipi di aziende su cui puntare nell'Europa...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-04-04 11:10:49</td>\n      <td>1378666361459597312</td>\n      <td>not saying there aren't any young people dying...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-04-02 15:04:47</td>\n      <td>1378000464147546112</td>\n      <td>L’Azienda Careggi sta contattando direttamente...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-04-01 04:19:42</td>\n      <td>1377475736672423936</td>\n      <td>i know y'all are enjoying pranking one another...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-03-30 10:28:01</td>\n      <td>1376843650873884674</td>\n      <td>Tre tipi di aziende su cui puntare nell'Europa...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second Approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 500000 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 200 tweets\n",
      "Downloaded 300 tweets\n",
      "Downloaded 400 tweets\n",
      "Downloaded 500 tweets\n",
      "Downloaded 600 tweets\n",
      "Downloaded 700 tweets\n",
      "Downloaded 800 tweets\n",
      "Downloaded 900 tweets\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 1100 tweets\n",
      "Downloaded 1200 tweets\n",
      "Downloaded 1300 tweets\n",
      "Downloaded 1400 tweets\n",
      "Downloaded 1500 tweets\n",
      "Downloaded 1600 tweets\n",
      "Downloaded 1700 tweets\n",
      "Downloaded 1800 tweets\n",
      "Downloaded 1900 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 2100 tweets\n",
      "Downloaded 2200 tweets\n",
      "Downloaded 2300 tweets\n",
      "Downloaded 2400 tweets\n",
      "Downloaded 2500 tweets\n",
      "Downloaded 2600 tweets\n",
      "Downloaded 2700 tweets\n",
      "Downloaded 2800 tweets\n",
      "Downloaded 2900 tweets\n",
      "Downloaded 3000 tweets\n",
      "Downloaded 3100 tweets\n",
      "Downloaded 3199 tweets\n",
      "Downloaded 3299 tweets\n",
      "Downloaded 3399 tweets\n",
      "Downloaded 3499 tweets\n",
      "Downloaded 3599 tweets\n",
      "Downloaded 3699 tweets\n",
      "Downloaded 3798 tweets\n",
      "Downloaded 3898 tweets\n",
      "Downloaded 3997 tweets\n",
      "Downloaded 4097 tweets\n",
      "Downloaded 4197 tweets\n",
      "Downloaded 4297 tweets\n",
      "Downloaded 4397 tweets\n",
      "Downloaded 4497 tweets\n",
      "Downloaded 4597 tweets\n",
      "Downloaded 4697 tweets\n",
      "Downloaded 4797 tweets\n",
      "Downloaded 4897 tweets\n",
      "Downloaded 4997 tweets\n",
      "Downloaded 5097 tweets\n",
      "Downloaded 5196 tweets\n",
      "Downloaded 5296 tweets\n",
      "Downloaded 5396 tweets\n",
      "Downloaded 5496 tweets\n",
      "Downloaded 5596 tweets\n",
      "Downloaded 5696 tweets\n",
      "Downloaded 5796 tweets\n",
      "Downloaded 5896 tweets\n",
      "Downloaded 5996 tweets\n",
      "Downloaded 6096 tweets\n",
      "Downloaded 6196 tweets\n",
      "Downloaded 6295 tweets\n",
      "Downloaded 6395 tweets\n",
      "Downloaded 6495 tweets\n",
      "Downloaded 6595 tweets\n",
      "Downloaded 6695 tweets\n",
      "Downloaded 6795 tweets\n",
      "Downloaded 6895 tweets\n",
      "Downloaded 6994 tweets\n",
      "Downloaded 7094 tweets\n",
      "Downloaded 7194 tweets\n",
      "Downloaded 7294 tweets\n",
      "Downloaded 7394 tweets\n",
      "Downloaded 7494 tweets\n",
      "Downloaded 7594 tweets\n",
      "Downloaded 7694 tweets\n",
      "Downloaded 7794 tweets\n",
      "Downloaded 7894 tweets\n",
      "Downloaded 7994 tweets\n",
      "Downloaded 8094 tweets\n",
      "Downloaded 8194 tweets\n",
      "Downloaded 8294 tweets\n",
      "Downloaded 8394 tweets\n",
      "Downloaded 8494 tweets\n",
      "Downloaded 8594 tweets\n",
      "Downloaded 8694 tweets\n",
      "Downloaded 8794 tweets\n",
      "Downloaded 8894 tweets\n",
      "Downloaded 8994 tweets\n",
      "Downloaded 9094 tweets\n",
      "Downloaded 9194 tweets\n",
      "Downloaded 9294 tweets\n",
      "Downloaded 9394 tweets\n",
      "Downloaded 9494 tweets\n",
      "Downloaded 9594 tweets\n",
      "Downloaded 9694 tweets\n",
      "Downloaded 9794 tweets\n",
      "Downloaded 9894 tweets\n",
      "Downloaded 9994 tweets\n",
      "Downloaded 10094 tweets\n",
      "Downloaded 10194 tweets\n",
      "Downloaded 10294 tweets\n",
      "Downloaded 10394 tweets\n",
      "Downloaded 10494 tweets\n",
      "Downloaded 10594 tweets\n",
      "Downloaded 10694 tweets\n",
      "Downloaded 10793 tweets\n",
      "Downloaded 10893 tweets\n",
      "Downloaded 10993 tweets\n",
      "Downloaded 11093 tweets\n",
      "Downloaded 11193 tweets\n",
      "Downloaded 11293 tweets\n",
      "Downloaded 11393 tweets\n",
      "Downloaded 11493 tweets\n",
      "Downloaded 11593 tweets\n",
      "Downloaded 11693 tweets\n",
      "Downloaded 11793 tweets\n",
      "Downloaded 11893 tweets\n",
      "Downloaded 11993 tweets\n",
      "Downloaded 12093 tweets\n",
      "Downloaded 12193 tweets\n",
      "Downloaded 12293 tweets\n",
      "Downloaded 12393 tweets\n",
      "Downloaded 12492 tweets\n",
      "some error : Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\n",
      "Downloaded 12492 tweets, Saved to data/tweets1.txt\n"
     ]
    }
   ],
   "source": [
    "searchQuery = 'covid'  # this is what we're searching for\n",
    "maxTweets = 500000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "fName = 'data/tweets1.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "\n",
    "places = []\n",
    "time = []\n",
    "tweets = []\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "\n",
    "                #add data to lists\n",
    "\n",
    "                #1. creted at\n",
    "                time.append((tweet.created_at.month,tweet.created_at.year))\n",
    "\n",
    "                #location of user\n",
    "                places.append(tweet.user.location)\n",
    "\n",
    "                #text of tweet\n",
    "                tweets.append(tweet.text)\n",
    "\n",
    "                f.write(jsonpickle.encode(tweet.text, unpicklable=False) +'\\n')\n",
    "\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reddit\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authentication"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "with open('credentials/reddit_credentials') as f:\n",
    "  credentials2 = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1000 posts and 4059 comments, Saved to data/reddit10000.txt\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id=credentials2[\"client_id\"], client_secret=credentials2[\"client_secret\"], user_agent=credentials2['user_agent'])\n",
    "fName = 'data/reddit10000.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "\n",
    "hot_posts = reddit.subreddit('Covid').top(limit=10000)\n",
    "posts = 0\n",
    "comments = 0\n",
    "with open(fName, 'w') as f:\n",
    "    for post in hot_posts:\n",
    "        post.comments.replace_more(limit=None)\n",
    "        #print(\"Title\",post.title)\n",
    "        f.write(jsonpickle.encode(post.title, unpicklable=False) +'\\n')\n",
    "        #print(\"Self Test\",post.selftext)\n",
    "        f.write(jsonpickle.encode(post.selftext, unpicklable=False) +'\\n')\n",
    "        posts = posts +1\n",
    "        for top_level_comment in post.comments:\n",
    "            #print(\"C:\",top_level_comment.body)\n",
    "            f.write(jsonpickle.encode(top_level_comment.body, unpicklable=False) +'\\n')\n",
    "            comments = comments + 1\n",
    "\n",
    "\n",
    "print (\"Downloaded {0} posts and {1} comments, Saved to {2}\".format(posts,comments, fName))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1000 posts and 4059 comments, Saved to data/reddit1.txt\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id=credentials2[\"client_id\"], client_secret=credentials2[\"client_secret\"], user_agent=credentials2['user_agent'])\n",
    "fName = 'data/reddit1.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "\n",
    "hot_posts = reddit.subreddit('Covid').top(limit=10000)\n",
    "posts = 0\n",
    "comments = 0\n",
    "with open(fName, 'w') as f:\n",
    "    for post in hot_posts:\n",
    "        post.comments.replace_more(limit=None)\n",
    "        #print(\"Title\",post.title)\n",
    "        f.write(jsonpickle.encode(post.title, unpicklable=False) +'\\n')\n",
    "        #print(\"Self Test\",post.selftext)\n",
    "        f.write(jsonpickle.encode(post.selftext, unpicklable=False) +'\\n')\n",
    "        posts = posts +1\n",
    "        for top_level_comment in post.comments:\n",
    "            #print(\"C:\",top_level_comment.body)\n",
    "            f.write(jsonpickle.encode(top_level_comment.body, unpicklable=False) +'\\n')\n",
    "            comments = comments + 1\n",
    "\n",
    "\n",
    "print (\"Downloaded {0} posts and {1} comments, Saved to {2}\".format(posts,comments, fName))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}