{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Punto 4 - Español\n",
    "Context Q&A system about COVID in a general setting. Type: Transformer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle as pk\n",
    "import string, re\n",
    "import torch\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Configuraciones generales"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "do_fine_tuning_locally = True\n",
    "\n",
    "lang = \"spanish\"\n",
    "\n",
    "'''\n",
    "Se deben crear los directorios en caso de no existir\n",
    "'''\n",
    "\n",
    "\n",
    "#HPC calculated files\n",
    "input_encodings_path = f\"data_hpc/{lang}/train_encoding_{lang}.pkl\"\n",
    "input_model_path = f\"data_hpc/{lang}/input_model_{lang}.pkl\"\n",
    "output_model_path = f\"data_hpc/{lang}/model_post_tunning_{lang}.pkl\"\n",
    "\n",
    "#Path for model to be stored\n",
    "model_folder_path = f'models/{lang}/qna_{lang}_custom'\n",
    "\n",
    "#Dataset\n",
    "dataset_path = f\"qna_database/{lang}/{lang}.json\"\n",
    "\n",
    "#BERT model name\n",
    "modelname = 'mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proceso de carga del Dataset traducido"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Contextos 926, Preguntas 926, Respuestas 926\n"
     ]
    }
   ],
   "source": [
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "with open(dataset_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        doc = json.loads(line)\n",
    "        contexts.append(doc[\"context\"])\n",
    "        answers.append(doc[\"answer\"])\n",
    "        questions.append(doc[\"question\"])\n",
    "\n",
    "print(f\"Contextos {str(len(contexts))}, Preguntas {str(len(questions))}, Respuestas {str(len(answers))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "30410.082089552237\n"
     ]
    }
   ],
   "source": [
    "def stats(contexts):\n",
    "    contextos = {}\n",
    "    for i in range(len(contexts)):\n",
    "        contexto = contexts[i]\n",
    "        titulo = contexto.partition(\"\\n\")[0];\n",
    "        if not titulo in contextos:\n",
    "            contextos[titulo] = contexto\n",
    "    char_size = 0\n",
    "    for contextos_unicos in contextos.values():\n",
    "        char_size += len(contextos_unicos)\n",
    "    print(len(contextos))\n",
    "    return char_size/len(contextos)\n",
    "\n",
    "\n",
    "print(stats(contexts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Separación en training y test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Versión completa\n",
    "# num_docs = len(contexts)\n",
    "# ochenta = int(num_docs*0.8)\n",
    "#\n",
    "# train_contexts = contexts[0:ochenta]\n",
    "# train_questions = questions[0:ochenta]\n",
    "# train_answers = answers[0:ochenta]\n",
    "# test_contexts = contexts[ochenta+1:num_docs]\n",
    "# test_questions = questions[ochenta+1:num_docs]\n",
    "# test_answers = answers[ochenta+1:num_docs]\n",
    "\n",
    "#Verisón corta\n",
    "num_docs = 50\n",
    "ochenta = int(num_docs*0.8)\n",
    "\n",
    "train_contexts = contexts[0:ochenta]\n",
    "train_questions = questions[0:ochenta]\n",
    "train_answers = answers[0:ochenta]\n",
    "test_contexts = contexts[ochenta+1:num_docs]\n",
    "test_questions = questions[ochenta+1:num_docs]\n",
    "test_answers = answers[ochenta+1:num_docs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40\n9\n"
     ]
    }
   ],
   "source": [
    "print(len(train_contexts))\n",
    "print(len(test_contexts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aproximación 1 - Already Fine-Tuned BERT transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 465/465 [00:00<00:00, 232kB/s]\n",
      "Downloading: 100%|██████████| 439M/439M [00:33<00:00, 13.0MB/s]\n",
      "Downloading: 100%|██████████| 242k/242k [00:00<00:00, 1.37MB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 113kB/s]\n",
      "Downloading: 100%|██████████| 135/135 [00:00<00:00, 67.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(modelname)\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creación del pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pruebas Genéricas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'score': 0.4067336320877075,\n 'start': 304,\n 'end': 342,\n 'answer': 'acceso equitativo mundial a una vacuna'}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"La pandemia mundial ya ha causado cientos de miles de muertes y ha perturbado la vida de miles de millones de personas. Además de reducir el trágico número de muertes y ayudar a controlar la pandemia, la introducción de una vacuna evitará que la economía mundial pierda US$ 375 000 millones cada mes. El acceso equitativo mundial a una vacuna, con la que se protegerá especialmente a los trabajadores de la salud y a las personas que corren un mayor riesgo, es la única forma de mitigar las repercusiones de la pandemia en la salud pública y la economía.\"\n",
    "\n",
    "nlp({\n",
    "    'question': '¿Como se pueden mitigar las repercusiones de la pandemia?',\n",
    "    'context': context\n",
    "})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'score': 0.6369205713272095,\n 'start': 34,\n 'end': 61,\n 'answer': 'cientos de miles de muertes'}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"La pandemia mundial ya ha causado cientos de miles de muertes y ha perturbado la vida de miles de millones de personas. Además de reducir el trágico número de muertes y ayudar a controlar la pandemia, la introducción de una vacuna evitará que la economía mundial pierda US$ 375 000 millones cada mes. El acceso equitativo mundial a una vacuna, con la que se protegerá especialmente a los trabajadores de la salud y a las personas que corren un mayor riesgo, es la única forma de mitigar las repercusiones de la pandemia en la salud pública y la economía.\"\n",
    "\n",
    "nlp({\n",
    "    'question': '¿Que ha causado la pandemia?',\n",
    "    'context': context\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_prediction(qid, questions, contexts):\n",
    "    specific_question = questions[qid]\n",
    "    specific_context = contexts[qid]\n",
    "\n",
    "    answer = nlp({\n",
    "    'question': specific_question,\n",
    "    'context': specific_context\n",
    "    })\n",
    "    #print(answer)\n",
    "    return answer[\"answer\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Métricas para Q&A\n",
    "#### F1 Score y Exact Match"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#Adaptado de: Evaluating QA: Metrics, Predictions, and the Null Response\n",
    "#https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "### Dataset COVID-QA\n",
    "COVID-QA es un conjunto de datos de respuestas a preguntas que consta de 2,019 pares de preguntas / respuestas anotados por expertos biomédicos voluntarios en artículos científicos relacionados con COVID-19. Un total de 147 artículos científicos del conjunto de datos CORD-19 fueron anotados por 15 expertos.\n",
    "\n",
    "Tomado de: https://huggingface.co/datasets/covid_qa_deepset\n",
    "Traducido automaticamente al español"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# El formato del dataset tenía listas en vez de string e ints\n",
    "\n",
    "def ajustar_answers(answers):\n",
    "    for answer in answers:\n",
    "         answer['text'] = answer['text'][0]\n",
    "         answer['answer_start'] = int(answer['answer_start'][0])\n",
    "    return answers\n",
    "\n",
    "train_answers = ajustar_answers(train_answers)\n",
    "test_answers = ajustar_answers(test_answers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluación Aproximación 1\n",
    "Modelo ya Fine-Tuned para Q&A"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Prueba Unitaria:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ¿Qué análogo de nucleósido es el foco del estudio actual?\n",
      "Prediction: gemcitabina\n",
      "Answer: gemcitabina\n",
      "EM: 1 \t F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "prediction = get_prediction(n, questions=test_questions, contexts=test_contexts)\n",
    "#print(prediction)\n",
    "answer = test_answers[n][\"text\"]\n",
    "#print(answer)\n",
    "\n",
    "em_score = compute_exact_match(prediction, answer)\n",
    "f1_score = compute_f1(prediction, answer)\n",
    "\n",
    "print(f\"Question: {test_questions[n]}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Prueba con todos los datos de test:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def average_metrics(contexts, answers, questions):\n",
    "    if len(contexts)!= len(answers) or len(contexts)!= len(questions):\n",
    "        print(\"Hay un error en el tamaño de los arreglos\")\n",
    "        return\n",
    "    test_size = len(contexts)\n",
    "    em_average = 0\n",
    "    f1_average = 0\n",
    "    for n in range(test_size):\n",
    "        prediction = get_prediction(n, questions, contexts)\n",
    "        answer = answers[n][\"text\"]\n",
    "        em_score = compute_exact_match(prediction, answer)\n",
    "        f1_score = 1 if em_score == 1 else compute_f1(prediction, answer)\n",
    "        em_average += em_score\n",
    "        f1_average += f1_score\n",
    "    em_average_tot = em_average/test_size\n",
    "    f1_average_tot = f1_average/test_size\n",
    "    return em_average_tot, f1_average_tot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "em_average, f1_score = average_metrics(test_contexts, test_answers, test_questions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1: 0.5607496351978525 \n",
      "Average EM: 0.3157894736842105\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average F1: {f1_score} \\nAverage EM: {em_average}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aproximación 2 - Fine Tunnind with COVID-19 Q&A Questions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aproximación inspirada en\n",
    "https://gist.github.com/jamescalam/55daf50c8da9eb3a7c18de058bc139a3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40\n40\n9\n9\n"
     ]
    }
   ],
   "source": [
    "#Basado en:\n",
    "\n",
    "def add_end_idx(answers, contexts):\n",
    "    count = 0\n",
    "    print(len(answers))\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "            count = count+1\n",
    "        else:\n",
    "            for n in range(4):\n",
    "                problema = True\n",
    "                if gold_text == context[start_idx+n:end_idx+n]:\n",
    "                    # this means the answer is off by 'n' tokens\n",
    "                    answer['answer_start'] = start_idx + n\n",
    "                    answer['answer_end'] = end_idx + n\n",
    "                    #print(\"entra4\")\n",
    "                    count = count+1\n",
    "                    problema = False\n",
    "                    break\n",
    "                if gold_text == context[start_idx-n:end_idx-n]:\n",
    "                    answer['answer_start'] = start_idx - n\n",
    "                    answer['answer_end'] = end_idx - n\n",
    "                    count = count+1\n",
    "\n",
    "                    problema = False\n",
    "                    break\n",
    "            if problema:\n",
    "                print(\"1\",gold_text)\n",
    "                print(\"2\",context[start_idx+n:end_idx+n])\n",
    "    print(count)\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(test_answers, test_contexts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Nuevamente se importa el modelo\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(modelname)\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True\n",
    ", max_length=512, add_special_tokens = True, \n",
    ")\n",
    "test_encodings = tokenizer(test_contexts, test_questions, truncation=True, padding=True\n",
    ",max_length=512, add_special_tokens = True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    # initialize lists to contain the token indices of answer start/end\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        # append start/end token position using char_to_token method\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        # end position cannot be found, char_to_token found space, so shift one token forward\n",
    "        go_back = 1\n",
    "        while end_positions[-1] is None:\n",
    "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end']-go_back)\n",
    "            go_back +=1\n",
    "    # update our encodings object with the new token-based start/end positions\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "# apply function to our data\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(test_encodings, test_answers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings):\n",
    "            self.encodings = encodings\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "test_dataset = SquadDataset(test_encodings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "with open(input_encodings_path, \"wb\") as file:\n",
    "    pk.dump(train_dataset, file)\n",
    "\n",
    "with open(input_model_path, \"wb\") as file:\n",
    "    pk.dump(model, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (11746) must match the size of tensor b (512) at non-singleton dimension 1",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-e91732099168>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m             outputs = model(input_ids, attention_mask=attention_mask,\n\u001b[0;32m     34\u001b[0m                             \u001b[0mstart_positions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_positions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                             end_positions=end_positions)\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[1;31m# extract loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1795\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1797\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1798\u001b[0m         )\n\u001b[0;32m   1799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    970\u001b[0m         )\n\u001b[0;32m    971\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"absolute\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[0membeddings\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (11746) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Fine tune your model locally or excecute it with the HPC\n",
    "'''\n",
    "if do_fine_tuning_locally:\n",
    "    #val_dataset = SquadDataset(val_encodings)\n",
    "\n",
    "    # setup GPU/CPU\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # move model over to detected device\n",
    "    model.to(device)\n",
    "    # activate training mode of model\n",
    "    model.train()\n",
    "    # initialize adam optimizer with weight decay (reduces chance of overfitting)\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # initialize data loader for training data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        # setup loop (we use tqdm for the progress bar)\n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        for batch in loop:\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optim.zero_grad()\n",
    "            # pull all the tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "            # train model on batch and return outputs (incl. loss)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "            # extract loss\n",
    "            loss = outputs[0]\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "            # print relevant info to progress bar\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "else:\n",
    "    with open(output_model_path, \"rb\") as file:\n",
    "        model = pk.load(file)\n",
    "\n",
    "\n",
    "model.save_pretrained(model_folder_path)\n",
    "tokenizer.save_pretrained(model_folder_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2 = BertForQuestionAnswering.from_pretrained(model_folder_path)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_folder_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creación del pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = pipeline('question-answering', model=model2, tokenizer=tokenizer2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pruebas genéricas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "context = \"Masks should be used as part of a comprehensive strategy of measures to suppress transmission and save lives; the use of a mask alone is not sufficient to provide an adequate level of protection against COVID-19. If COVID-19 is spreading in your community, stay safe by taking some simple precautions, such as physical distancing, wearing a mask, keeping rooms well ventilated, avoiding crowds, cleaning your hands, and coughing into a bent elbow or tissue. Check local advice where you live and work. Do it all! Make wearing a mask a normal part of being around other people. The appropriate use, storage and cleaning or disposal of masks are essential to make them as effective as possible.\"\n",
    "\n",
    "nlp({\n",
    "    'question': 'What precautions can you take to stay safe?',\n",
    "    'context': context\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Taken from: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/covid-19-vaccines/advice\n",
    "\n",
    "context = \"The world is in the midst of a COVID-19 pandemic. As WHO and partners work together on the response -- tracking the pandemic, advising on critical interventions, distributing vital medical supplies to those in need--- they are racing to develop and deploy safe and effective vaccines. Vaccines save millions of lives each year. Vaccines work by training and preparing the body’s natural defences – the immune system – to recognize and fight off the viruses and bacteria they target. After vaccination, if the body is later exposed to those disease-causing germs, the body is immediately ready to destroy them, preventing illness. There are several safe and effective vaccines that prevent people from getting seriously ill or dying from COVID-19. This is one part of managing COVID-19, in addition to the main preventive measures of staying at least 1 metre away from others, covering a cough or sneeze in your elbow, frequently cleaning your hands, wearing a mask and avoiding poorly ventilated rooms or opening a window.\"\n",
    "\n",
    "nlp({\n",
    "    'question': 'What happens after vaccination?',\n",
    "    'context': context\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validación Aproximación 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " - Prueba de ejemplo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = 3\n",
    "prediction = get_prediction(n, questions=test_questions, contexts=test_contexts)\n",
    "#print(prediction)\n",
    "answer = test_answers[n][\"text\"]\n",
    "#print(answer)\n",
    "\n",
    "em_score = compute_exact_match(prediction, answer)\n",
    "f1_score = 1 if em_score == 1 else compute_f1(prediction, answer)\n",
    "\n",
    "print(f\"Question: {test_questions[n]}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def average_metrics(contexts, answers, questions):\n",
    "    if len(contexts)!= len(answers) or len(contexts)!= len(questions):\n",
    "        print(\"Hay un error en el tamaño de los arreglos\")\n",
    "        return\n",
    "    test_size = len(contexts)\n",
    "    em_average = 0\n",
    "    f1_average = 0\n",
    "    for n in range(test_size):\n",
    "        #print(n)\n",
    "        prediction = get_prediction(n, questions, contexts)\n",
    "        #print(\"Prediction: \",prediction)\n",
    "        answer = answers[n][\"text\"]\n",
    "        #print(\"Answer: \",answer)\n",
    "        em_score = compute_exact_match(prediction, answer)\n",
    "        f1_score = 1 if em_score == 1 else compute_f1(prediction, answer)\n",
    "        #print(f\"EM: {em_score} \\t F1: {f1_score}\")\n",
    "        em_average += em_score\n",
    "        f1_average += f1_score\n",
    "    em_average_tot = em_average/test_size\n",
    "    f1_average_tot = f1_average/test_size\n",
    "    return em_average_tot, f1_average_tot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "em_average, f1_score = average_metrics( test_contexts, test_answers, test_questions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Average F1: {f1_score} \\nAverage EM: {em_average}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd06c70104609522f67cb30e8200dfa6f77a5bec0e1b1538a23f62dccdf26f51f37",
   "display_name": "Python 3.7.10 64-bit ('TF-Keras': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}