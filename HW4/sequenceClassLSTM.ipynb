{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd06c70104609522f67cb30e8200dfa6f77a5bec0e1b1538a23f62dccdf26f51f37",
   "display_name": "Python 3.7.10 64-bit ('TF-Keras': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import pickle "
   ]
  },
  {
   "source": [
    "Loading the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   raw_character_text                         spoken_words\n",
       "1        Lisa Simpson               Where's Mr. Bergstrom?\n",
       "3        Lisa Simpson           That life is worth living.\n",
       "7        Bart Simpson       Victory party under the slide!\n",
       "9        Lisa Simpson        Mr. Bergstrom! Mr. Bergstrom!\n",
       "11       Lisa Simpson  Do you know where I could find him?"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>raw_character_text</th>\n      <th>spoken_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Lisa Simpson</td>\n      <td>Where's Mr. Bergstrom?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Lisa Simpson</td>\n      <td>That life is worth living.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Bart Simpson</td>\n      <td>Victory party under the slide!</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Lisa Simpson</td>\n      <td>Mr. Bergstrom! Mr. Bergstrom!</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Lisa Simpson</td>\n      <td>Do you know where I could find him?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "ruta = './datasets/'\n",
    "df_0 = pd.read_csv(ruta+'simpsons/simpsons_dataset.csv').dropna()\n",
    "target_simpsons = [\n",
    "    'Lisa Simpson',\n",
    "    'Bart Simpson',\n",
    "    'Homer Simpson',\n",
    "    'Marge Simpson',\n",
    "    'Maggie Simpson',\n",
    "    'Grampa Simpson'\n",
    "]\n",
    "data = df_0.loc[df_0['raw_character_text'].isin(target_simpsons)]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['spoken_words'].values\n",
    "y = data['raw_character_text'].values"
   ]
  },
  {
   "source": [
    "Splitting the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.5, random_state=1000)"
   ]
  },
  {
   "source": [
    "Vectorize the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "top_words = 5000\n",
    "vectorize_layer = TextVectorization(\n",
    "    ngrams=None, max_tokens=top_words, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(sentences_train)\n",
    "print(len(vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_layer(np.array([[s] for s in sentences_train])).numpy()\n",
    "x_test = vectorize_layer(np.array([[s] for s in sentences_test])).numpy()\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=top_words)\n",
    "# Parece que te tira el dataset como 25 mil train, 25 mil test, con cada entrada el embedding 'int' del doc.\n",
    "# Esto nos tocaría usar un vectorizze layer sobre nuestros docs con output mode 'int'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n33293\n"
     ]
    }
   ],
   "source": [
    "# len(X_train[3])\n",
    "print(type(x_train))\n",
    "len(x_test)\n",
    "print(len(y_test))"
   ]
  },
  {
   "source": [
    "Pad or truncate all sequences to half of max lenght"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Máximo tamaño de documento en train: 122\n"
     ]
    }
   ],
   "source": [
    "# Podriamos usar esto o simplemente definir un número 'bueno' y truncar o paddear todo a ese número\n",
    "# Con twitter depronto sí sirve un número de 25 por ejemplo.\n",
    "max_doc_length = np.max([len(doc) for doc in x_train])\n",
    "\n",
    "print('Máximo tamaño de documento en train:', max_doc_length)\n",
    "x_train_padded = sequence.pad_sequences(x_train, maxlen=(max_doc_length))\n",
    "x_test_padded = sequence.pad_sequences(x_test, maxlen=(max_doc_length))"
   ]
  },
  {
   "source": [
    "Enconding classification classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "\n",
    "encoder.fit(y_train)\n",
    "y_train_bin = encoder.transform(y_train)\n",
    "y_test_bin = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Número de clases: 6\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(encoder.classes_)\n",
    "print('Número de clases:', num_classes)"
   ]
  },
  {
   "source": [
    "## Model definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá hay que tener cuidado porque nuestro problema de clasificación No es binario\n",
    "# Con los datos reales el Compile debe ser con 'categorical_crossentropy'\n",
    "# Además el tamaño de la última softmax debe ser el mismo que el número de categorias\n",
    "# Se tiene también que pasar los datos en Y a oneHotvectors, ese código lo podemos sacar del notebool model_V1\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 256\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_doc_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "source": [
    "Compilation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 122, 256)          1280000   \n_________________________________________________________________\nlstm_4 (LSTM)                (None, 100)               142800    \n_________________________________________________________________\ndense_4 (Dense)              (None, 6)                 606       \n=================================================================\nTotal params: 1,423,406\nTrainable params: 1,423,406\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "source": [
    "Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "521/521 [==============================] - 21s 37ms/step - loss: 1.4290 - accuracy: 0.4115 - val_loss: 1.4005 - val_accuracy: 0.4194\n",
      "Epoch 2/5\n",
      "521/521 [==============================] - 20s 37ms/step - loss: 1.4064 - accuracy: 0.4135 - val_loss: 1.3983 - val_accuracy: 0.4194\n",
      "Epoch 3/5\n",
      "521/521 [==============================] - 20s 38ms/step - loss: 1.4040 - accuracy: 0.4144 - val_loss: 1.3983 - val_accuracy: 0.4194\n",
      "Epoch 4/5\n",
      "521/521 [==============================] - 25s 47ms/step - loss: 1.3968 - accuracy: 0.4209 - val_loss: 1.4000 - val_accuracy: 0.4194\n",
      "Epoch 5/5\n",
      "521/521 [==============================] - 36s 69ms/step - loss: 1.4053 - accuracy: 0.4132 - val_loss: 1.3991 - val_accuracy: 0.4194\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2475f3b1ec8>"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "model.fit(x_train_padded, y_train_bin, validation_data=(x_test_padded, y_test_bin), epochs=5, batch_size=64)"
   ]
  }
 ]
}