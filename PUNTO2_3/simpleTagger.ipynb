{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd06c70104609522f67cb30e8200dfa6f77a5bec0e1b1538a23f62dccdf26f51f37",
   "display_name": "Python 3.7.10 64-bit ('TF-Keras': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import pipeline\n",
    "import json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataset(file, n=-1):\n",
    "    file = open(file, 'r')\n",
    "    data = []\n",
    "    for i, line in enumerate(file):\n",
    "        data.append(json.loads(line))\n",
    "        if n != -1 and i == n:\n",
    "            break\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "def splitData(data):\n",
    "    sentences = data['text'].values\n",
    "    y = data['tag'].values\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.20, random_state=1000)\n",
    "    return sentences_train, sentences_test, y_train, y_test\n",
    "\n",
    "def adaptVocab(text_dataset):\n",
    "    vectorize_layer = TextVectorization(\n",
    "    ngrams=None, max_tokens=None, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None, pad_to_max_tokens=True, \n",
    ")\n",
    "    vectorize_layer.adapt(text_dataset.batch(32))\n",
    "    vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "    print('Vocab size:', vocab_size)\n",
    "    return vectorize_layer, vocab_size\n",
    "\n",
    "def getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "\n",
    "    x_train = tokenizer.texts_to_matrix(sentences_train)\n",
    "    x_test = tokenizer.texts_to_matrix(sentences_test)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, tokenizer, encoder\n",
    "\n",
    "def getModel(num_labels):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "English"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 20150\n"
     ]
    }
   ],
   "source": [
    "english_path = './datasets/en.jsonl'\n",
    "data = readDataset(english_path)\n",
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               10317312  \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 10,583,046\n",
      "Trainable params: 10,583,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "30/30 - 5s - loss: 1.2497 - accuracy: 0.5075 - val_loss: 0.8930 - val_accuracy: 0.6422\n",
      "Epoch 2/10\n",
      "30/30 - 1s - loss: 0.5215 - accuracy: 0.8072 - val_loss: 0.8941 - val_accuracy: 0.6677\n",
      "Epoch 3/10\n",
      "30/30 - 1s - loss: 0.1538 - accuracy: 0.9555 - val_loss: 1.0631 - val_accuracy: 0.6645\n",
      "Epoch 4/10\n",
      "30/30 - 1s - loss: 0.0457 - accuracy: 0.9896 - val_loss: 1.1891 - val_accuracy: 0.6613\n",
      "Epoch 5/10\n",
      "30/30 - 0s - loss: 0.0243 - accuracy: 0.9947 - val_loss: 1.2566 - val_accuracy: 0.6603\n",
      "Epoch 6/10\n",
      "30/30 - 0s - loss: 0.0121 - accuracy: 0.9973 - val_loss: 1.3527 - val_accuracy: 0.6613\n",
      "Epoch 7/10\n",
      "30/30 - 0s - loss: 0.0069 - accuracy: 0.9981 - val_loss: 1.4413 - val_accuracy: 0.6603\n",
      "Epoch 8/10\n",
      "30/30 - 0s - loss: 0.0042 - accuracy: 0.9995 - val_loss: 1.4540 - val_accuracy: 0.6560\n",
      "Epoch 9/10\n",
      "30/30 - 0s - loss: 0.0053 - accuracy: 0.9989 - val_loss: 1.5054 - val_accuracy: 0.6528\n",
      "Epoch 10/10\n",
      "30/30 - 0s - loss: 0.0046 - accuracy: 0.9984 - val_loss: 1.5104 - val_accuracy: 0.6656\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_classes()\n",
    "prediction = model.predict(tokenizer.texts_to_matrix([\"The pfizer vaccine with ARNm is the best of them \"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vaccines\n"
     ]
    }
   ],
   "source": [
    "predict_class = np.argmax(prediction, axis=-1)\n",
    "print(encoder.classes_[predict_class[0]])\n"
   ]
  },
  {
   "source": [
    "Tagging"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1001, 18)\n"
     ]
    }
   ],
   "source": [
    "dfAll = readDataset('./datasets/en_hash.json', n=1000)\n",
    "print(dfAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictClass(text):\n",
    "    prediction = model.predict(tokenizer.texts_to_matrix([text]))\n",
    "    predict_class = np.argmax(prediction, axis=-1)\n",
    "    return encoder.classes_[predict_class[0]]\n",
    "\n",
    "def predictAll(dfAll):\n",
    "    predictions = []\n",
    "    for i, text in enumerate(dfAll['text'].values):\n",
    "        modelPrediction = predictClass(text)\n",
    "        predictions.append(modelPrediction)\n",
    "        if i % 1000 == 0:\n",
    "            endChar = '\\n' if i % 10000 == 0 else ' '\n",
    "            print(i, end=endChar)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1000 "
     ]
    }
   ],
   "source": [
    "predictions = predictAll(dfAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = pd.DataFrame(dfAll.iloc[:1001, : ])\n",
    "subsets['simpleTag'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 text         simpleTag\n",
       "0   Ways to Know It's Time for New Office Space #C...       vaccination\n",
       "1   A Utah pharmacist will not serve prison time f...              NONE\n",
       "2   Wasn't one of those principles sending the unc...              NONE\n",
       "3   ugggggggggg come on people https://t.co/BIcFCB...       vaccination\n",
       "4   @dougquan @TorontoStar I dont care what millio...       vaccination\n",
       "..                                                ...               ...\n",
       "95  Went to the gym for the first time since the p...              NONE\n",
       "96  @JuliaMorales Sadly we can’t keep them from br...  school-reopening\n",
       "97  @Travisdhanraj What about York Region and Toro...  school-reopening\n",
       "98  what makes Walking Dead so beloved to so many....              NONE\n",
       "99  xiao winning the social distance game without ...              NONE\n",
       "\n",
       "[100 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>simpleTag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ways to Know It's Time for New Office Space #C...</td>\n      <td>vaccination</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A Utah pharmacist will not serve prison time f...</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Wasn't one of those principles sending the unc...</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ugggggggggg come on people https://t.co/BIcFCB...</td>\n      <td>vaccination</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@dougquan @TorontoStar I dont care what millio...</td>\n      <td>vaccination</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>Went to the gym for the first time since the p...</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>@JuliaMorales Sadly we can’t keep them from br...</td>\n      <td>school-reopening</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>@Travisdhanraj What about York Region and Toro...</td>\n      <td>school-reopening</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>what makes Walking Dead so beloved to so many....</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>xiao winning the social distance game without ...</td>\n      <td>NONE</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "subsets[['text', 'simpleTag']][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NONE                455\n",
       "vaccination         314\n",
       "vaccines            145\n",
       "school-reopening     56\n",
       "mental-health        31\n",
       "Name: simpleTag, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "subsets['simpleTag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets.to_json('./datasets/1000simpleTagged_en.json', orient='records', lines=True)"
   ]
  },
  {
   "source": [
    "Español"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 20150\n"
     ]
    }
   ],
   "source": [
    "spanish_path = './datasets/en.jsonl'\n",
    "data = readDataset(spanish_path)\n",
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               10317312  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 10,583,046\n",
      "Trainable params: 10,583,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "30/30 - 2s - loss: 1.2378 - accuracy: 0.5154 - val_loss: 0.8811 - val_accuracy: 0.6592\n",
      "Epoch 2/10\n",
      "30/30 - 0s - loss: 0.5089 - accuracy: 0.8126 - val_loss: 0.9106 - val_accuracy: 0.6656\n",
      "Epoch 3/10\n",
      "30/30 - 0s - loss: 0.1488 - accuracy: 0.9622 - val_loss: 1.0523 - val_accuracy: 0.6816\n",
      "Epoch 4/10\n",
      "30/30 - 0s - loss: 0.0428 - accuracy: 0.9907 - val_loss: 1.1770 - val_accuracy: 0.6635\n",
      "Epoch 5/10\n",
      "30/30 - 0s - loss: 0.0203 - accuracy: 0.9947 - val_loss: 1.2987 - val_accuracy: 0.6645\n",
      "Epoch 6/10\n",
      "30/30 - 0s - loss: 0.0119 - accuracy: 0.9968 - val_loss: 1.4410 - val_accuracy: 0.6528\n",
      "Epoch 7/10\n",
      "30/30 - 0s - loss: 0.0117 - accuracy: 0.9981 - val_loss: 1.4183 - val_accuracy: 0.6560\n",
      "Epoch 8/10\n",
      "30/30 - 0s - loss: 0.0053 - accuracy: 0.9987 - val_loss: 1.4567 - val_accuracy: 0.6496\n",
      "Epoch 9/10\n",
      "30/30 - 0s - loss: 0.0070 - accuracy: 0.9987 - val_loss: 1.5134 - val_accuracy: 0.6550\n",
      "Epoch 10/10\n",
      "30/30 - 0s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 1.5278 - val_accuracy: 0.6475\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1001, 17)\n",
      "0\n",
      "1000 "
     ]
    }
   ],
   "source": [
    "dfAll = readDataset('./datasets/es_hash.json', n=1000)\n",
    "print(dfAll.shape)\n",
    "\n",
    "predictions = predictAll(dfAll)\n",
    "subsets = pd.DataFrame(dfAll.iloc[:1001, : ])\n",
    "subsets['simpleTag'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "vaccination      677\n",
       "vaccines         161\n",
       "NONE             147\n",
       "mental-health     16\n",
       "Name: simpleTag, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "subsets['simpleTag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets.to_json('./datasets/1000simpleTagged_es.json', orient='records', lines=True)"
   ]
  },
  {
   "source": [
    "Frances"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_path = './datasets/fr.jsonl'\n",
    "data = readDataset(french_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['vaccination', 'vaccines', 'mental-health', 'NONE',\n",
       "       'school-reopening', 'household-violence', 'vaccine',\n",
       "       'school reopening', 'none', 'mental health', 'vaccination ',\n",
       "       'mental health ', 'mental-health '], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "data['tag'].unique()"
   ]
  },
  {
   "source": [
    "- vaccines\n",
    "- vaccination\n",
    "- mental-health\n",
    "- school-reopening\n",
    "- household-violence\n",
    "- NONE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = data['tag'].apply(lambda x: x.replace(\"vaccines\", \"vaccine\").replace(\"vaccine\", \"vaccines\"))\n",
    "newData = newData.apply(lambda x: x.strip())\n",
    "newData = newData.apply(lambda x: x.replace(\"mental health\", \"mental-health\").replace(\"none\", \"NONE\"))\n",
    "newData = newData.apply(lambda x: x.replace(\"school reopening\", \"school-reopening\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['vaccination', 'vaccines', 'mental-health', 'NONE',\n",
       "       'school-reopening', 'household-violence'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "newData.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['vaccination', 'vaccines', 'mental-health', 'NONE',\n",
       "       'school-reopening', 'household-violence'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "data['tag'] = newData\n",
    "data['tag'].unique()"
   ]
  },
  {
   "source": [
    "Save cleaned Data\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json('./datasets/tagged/fr.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 16361\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "# y_train\n",
    "# data\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               8377344   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 8,643,078\n",
      "Trainable params: 8,643,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "20/20 - 1s - loss: 1.3194 - accuracy: 0.4865 - val_loss: 1.0725 - val_accuracy: 0.5656\n",
      "Epoch 2/10\n",
      "20/20 - 0s - loss: 0.7319 - accuracy: 0.7163 - val_loss: 0.9541 - val_accuracy: 0.6078\n",
      "Epoch 3/10\n",
      "20/20 - 0s - loss: 0.2851 - accuracy: 0.9304 - val_loss: 1.0317 - val_accuracy: 0.6391\n",
      "Epoch 4/10\n",
      "20/20 - 0s - loss: 0.0788 - accuracy: 0.9801 - val_loss: 1.2111 - val_accuracy: 0.6234\n",
      "Epoch 5/10\n",
      "20/20 - 0s - loss: 0.0293 - accuracy: 0.9957 - val_loss: 1.3648 - val_accuracy: 0.6297\n",
      "Epoch 6/10\n",
      "20/20 - 0s - loss: 0.0116 - accuracy: 0.9996 - val_loss: 1.4496 - val_accuracy: 0.6297\n",
      "Epoch 7/10\n",
      "20/20 - 0s - loss: 0.0049 - accuracy: 0.9996 - val_loss: 1.5084 - val_accuracy: 0.6344\n",
      "Epoch 8/10\n",
      "20/20 - 0s - loss: 0.0033 - accuracy: 0.9996 - val_loss: 1.5789 - val_accuracy: 0.6391\n",
      "Epoch 9/10\n",
      "20/20 - 0s - loss: 0.0022 - accuracy: 0.9996 - val_loss: 1.6174 - val_accuracy: 0.6359\n",
      "Epoch 10/10\n",
      "20/20 - 0s - loss: 0.0020 - accuracy: 0.9996 - val_loss: 1.6462 - val_accuracy: 0.6375\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1001, 18)\n",
      "0\n",
      "1000 "
     ]
    }
   ],
   "source": [
    "dfAll = readDataset('./datasets/fr_hash.json', n=1000)\n",
    "print(dfAll.shape)\n",
    "\n",
    "predictions = predictAll(dfAll)\n",
    "subsets = pd.DataFrame(dfAll.iloc[:1001, : ])\n",
    "subsets['simpleTag'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "vaccines            448\n",
       "vaccination         357\n",
       "NONE                169\n",
       "school-reopening     22\n",
       "mental-health         5\n",
       "Name: simpleTag, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "subsets['simpleTag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets.to_json('./datasets/1000simpleTagged_fr.json', orient='records', lines=True)"
   ]
  }
 ]
}