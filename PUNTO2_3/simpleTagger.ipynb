{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd06c70104609522f67cb30e8200dfa6f77a5bec0e1b1538a23f62dccdf26f51f37",
   "display_name": "Python 3.7.10 64-bit ('TF-Keras': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import pipeline\n",
    "import json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataset(file, n=-1):\n",
    "    file = open(file, 'r')\n",
    "    data = []\n",
    "    for i, line in enumerate(file):\n",
    "        data.append(json.loads(line))\n",
    "        if n != -1 and i == n:\n",
    "            break\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "def splitData(data):\n",
    "    sentences = data['text'].values\n",
    "    y = data['tag'].values\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.20, random_state=1000)\n",
    "    return sentences_train, sentences_test, y_train, y_test\n",
    "\n",
    "def adaptVocab(text_dataset):\n",
    "    vectorize_layer = TextVectorization(\n",
    "    ngrams=None, max_tokens=None, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None, pad_to_max_tokens=True, \n",
    ")\n",
    "    vectorize_layer.adapt(text_dataset.batch(32))\n",
    "    vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "    print('Vocab size:', vocab_size)\n",
    "    return vectorize_layer, vocab_size\n",
    "\n",
    "def getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "\n",
    "    x_train = tokenizer.texts_to_matrix(sentences_train)\n",
    "    x_test = tokenizer.texts_to_matrix(sentences_test)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, tokenizer, encoder\n",
    "\n",
    "def getModel(num_labels):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_metrics_by_class(model, x, y):\n",
    "    y_pred = model.predict(x, batch_size=64, verbose=1)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "    y_label = np.argmax(y, axis=1)\n",
    "    #print(confusion_matrix(y_pred_bool, y_label))\n",
    "    print(classification_report(y_label, y_pred_bool))\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "English"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 20150\n"
     ]
    }
   ],
   "source": [
    "english_path = './datasets/en.jsonl'\n",
    "data = readDataset(english_path)\n",
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               10317312  \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 10,583,046\n",
      "Trainable params: 10,583,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "30/30 - 4s - loss: 1.2497 - accuracy: 0.5149 - val_loss: 0.8887 - val_accuracy: 0.6400\n",
      "Epoch 2/10\n",
      "30/30 - 0s - loss: 0.5379 - accuracy: 0.7990 - val_loss: 0.8763 - val_accuracy: 0.6912\n",
      "Epoch 3/10\n",
      "30/30 - 0s - loss: 0.1527 - accuracy: 0.9577 - val_loss: 1.0564 - val_accuracy: 0.6709\n",
      "Epoch 4/10\n",
      "30/30 - 0s - loss: 0.0435 - accuracy: 0.9899 - val_loss: 1.2066 - val_accuracy: 0.6635\n",
      "Epoch 5/10\n",
      "30/30 - 0s - loss: 0.0212 - accuracy: 0.9944 - val_loss: 1.3965 - val_accuracy: 0.6571\n",
      "Epoch 6/10\n",
      "30/30 - 0s - loss: 0.0105 - accuracy: 0.9976 - val_loss: 1.4000 - val_accuracy: 0.6635\n",
      "Epoch 7/10\n",
      "30/30 - 0s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 1.4286 - val_accuracy: 0.6635\n",
      "Epoch 8/10\n",
      "30/30 - 0s - loss: 0.0085 - accuracy: 0.9979 - val_loss: 1.4935 - val_accuracy: 0.6741\n",
      "Epoch 9/10\n",
      "30/30 - 0s - loss: 0.0078 - accuracy: 0.9992 - val_loss: 1.4882 - val_accuracy: 0.6592\n",
      "Epoch 10/10\n",
      "30/30 - 0s - loss: 0.0054 - accuracy: 0.9987 - val_loss: 1.5123 - val_accuracy: 0.6635\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_classes()\n",
    "prediction = model.predict(tokenizer.texts_to_matrix([\"The pfizer vaccine with ARNm is the best of them \"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vaccines\n"
     ]
    }
   ],
   "source": [
    "predict_class = np.argmax(prediction, axis=-1)\n",
    "print(encoder.classes_[predict_class[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19/19 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.49      0.48       247\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.80      0.68      0.74        72\n",
      "           3       0.49      0.47      0.48       129\n",
      "           4       0.74      0.79      0.76       525\n",
      "           5       0.58      0.52      0.55       199\n",
      "\n",
      "    accuracy                           0.64      1174\n",
      "   macro avg       0.52      0.49      0.50      1174\n",
      "weighted avg       0.63      0.64      0.64      1174\n",
      "\n",
      "C:\\Users\\diego\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\diego\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\diego\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "get_metrics_by_class(model, x_test, y_test)"
   ]
  },
  {
   "source": [
    "Tagging"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1001, 18)\n"
     ]
    }
   ],
   "source": [
    "dfAll = readDataset('./datasets/en_hash.json', n=1000)\n",
    "print(dfAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictClass(text):\n",
    "    prediction = model.predict(tokenizer.texts_to_matrix([text]))\n",
    "    predict_class = np.argmax(prediction, axis=-1)\n",
    "    return encoder.classes_[predict_class[0]]\n",
    "\n",
    "def predictAll(dfAll):\n",
    "    predictions = []\n",
    "    for i, text in enumerate(dfAll['text'].values):\n",
    "        modelPrediction = predictClass(text)\n",
    "        predictions.append(modelPrediction)\n",
    "        if i % 1000 == 0:\n",
    "            endChar = '\\n' if i % 10000 == 0 else ' '\n",
    "            print(i, end=endChar)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1000 "
     ]
    }
   ],
   "source": [
    "predictions = predictAll(dfAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = pd.DataFrame(dfAll.iloc[:1001, : ])\n",
    "subsets['simpleTag'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 text         simpleTag\n",
       "0   Ways to Know It's Time for New Office Space #C...       vaccination\n",
       "1   A Utah pharmacist will not serve prison time f...              NONE\n",
       "2   Wasn't one of those principles sending the unc...              NONE\n",
       "3   ugggggggggg come on people https://t.co/BIcFCB...       vaccination\n",
       "4   @dougquan @TorontoStar I dont care what millio...          vaccines\n",
       "..                                                ...               ...\n",
       "95  Went to the gym for the first time since the p...              NONE\n",
       "96  @JuliaMorales Sadly we can’t keep them from br...  school-reopening\n",
       "97  @Travisdhanraj What about York Region and Toro...  school-reopening\n",
       "98  what makes Walking Dead so beloved to so many....              NONE\n",
       "99  xiao winning the social distance game without ...              NONE\n",
       "\n",
       "[100 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>simpleTag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ways to Know It's Time for New Office Space #C...</td>\n      <td>vaccination</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A Utah pharmacist will not serve prison time f...</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Wasn't one of those principles sending the unc...</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ugggggggggg come on people https://t.co/BIcFCB...</td>\n      <td>vaccination</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@dougquan @TorontoStar I dont care what millio...</td>\n      <td>vaccines</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>Went to the gym for the first time since the p...</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>@JuliaMorales Sadly we can’t keep them from br...</td>\n      <td>school-reopening</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>@Travisdhanraj What about York Region and Toro...</td>\n      <td>school-reopening</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>what makes Walking Dead so beloved to so many....</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>xiao winning the social distance game without ...</td>\n      <td>NONE</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "subsets[['text', 'simpleTag']][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NONE                440\n",
       "vaccination         345\n",
       "vaccines            136\n",
       "school-reopening     57\n",
       "mental-health        23\n",
       "Name: simpleTag, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "subsets['simpleTag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets.to_json('./datasets/1000simpleTagged_en.json', orient='records', lines=True)"
   ]
  },
  {
   "source": [
    "Español"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 20150\n"
     ]
    }
   ],
   "source": [
    "spanish_path = './datasets/en.jsonl'\n",
    "data = readDataset(spanish_path)\n",
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               10317312  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 10,583,046\n",
      "Trainable params: 10,583,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "30/30 - 1s - loss: 1.2634 - accuracy: 0.5003 - val_loss: 0.9016 - val_accuracy: 0.6464\n",
      "Epoch 2/10\n",
      "30/30 - 0s - loss: 0.5410 - accuracy: 0.8024 - val_loss: 0.8556 - val_accuracy: 0.6965\n",
      "Epoch 3/10\n",
      "30/30 - 0s - loss: 0.1527 - accuracy: 0.9593 - val_loss: 1.0132 - val_accuracy: 0.6848\n",
      "Epoch 4/10\n",
      "30/30 - 0s - loss: 0.0453 - accuracy: 0.9894 - val_loss: 1.1799 - val_accuracy: 0.6709\n",
      "Epoch 5/10\n",
      "30/30 - 0s - loss: 0.0228 - accuracy: 0.9949 - val_loss: 1.2846 - val_accuracy: 0.6571\n",
      "Epoch 6/10\n",
      "30/30 - 0s - loss: 0.0153 - accuracy: 0.9965 - val_loss: 1.3527 - val_accuracy: 0.6624\n",
      "Epoch 7/10\n",
      "30/30 - 0s - loss: 0.0086 - accuracy: 0.9973 - val_loss: 1.4356 - val_accuracy: 0.6709\n",
      "Epoch 8/10\n",
      "30/30 - 0s - loss: 0.0104 - accuracy: 0.9976 - val_loss: 1.4375 - val_accuracy: 0.6581\n",
      "Epoch 9/10\n",
      "30/30 - 0s - loss: 0.0052 - accuracy: 0.9989 - val_loss: 1.5087 - val_accuracy: 0.6539\n",
      "Epoch 10/10\n",
      "30/30 - 0s - loss: 0.0059 - accuracy: 0.9992 - val_loss: 1.5176 - val_accuracy: 0.6581\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19/19 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.55      0.51       247\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.75      0.67      0.71        72\n",
      "           3       0.51      0.36      0.42       129\n",
      "           4       0.76      0.81      0.78       525\n",
      "           5       0.62      0.52      0.57       199\n",
      "\n",
      "    accuracy                           0.65      1174\n",
      "   macro avg       0.52      0.49      0.50      1174\n",
      "weighted avg       0.65      0.65      0.64      1174\n",
      "\n",
      "C:\\Users\\diego\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\diego\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\diego\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "get_metrics_by_class(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1001, 17)\n",
      "0\n",
      "1000 "
     ]
    }
   ],
   "source": [
    "dfAll = readDataset('./datasets/es_hash.json', n=1000)\n",
    "print(dfAll.shape)\n",
    "\n",
    "predictions = predictAll(dfAll)\n",
    "subsets = pd.DataFrame(dfAll.iloc[:1001, : ])\n",
    "subsets['simpleTag'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "vaccination      678\n",
       "NONE             229\n",
       "vaccines          78\n",
       "mental-health     16\n",
       "Name: simpleTag, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "subsets['simpleTag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets.to_json('./datasets/1000simpleTagged_es.json', orient='records', lines=True)"
   ]
  },
  {
   "source": [
    "Frances"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_path = './datasets/fr.jsonl'\n",
    "data = readDataset(french_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['vaccination', 'vaccines', 'mental-health', 'NONE',\n",
       "       'school-reopening', 'household-violence', 'vaccine',\n",
       "       'school reopening', 'none', 'mental health', 'vaccination ',\n",
       "       'mental health ', 'mental-health '], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "data['tag'].unique()"
   ]
  },
  {
   "source": [
    "- vaccines\n",
    "- vaccination\n",
    "- mental-health\n",
    "- school-reopening\n",
    "- household-violence\n",
    "- NONE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = data['tag'].apply(lambda x: x.replace(\"vaccines\", \"vaccine\").replace(\"vaccine\", \"vaccines\"))\n",
    "newData = newData.apply(lambda x: x.strip())\n",
    "newData = newData.apply(lambda x: x.replace(\"mental health\", \"mental-health\").replace(\"none\", \"NONE\"))\n",
    "newData = newData.apply(lambda x: x.replace(\"school reopening\", \"school-reopening\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['vaccination', 'vaccines', 'mental-health', 'NONE',\n",
       "       'school-reopening', 'household-violence'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "newData.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['vaccination', 'vaccines', 'mental-health', 'NONE',\n",
       "       'school-reopening', 'household-violence'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "data['tag'] = newData\n",
    "data['tag'].unique()"
   ]
  },
  {
   "source": [
    "Save cleaned Data\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json('./datasets/tagged/fr.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 16361\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "# y_train\n",
    "# data\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               8377344   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 8,643,078\n",
      "Trainable params: 8,643,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "20/20 - 1s - loss: 1.3361 - accuracy: 0.4850 - val_loss: 1.1019 - val_accuracy: 0.5531\n",
      "Epoch 2/10\n",
      "20/20 - 0s - loss: 0.7494 - accuracy: 0.7097 - val_loss: 0.9575 - val_accuracy: 0.6062\n",
      "Epoch 3/10\n",
      "20/20 - 0s - loss: 0.3052 - accuracy: 0.9215 - val_loss: 1.0805 - val_accuracy: 0.6234\n",
      "Epoch 4/10\n",
      "20/20 - 0s - loss: 0.0801 - accuracy: 0.9785 - val_loss: 1.2682 - val_accuracy: 0.5969\n",
      "Epoch 5/10\n",
      "20/20 - 0s - loss: 0.0332 - accuracy: 0.9898 - val_loss: 1.4006 - val_accuracy: 0.6187\n",
      "Epoch 6/10\n",
      "20/20 - 0s - loss: 0.0134 - accuracy: 0.9996 - val_loss: 1.4837 - val_accuracy: 0.6219\n",
      "Epoch 7/10\n",
      "20/20 - 0s - loss: 0.0058 - accuracy: 0.9996 - val_loss: 1.5610 - val_accuracy: 0.6172\n",
      "Epoch 8/10\n",
      "20/20 - 0s - loss: 0.0039 - accuracy: 0.9996 - val_loss: 1.6171 - val_accuracy: 0.6250\n",
      "Epoch 9/10\n",
      "20/20 - 0s - loss: 0.0023 - accuracy: 0.9996 - val_loss: 1.6452 - val_accuracy: 0.6219\n",
      "Epoch 10/10\n",
      "20/20 - 0s - loss: 0.0018 - accuracy: 0.9996 - val_loss: 1.6728 - val_accuracy: 0.6187\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13/13 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.26      0.33       103\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.67      0.33      0.44        12\n",
      "           3       0.79      0.76      0.77        49\n",
      "           4       0.61      0.61      0.61       296\n",
      "           5       0.65      0.75      0.70       338\n",
      "\n",
      "    accuracy                           0.63       800\n",
      "   macro avg       0.52      0.45      0.48       800\n",
      "weighted avg       0.62      0.63      0.62       800\n",
      "\n",
      "C:\\Users\\diego\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\diego\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\diego\\anaconda3\\envs\\TF-Keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "get_metrics_by_class(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1001, 18)\n",
      "0\n",
      "1000 "
     ]
    }
   ],
   "source": [
    "dfAll = readDataset('./datasets/fr_hash.json', n=1000)\n",
    "print(dfAll.shape)\n",
    "\n",
    "predictions = predictAll(dfAll)\n",
    "subsets = pd.DataFrame(dfAll.iloc[:1001, : ])\n",
    "subsets['simpleTag'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "vaccines            453\n",
       "vaccination         326\n",
       "NONE                195\n",
       "school-reopening     21\n",
       "mental-health         6\n",
       "Name: simpleTag, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "subsets['simpleTag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets.to_json('./datasets/1000simpleTagged_fr.json', orient='records', lines=True)"
   ]
  }
 ]
}