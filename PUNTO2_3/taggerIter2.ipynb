{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd06c70104609522f67cb30e8200dfa6f77a5bec0e1b1538a23f62dccdf26f51f37",
   "display_name": "Python 3.7.10 64-bit ('TF-Keras': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import pipeline\n",
    "import json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './datasets/tagged/'\n",
    "path_en = folder + 'integrated_en.json'\n",
    "path_es = folder + 'integrated_es.json'\n",
    "path_fr = folder + 'integrated_fr.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataset(file, n=-1):\n",
    "    file = open(file, 'r')\n",
    "    data = []\n",
    "    for i, line in enumerate(file):\n",
    "        data.append(json.loads(line))\n",
    "        if n != -1 and i == n:\n",
    "            break\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "def splitData(data):\n",
    "    sentences = data['text'].values\n",
    "    y = data['tag'].values\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.20, random_state=1000)\n",
    "    return sentences_train, sentences_test, y_train, y_test\n",
    "\n",
    "def adaptVocab(text_dataset):\n",
    "    vectorize_layer = TextVectorization(\n",
    "    ngrams=None, max_tokens=None, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None, pad_to_max_tokens=True, \n",
    ")\n",
    "    vectorize_layer.adapt(text_dataset.batch(32))\n",
    "    vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "    print('Vocab size:', vocab_size)\n",
    "    return vectorize_layer, vocab_size\n",
    "\n",
    "def getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "\n",
    "    x_train = tokenizer.texts_to_matrix(sentences_train)\n",
    "    x_test = tokenizer.texts_to_matrix(sentences_test)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, tokenizer, encoder\n",
    "\n",
    "def getModel(num_labels):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_metrics_by_class(model, x, y):\n",
    "    y_pred = model.predict(x, batch_size=64, verbose=1)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "    y_label = np.argmax(y, axis=1)\n",
    "    #print(confusion_matrix(y_pred_bool, y_label))\n",
    "    print(classification_report(y_label, y_pred_bool))\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "English"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 22779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = readDataset(path_en)\n",
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 512)               11663360  \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 11,929,094\n",
      "Trainable params: 11,929,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "35/35 - 4s - loss: 1.2558 - accuracy: 0.5183 - val_loss: 0.9061 - val_accuracy: 0.6457\n",
      "Epoch 2/10\n",
      "35/35 - 1s - loss: 0.5079 - accuracy: 0.8137 - val_loss: 0.9253 - val_accuracy: 0.6730\n",
      "Epoch 3/10\n",
      "35/35 - 1s - loss: 0.1167 - accuracy: 0.9706 - val_loss: 1.1495 - val_accuracy: 0.6603\n",
      "Epoch 4/10\n",
      "35/35 - 1s - loss: 0.0368 - accuracy: 0.9936 - val_loss: 1.3333 - val_accuracy: 0.6466\n",
      "Epoch 5/10\n",
      "35/35 - 1s - loss: 0.0223 - accuracy: 0.9954 - val_loss: 1.3744 - val_accuracy: 0.6375\n",
      "Epoch 6/10\n",
      "35/35 - 1s - loss: 0.0110 - accuracy: 0.9970 - val_loss: 1.4243 - val_accuracy: 0.6530\n",
      "Epoch 7/10\n",
      "35/35 - 1s - loss: 0.0112 - accuracy: 0.9984 - val_loss: 1.4758 - val_accuracy: 0.6548\n",
      "Epoch 8/10\n",
      "35/35 - 1s - loss: 0.0070 - accuracy: 0.9986 - val_loss: 1.5223 - val_accuracy: 0.6494\n",
      "Epoch 9/10\n",
      "35/35 - 0s - loss: 0.0066 - accuracy: 0.9982 - val_loss: 1.6150 - val_accuracy: 0.6439\n",
      "Epoch 10/10\n",
      "35/35 - 0s - loss: 0.0076 - accuracy: 0.9991 - val_loss: 1.6172 - val_accuracy: 0.6557\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vaccines\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(tokenizer.texts_to_matrix([\"The pfizer vaccine with ARNm is the best of them \"]))\n",
    "predict_class = np.argmax(prediction, axis=-1)\n",
    "print(encoder.classes_[predict_class[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "22/22 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61       374\n",
      "           1       0.89      0.67      0.76        24\n",
      "           2       0.84      0.69      0.76        75\n",
      "           3       0.50      0.55      0.52       122\n",
      "           4       0.71      0.84      0.77       545\n",
      "           5       0.67      0.50      0.57       233\n",
      "\n",
      "    accuracy                           0.67      1373\n",
      "   macro avg       0.71      0.64      0.66      1373\n",
      "weighted avg       0.67      0.67      0.67      1373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics_by_class(model, x_test, y_test)"
   ]
  },
  {
   "source": [
    "Spanish"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 24550\n"
     ]
    }
   ],
   "source": [
    "data = readDataset(path_es)\n",
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 512)               12570112  \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 12,835,846\n",
      "Trainable params: 12,835,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "33/33 - 1s - loss: 1.2752 - accuracy: 0.4935 - val_loss: 1.0637 - val_accuracy: 0.5871\n",
      "Epoch 2/10\n",
      "33/33 - 0s - loss: 0.6014 - accuracy: 0.7837 - val_loss: 1.0511 - val_accuracy: 0.5871\n",
      "Epoch 3/10\n",
      "33/33 - 0s - loss: 0.1905 - accuracy: 0.9444 - val_loss: 1.2813 - val_accuracy: 0.5919\n",
      "Epoch 4/10\n",
      "33/33 - 0s - loss: 0.0488 - accuracy: 0.9904 - val_loss: 1.5166 - val_accuracy: 0.5813\n",
      "Epoch 5/10\n",
      "33/33 - 0s - loss: 0.0135 - accuracy: 0.9974 - val_loss: 1.6155 - val_accuracy: 0.5833\n",
      "Epoch 6/10\n",
      "33/33 - 0s - loss: 0.0065 - accuracy: 0.9993 - val_loss: 1.7506 - val_accuracy: 0.5852\n",
      "Epoch 7/10\n",
      "33/33 - 0s - loss: 0.0063 - accuracy: 0.9990 - val_loss: 1.7480 - val_accuracy: 0.5852\n",
      "Epoch 8/10\n",
      "33/33 - 0s - loss: 0.0041 - accuracy: 0.9998 - val_loss: 1.7963 - val_accuracy: 0.5861\n",
      "Epoch 9/10\n",
      "33/33 - 0s - loss: 0.0031 - accuracy: 0.9998 - val_loss: 1.8531 - val_accuracy: 0.5746\n",
      "Epoch 10/10\n",
      "33/33 - 0s - loss: 0.0020 - accuracy: 0.9998 - val_loss: 1.8739 - val_accuracy: 0.5852\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vaccination\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(tokenizer.texts_to_matrix([\"Las vacunas llegarán este jueves a Francia para empezar con el proceso de vacunación\"]))\n",
    "predict_class = np.argmax(prediction, axis=-1)\n",
    "print(encoder.classes_[predict_class[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21/21 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.68      0.64       490\n",
      "           1       0.77      0.77      0.77        30\n",
      "           2       0.50      0.18      0.27        22\n",
      "           3       0.80      0.78      0.79        36\n",
      "           4       0.62      0.63      0.62       460\n",
      "           5       0.52      0.42      0.46       261\n",
      "\n",
      "    accuracy                           0.60      1299\n",
      "   macro avg       0.63      0.57      0.59      1299\n",
      "weighted avg       0.60      0.60      0.60      1299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics_by_class(model, x_test, y_test)"
   ]
  },
  {
   "source": [
    "French"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 18508\n"
     ]
    }
   ],
   "source": [
    "data = readDataset(path_fr)\n",
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 512)               9476608   \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 9,742,342\n",
      "Trainable params: 9,742,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "24/24 - 1s - loss: 1.3483 - accuracy: 0.4337 - val_loss: 1.0585 - val_accuracy: 0.5296\n",
      "Epoch 2/10\n",
      "24/24 - 0s - loss: 0.6903 - accuracy: 0.7440 - val_loss: 0.9216 - val_accuracy: 0.6590\n",
      "Epoch 3/10\n",
      "24/24 - 0s - loss: 0.2257 - accuracy: 0.9322 - val_loss: 1.1748 - val_accuracy: 0.6361\n",
      "Epoch 4/10\n",
      "24/24 - 0s - loss: 0.0764 - accuracy: 0.9771 - val_loss: 1.2648 - val_accuracy: 0.6334\n",
      "Epoch 5/10\n",
      "24/24 - 0s - loss: 0.0288 - accuracy: 0.9973 - val_loss: 1.4393 - val_accuracy: 0.6173\n",
      "Epoch 6/10\n",
      "24/24 - 0s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 1.5356 - val_accuracy: 0.6388\n",
      "Epoch 7/10\n",
      "24/24 - 0s - loss: 0.0036 - accuracy: 0.9997 - val_loss: 1.5871 - val_accuracy: 0.6173\n",
      "Epoch 8/10\n",
      "24/24 - 0s - loss: 0.0027 - accuracy: 0.9997 - val_loss: 1.6421 - val_accuracy: 0.6199\n",
      "Epoch 9/10\n",
      "24/24 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.6755 - val_accuracy: 0.6146\n",
      "Epoch 10/10\n",
      "24/24 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.7097 - val_accuracy: 0.6199\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15/15 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.45      0.52       165\n",
      "           1       1.00      0.86      0.92         7\n",
      "           2       0.86      0.33      0.48        18\n",
      "           3       0.67      0.71      0.69        55\n",
      "           4       0.58      0.67      0.62       323\n",
      "           5       0.67      0.67      0.67       359\n",
      "\n",
      "    accuracy                           0.63       927\n",
      "   macro avg       0.73      0.62      0.65       927\n",
      "weighted avg       0.63      0.63      0.63       927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics_by_class(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NONE\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(tokenizer.texts_to_matrix([\"School called me saying that my kid was contact traced, a classmate got covid, so he has to stay home. His sibling also, because, well, they are siblings\\nI love them both, but...all the cooking! Why do they have to eat more than once a day??! They don't make good pets.\"]))\n",
    "predict_class = np.argmax(prediction, axis=-1)\n",
    "print(encoder.classes_[predict_class[0]])"
   ]
  }
 ]
}