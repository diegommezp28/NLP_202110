{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd06c70104609522f67cb30e8200dfa6f77a5bec0e1b1538a23f62dccdf26f51f37",
   "display_name": "Python 3.7.10 64-bit ('TF-Keras': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import pipeline\n",
    "import json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './datasets/tagged/'\n",
    "path_en = folder + 'integrated_en.json'\n",
    "path_es = folder + 'integrated_es.json'\n",
    "path_fr = folder + 'integrated_fr.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataset(file, n=-1):\n",
    "    file = open(file, 'r')\n",
    "    data = []\n",
    "    for i, line in enumerate(file):\n",
    "        data.append(json.loads(line))\n",
    "        if n != -1 and i == n:\n",
    "            break\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "def splitData(data):\n",
    "    sentences = data['text'].values\n",
    "    y = data['tag'].values\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.20, random_state=1000)\n",
    "    return sentences_train, sentences_test, y_train, y_test\n",
    "\n",
    "def adaptVocab(text_dataset):\n",
    "    vectorize_layer = TextVectorization(\n",
    "    ngrams=None, max_tokens=None, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None, pad_to_max_tokens=True, \n",
    ")\n",
    "    vectorize_layer.adapt(text_dataset.batch(32))\n",
    "    vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "    print('Vocab size:', vocab_size)\n",
    "    return vectorize_layer, vocab_size\n",
    "\n",
    "def getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "\n",
    "    x_train = tokenizer.texts_to_matrix(sentences_train)\n",
    "    x_test = tokenizer.texts_to_matrix(sentences_test)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, tokenizer, encoder\n",
    "\n",
    "def getModel(num_labels):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "English"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 22779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = readDataset(path_en)\n",
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               11663360  \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 11,929,094\n",
      "Trainable params: 11,929,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "35/35 - 2s - loss: 1.2290 - accuracy: 0.5249 - val_loss: 0.9124 - val_accuracy: 0.6485\n",
      "Epoch 2/10\n",
      "35/35 - 1s - loss: 0.4998 - accuracy: 0.8185 - val_loss: 0.9290 - val_accuracy: 0.6694\n",
      "Epoch 3/10\n",
      "35/35 - 1s - loss: 0.1162 - accuracy: 0.9706 - val_loss: 1.1566 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "35/35 - 1s - loss: 0.0349 - accuracy: 0.9923 - val_loss: 1.3024 - val_accuracy: 0.6694\n",
      "Epoch 5/10\n",
      "35/35 - 1s - loss: 0.0183 - accuracy: 0.9959 - val_loss: 1.3693 - val_accuracy: 0.6658\n",
      "Epoch 6/10\n",
      "35/35 - 1s - loss: 0.0118 - accuracy: 0.9975 - val_loss: 1.4226 - val_accuracy: 0.6712\n",
      "Epoch 7/10\n",
      "35/35 - 1s - loss: 0.0098 - accuracy: 0.9977 - val_loss: 1.4420 - val_accuracy: 0.6594\n",
      "Epoch 8/10\n",
      "35/35 - 1s - loss: 0.0085 - accuracy: 0.9980 - val_loss: 1.4987 - val_accuracy: 0.6648\n",
      "Epoch 9/10\n",
      "35/35 - 1s - loss: 0.0079 - accuracy: 0.9975 - val_loss: 1.5185 - val_accuracy: 0.6648\n",
      "Epoch 10/10\n",
      "35/35 - 1s - loss: 0.0077 - accuracy: 0.9989 - val_loss: 1.5317 - val_accuracy: 0.6548\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vaccines\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(tokenizer.texts_to_matrix([\"The pfizer vaccine with ARNm is the best of them \"]))\n",
    "predict_class = np.argmax(prediction, axis=-1)\n",
    "print(encoder.classes_[predict_class[0]])"
   ]
  },
  {
   "source": [
    "Spanish"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 24550\n"
     ]
    }
   ],
   "source": [
    "data = readDataset(path_es)\n",
    "num_labels = len(data[\"tag\"].unique())\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               12570112  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 12,835,846\n",
      "Trainable params: 12,835,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "33/33 - 1s - loss: 1.2959 - accuracy: 0.4755 - val_loss: 1.0788 - val_accuracy: 0.5650\n",
      "Epoch 2/10\n",
      "33/33 - 1s - loss: 0.6149 - accuracy: 0.7825 - val_loss: 1.0801 - val_accuracy: 0.5794\n",
      "Epoch 3/10\n",
      "33/33 - 1s - loss: 0.1992 - accuracy: 0.9403 - val_loss: 1.2438 - val_accuracy: 0.5919\n",
      "Epoch 4/10\n",
      "33/33 - 1s - loss: 0.0443 - accuracy: 0.9925 - val_loss: 1.5576 - val_accuracy: 0.5958\n",
      "Epoch 5/10\n",
      "33/33 - 1s - loss: 0.0124 - accuracy: 0.9990 - val_loss: 1.6143 - val_accuracy: 0.6006\n",
      "Epoch 6/10\n",
      "33/33 - 1s - loss: 0.0073 - accuracy: 0.9993 - val_loss: 1.6346 - val_accuracy: 0.5967\n",
      "Epoch 7/10\n",
      "33/33 - 1s - loss: 0.0035 - accuracy: 0.9998 - val_loss: 1.6945 - val_accuracy: 0.6054\n",
      "Epoch 8/10\n",
      "33/33 - 1s - loss: 0.0061 - accuracy: 0.9995 - val_loss: 1.7266 - val_accuracy: 0.5967\n",
      "Epoch 9/10\n",
      "33/33 - 1s - loss: 0.0042 - accuracy: 0.9993 - val_loss: 1.8103 - val_accuracy: 0.5910\n",
      "Epoch 10/10\n",
      "33/33 - 1s - loss: 0.0021 - accuracy: 0.9993 - val_loss: 1.7659 - val_accuracy: 0.5938\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vaccination\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(tokenizer.texts_to_matrix([\"Las vacunas llegarán este jueves a Francia para empezar con el proceso de vacunación\"]))\n",
    "predict_class = np.argmax(prediction, axis=-1)\n",
    "print(encoder.classes_[predict_class[0]])"
   ]
  },
  {
   "source": [
    "French"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 18508\n"
     ]
    }
   ],
   "source": [
    "data = readDataset(path_fr)\r\n",
    "num_labels = len(data[\"tag\"].unique())\r\n",
    "sentences_train, sentences_test, y_train, y_test = splitData(data)\r\n",
    "text_dataset = Dataset.from_tensor_slices((sentences_train))\r\n",
    "vectorize_layer, vocab_size = adaptVocab(text_dataset)\r\n",
    "x_train, y_train, x_test, y_test, tokenizer, encoder = getTokenizer(vocab_size, sentences_train, sentences_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               9476608   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 9,742,342\n",
      "Trainable params: 9,742,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "24/24 - 1s - loss: 1.3569 - accuracy: 0.4449 - val_loss: 1.0836 - val_accuracy: 0.5310\n",
      "Epoch 2/10\n",
      "24/24 - 0s - loss: 0.7100 - accuracy: 0.7400 - val_loss: 0.9347 - val_accuracy: 0.6644\n",
      "Epoch 3/10\n",
      "24/24 - 0s - loss: 0.2403 - accuracy: 0.9332 - val_loss: 1.0334 - val_accuracy: 0.6496\n",
      "Epoch 4/10\n",
      "24/24 - 0s - loss: 0.0773 - accuracy: 0.9777 - val_loss: 1.2825 - val_accuracy: 0.6199\n",
      "Epoch 5/10\n",
      "24/24 - 0s - loss: 0.0324 - accuracy: 0.9973 - val_loss: 1.3828 - val_accuracy: 0.6294\n",
      "Epoch 6/10\n",
      "24/24 - 0s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.4690 - val_accuracy: 0.6280\n",
      "Epoch 7/10\n",
      "24/24 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.5296 - val_accuracy: 0.6280\n",
      "Epoch 8/10\n",
      "24/24 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.5766 - val_accuracy: 0.6280\n",
      "Epoch 9/10\n",
      "24/24 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.5917 - val_accuracy: 0.6361\n",
      "Epoch 10/10\n",
      "24/24 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.6312 - val_accuracy: 0.6307\n"
     ]
    }
   ],
   "source": [
    "model = getModel(num_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NONE\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(tokenizer.texts_to_matrix([\"School called me saying that my kid was contact traced, a classmate got covid, so he has to stay home. His sibling also, because, well, they are siblings\\nI love them both, but...all the cooking! Why do they have to eat more than once a day??! They don't make good pets.\"]))\n",
    "predict_class = np.argmax(prediction, axis=-1)\n",
    "print(encoder.classes_[predict_class[0]])"
   ]
  }
 ]
}